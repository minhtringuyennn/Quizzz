{
  "count": 30,
  "next": null,
  "previous": null,
  "results": [
    {
      "_class": "assessment",
      "id": 57141708,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A financial services company wants to set up an AWS WAF-based solution to manage AWS WAF rules across multiple AWS accounts that are structured under different Organization Units (OUs) in AWS Organizations. The solution should automatically update and remediate noncompliant AWS WAF rules in all accounts. The solution should also facilitate adding or removing accounts or OUs from managed AWS WAF rule sets as needed.</p>\n\n<p>Which of the following solutions is the most operationally efficient to address the given use case?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Organizations organization-wide AWS Config rule that mandates all resources in the selected OUs to be associated with the AWS WAF rules. Configure automated remediation actions by using AWS Systems Manager Automation documents to fix non-compliant resources. Set up AWS WAF rules by using an AWS CloudFormation stack set to target the same OUs where the AWS Config rule is applied</strong></p>\n\n<p>AWS Config allows you to manage AWS Config rules across all AWS accounts within an organization. You can:</p>\n\n<p>Centrally create, update, and delete AWS Config rules across all accounts in your organization.</p>\n\n<p>Deploy a common set of AWS Config rules across all accounts and specify accounts where AWS Config rules should not be created.</p>\n\n<p>Use the APIs from the management account in AWS Organizations to enforce governance by ensuring that the underlying AWS Config rules are not modifiable by your organizationâ€™s member accounts.</p>\n\n<p>If a new account joins an organization, the rule or conformance pack is deployed to that account. When an account leaves an organization, the rule or conformance pack is removed.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q16-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html\">https://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html</a></p>\n\n<p>AWS Config allows you to remediate noncompliant resources that are evaluated by AWS Config Rules. AWS Config applies remediation using AWS Systems Manager Automation documents. These documents define the actions to be performed on noncompliant AWS resources evaluated by AWS Config Rules. You can associate SSM documents by using AWS Management Console or by using APIs. To apply remediation on non-compliant resources, you can either choose the remediation action you want to associate from a prepopulated list or create your own custom remediation actions using SSM documents. AWS Config provides a recommended list of remediation actions in the AWS Management Console.</p>\n\n<p>AWS CloudFormation StackSets extends the capability of CloudFormation stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified AWS Regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Leverage AWS Systems Manager Parameter Store to store account numbers and OUs. Update AWS Systems Manager Parameter Store as needed to add or remove accounts or OUs. Create cross-account IAM roles in member accounts with permissions to create and update AWS WAF rules. Create a Lambda function to assume IAM roles in the management account to create and update AWS WAF rules in the member accounts</strong> - This option involves significant manual work every time an account is added/removed from the organization. You need to update the items in Systems Manager Parameter Store and further update the Lambda to assume the role for the new account. Hence this option is incorrect.</p>\n\n<p><strong>Use AWS Control Tower to manage AWS WAF rules across accounts in the organization. Leverage AWS Secrets Manager to store account numbers and OUs. Update AWS Secrets Manager as needed to add or remove accounts or OUs. Create cross-account IAM roles in member accounts with permissions to create and update AWS WAF rules. Create a Lambda function to assume IAM roles in the management account to create and update AWS WAF rules in the member accounts</strong> - This option involves significant manual work every time an account is added/removed from the organization. You need to update the items in Secrets Manager and further update the Lambda to assume the role for the new account. Hence this option is incorrect.</p>\n\n<p><strong>Use AWS Security Hub to manage AWS WAF rules across accounts in the organization. Leverage AWS KMS to store account numbers and OUs. Update AWS KMS as needed to add or remove accounts or OUs. Create IAM users in member accounts. Allow AWS Firewall Manager in the management account to use the access key and secret access key to create and update AWS WAF rules in the member accounts</strong> - This option has been added as a distractor. You cannot use AWS Security Hub to manage AWS WAF rules across accounts in the organization, rather you need to use AWS Firewall Manager to accomplish this. AWS KMS is a managed service that helps you more easily create and control the keys used for cryptographic operations. The service provides a highly available key generation, storage, management, and auditing solution for you to encrypt or digitally sign data within your own applications or control the encryption of data across AWS services. You cannot use AWS KMS to store account numbers and OUs.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html\">https://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/12/aws-security-hub-integrates-with-aws-firewall-manager/\">https://aws.amazon.com/about-aws/whats-new/2019/12/aws-security-hub-integrates-with-aws-firewall-manager/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/remediation.html\">https://docs.aws.amazon.com/config/latest/developerguide/remediation.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/</a></p>\n",
        "answers": [
          "<p>Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Leverage AWS Systems Manager Parameter Store to store account numbers and OUs. Update AWS Systems Manager Parameter Store as needed to add or remove accounts or OUs. Create cross-account IAM roles in member accounts with permissions to create and update AWS WAF rules. Create a Lambda function to assume IAM roles in the management account to create and update AWS WAF rules in the member accounts</p>",
          "<p>Use AWS Control Tower to manage AWS WAF rules across accounts in the organization. Leverage AWS Secrets Manager to store account numbers and OUs. Update AWS Secrets Manager as needed to add or remove accounts or OUs. Create cross-account IAM roles in member accounts with permissions to create and update AWS WAF rules. Create a Lambda function to assume IAM roles in the management account to create and update AWS WAF rules in the member accounts</p>",
          "<p>Create an AWS Organizations organization-wide AWS Config rule that mandates all resources in the selected OUs to be associated with the AWS WAF rules. Configure automated remediation actions by using AWS Systems Manager Automation documents to fix non-compliant resources. Set up AWS WAF rules by using an AWS CloudFormation stack set to target the same OUs where the AWS Config rule is applied</p>",
          "<p>Use AWS Security Hub to manage AWS WAF rules across accounts in the organization. Leverage AWS KMS to store account numbers and OUs. Update AWS KMS as needed to add or remove accounts or OUs. Create IAM users in member accounts. Allow AWS Firewall Manager in the management account to use the access key and secret access key to create and update AWS WAF rules in the member accounts</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "Design Solutions for Organizational Complexity",
      "question_plain": "A financial services company wants to set up an AWS WAF-based solution to manage AWS WAF rules across multiple AWS accounts that are structured under different Organization Units (OUs) in AWS Organizations. The solution should automatically update and remediate noncompliant AWS WAF rules in all accounts. The solution should also facilitate adding or removing accounts or OUs from managed AWS WAF rule sets as needed.\n\nWhich of the following solutions is the most operationally efficient to address the given use case?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141736,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A leading pharmaceutical company has significant investments in running Oracle and PostgreSQL services on Amazon RDS which provide their scientists with near real-time analysis of millions of rows of manufacturing data generated by continuous manufacturing equipment with 1,600 data points per row. The business analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The engineering team has observed that the database performance takes a hit whenever these reports are run by the analytics team. To facilitate the business analytics reporting, the engineering team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.</p>\n\n<p>As a Solutions Architect Professional, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</strong></p>\n\n<p>AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS Database Migration Service, you can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3.</p>\n\n<p>Continuous Data Replication\n<img src=\"https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram-AWS-DMS_continuous-data-replication.a0e3bd328d2a4bd9b40a83e767199dcc13cf678f.png\">\nvia - https://aws.amazon.com/dms/</p>\n\n<p>You can migrate data to Amazon Redshift databases using AWS Database Migration Service. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. With an Amazon Redshift database as a target, you can migrate data from all of the other supported source databases.</p>\n\n<p>The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance.\nDuring a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database. The AWS DMS replication instance must be located in that same region.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Glue to replicate the data from the databases into Amazon Redshift</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing.</p>\n\n<p>Using AWS Glue involves significant development efforts to write custom migration scripts to copy the database data into Redshift.</p>\n\n<p><strong>Use Amazon EMR to replicate the data from the databases into Amazon Redshift</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.</p>\n\n<p>Using EMR involves significant infrastructure management efforts to set up and maintain the EMR cluster. Additionally, this option involves a major development effort to write custom migration jobs to copy the database data into Redshift.</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>However, the user is expected to manually provision an appropriate number of shards to process the expected volume of the incoming data stream. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. Therefore Kinesis Data Streams is not the right fit for this use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p>\n",
        "answers": [
          "<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</p>",
          "<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift</p>",
          "<p>Use Amazon EMR to replicate the data from the databases into Amazon Redshift</p>",
          "<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "Continuous Improvement for Existing Solutions",
      "question_plain": "A leading pharmaceutical company has significant investments in running Oracle and PostgreSQL services on Amazon RDS which provide their scientists with near real-time analysis of millions of rows of manufacturing data generated by continuous manufacturing equipment with 1,600 data points per row. The business analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The engineering team has observed that the database performance takes a hit whenever these reports are run by the analytics team. To facilitate the business analytics reporting, the engineering team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.\n\nAs a Solutions Architect Professional, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141734,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company uses Elastic Load Balancing to distribute traffic across multiple Amazon EC2 instances. Auto Scaling groups start and stop Amazon EC2 machines based on the number of incoming requests. The company has recently started operations in a new AWS Region and is setting up an Application Load Balancer for its fleet of EC2 instances spread across two Availability Zones, with one instance as a target in Availability Zone X and four instances as targets in Availability Zone Y. The company is doing benchmarking for server performance in the new Region for the case when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled.</p>\n\n<p>As a Solutions Architect Professional, which of the following traffic distribution outcomes would you identify as correct?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>With cross-zone load balancing enabled, one instance in Availability Zone X receives 20% traffic and four instances in Availability Zone Y receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone X receives 50% traffic and four instances in Availability Zone Y receive 12.5% traffic each</strong></p>\n\n<p>The nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. Therefore, one instance in Availability Zone X receives 20% traffic and four instances in Availability Zone Y receive 20% traffic each. When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. Therefore, one instance in Availability Zone X receives 50% traffic and four instances in Availability Zone Y receive 12.5% traffic each.</p>\n\n<p>Consider the following diagrams (the scenario illustrated in the diagrams involves 10 target instances split across 2 AZs) to understand the effect of cross-zone load balancing.</p>\n\n<p>If cross-zone load balancing is enabled, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route its 50% of the client traffic to all 10 targets.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/images/cross_zone_load_balancing_enabled.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n\n<p>If cross-zone load balancing is disabled:</p>\n\n<p>Each of the two targets in Availability Zone X receives 25% of the traffic.</p>\n\n<p>Each of the eight targets in Availability Zone Y receives 6.25% of the traffic.</p>\n\n<p>This is because each load balancer node can route its 50% of the client traffic only to targets in its Availability Zone</p>\n\n<p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/images/cross_zone_load_balancing_disabled.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>With cross-zone load balancing enabled, one instance in Availability Zone X receives 50% traffic and four instances in Availability Zone Y receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone X receives 20% traffic and four instances in Availability Zone Y receive 20% traffic each</strong></p>\n\n<p><strong>With cross-zone load balancing enabled, one instance in Availability Zone X receives no traffic and four instances in Availability Zone Y receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone X receives 50% traffic and four instances in Availability Zone Y receive 12.5% traffic each</strong></p>\n\n<p><strong>With cross-zone load balancing enabled, one instance in Availability Zone X receives 20% traffic and four instances in Availability Zone Y receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone X receives no traffic and four instances in Availability Zone Y receive 25% traffic each</strong></p>\n\n<p>These three options contradict the description provided in the explanation above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p>\n",
        "answers": [
          "<p>With cross-zone load balancing enabled, one instance in Availability Zone X receives 20% traffic and four instances in Availability Zone Y receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone X receives 50% traffic and four instances in Availability Zone Y receive 12.5% traffic each</p>",
          "<p>With cross-zone load balancing enabled, one instance in Availability Zone X receives 50% traffic and four instances in Availability Zone Y receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone X receives 20% traffic and four instances in Availability Zone Y receive 20% traffic each</p>",
          "<p>With cross-zone load balancing enabled, one instance in Availability Zone X receives no traffic and four instances in Availability Zone Y receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone X receives 50% traffic and four instances in Availability Zone Y receive 12.5% traffic each</p>",
          "<p>With cross-zone load balancing enabled, one instance in Availability Zone X receives 20% traffic and four instances in Availability Zone Y receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone X receives no traffic and four instances in Availability Zone Y receive 25% traffic each</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "Design Solutions for Organizational Complexity",
      "question_plain": "A company uses Elastic Load Balancing to distribute traffic across multiple Amazon EC2 instances. Auto Scaling groups start and stop Amazon EC2 machines based on the number of incoming requests. The company has recently started operations in a new AWS Region and is setting up an Application Load Balancer for its fleet of EC2 instances spread across two Availability Zones, with one instance as a target in Availability Zone X and four instances as targets in Availability Zone Y. The company is doing benchmarking for server performance in the new Region for the case when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled.\n\nAs a Solutions Architect Professional, which of the following traffic distribution outcomes would you identify as correct?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141732,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A financial services company has multiple AWS accounts hosting its portfolio of IT applications that serve the company's retail and enterprise customers. A CloudWatch Logs agent is installed on each of the EC2 instances running these IT applications. The company wants to aggregate all security events in a centralized AWS account dedicated to log storage. The centralized operations team at the company needs to perform near-real-time gathering and collating events across multiple AWS accounts.</p>\n\n<p>As a Solutions Architect Professional, which of the following solutions would you suggest to meet these requirements?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up Kinesis Data Firehose in the logging account and then subscribe the delivery stream to CloudWatch Logs streams in each application AWS account via subscription filters. Persist the log data in an Amazon S3 bucket inside the logging AWS account</strong></p>\n\n<p>You can configure Amazon Kinesis Data Firehose to aggregate and collate CloudWatch Logs from different AWS accounts and receive their log events in a centralized logging AWS Account (this is known as cross-account data sharing) by using a CloudWatch Logs destination and then creating a Subscription Filter. This log event data can be read from a centralized Amazon Kinesis Firehose delivery stream to perform downstream processing and analysis.</p>\n\n<p>You can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis or Amazon Kinesis Data Firehose stream (this is known as cross-account data sharing). You can use a subscription filter with Kinesis Streams, Lambda, or Kinesis Data Firehose. Logs that are sent to a receiving service through a subscription filter are Base64 encoded and compressed with the gzip format.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a new IAM role in each application AWS account with permissions to view CloudWatch Logs. Create a Lambda function to assume this new role and perform an hourly export of each AWS account's CloudWatch Logs data to an S3 bucket in the centralized logging AWS account</strong> - As the Lambda function is performing an hourly export, so it's not a near-real time soluton. In addition, Lambda is not the right choice to build a high volume and high-velocity streaming solution which is better handled by using the Kinesis Family of services.</p>\n\n<p><strong>Set up CloudWatch Logs agents to publish data to a Kinesis Data Firehose stream in the centralized logging AWS account. Create a Lambda function to read messages from the stream and push messages to Kinesis Data Firehose and then store the data in S3</strong> - The CloudWatch Logs agent (on the path to deprecation) supports the collection of logs from only servers running Linux. It is recommended to use the unified CloudWatch agent. It enables you to collect both logs and advanced metrics with one agent. It offers support across operating systems, including servers running Windows Server. This agent also provides better performance. CloudWatch Logs agent cannot publish data to a Kinesis Data Firehose stream, so this option is incorrect.</p>\n\n<p><strong>Set up CloudWatch Logs streams in each application AWS account to forward events to CloudWatch Logs in the centralized logging AWS account. In the centralized logging AWS account, subscribe a Kinesis Data Firehose stream to Amazon EventBridge events and further use the Firehose stream to store the log data in S3</strong> - You can use a subscription filter with Kinesis Streams, Lambda, or Kinesis Data Firehose. So you cannot just forward events directly to CloudWatch Logs in another account. In addition, Kinesis Data Firehose stream cannot subscribe to EventBridge events, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/\">https://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/</a></p>\n",
        "answers": [
          "<p>Set up a new IAM role in each application AWS account with permissions to view CloudWatch Logs. Create a Lambda function to assume this new role and perform an hourly export of each AWS account's CloudWatch Logs data to an S3 bucket in the centralized logging AWS account</p>",
          "<p>Set up CloudWatch Logs agents to publish data to a Kinesis Data Firehose stream in the centralized logging AWS account. Create a Lambda function to read messages from the stream and push messages to Kinesis Data Firehose and then store the data in S3</p>",
          "<p>Set up Kinesis Data Firehose in the logging account and then subscribe the delivery stream to CloudWatch Logs streams in each application AWS account via subscription filters. Persist the log data in an Amazon S3 bucket inside the logging AWS account</p>",
          "<p>Set up CloudWatch Logs streams in each application AWS account to forward events to CloudWatch Logs in the centralized logging AWS account. In the centralized logging AWS account, subscribe a Kinesis Data Firehose stream to Amazon EventBridge events and further use the Firehose stream to store the log data in S3</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "Continuous Improvement for Existing Solutions",
      "question_plain": "A financial services company has multiple AWS accounts hosting its portfolio of IT applications that serve the company's retail and enterprise customers. A CloudWatch Logs agent is installed on each of the EC2 instances running these IT applications. The company wants to aggregate all security events in a centralized AWS account dedicated to log storage. The centralized operations team at the company needs to perform near-real-time gathering and collating events across multiple AWS accounts.\n\nAs a Solutions Architect Professional, which of the following solutions would you suggest to meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141730,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>The engineering team at a retail company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection.</p>\n\n<p>Which of the following options represents the MOST optimal solution with the LEAST infrastructure set up required for provisioning the end to end connection?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:\n<strong>Use AWS Direct Connect along with a site-to-site VPN to establish a connection between the data center and AWS Cloud</strong></p>\n\n<p>AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations.</p>\n\n<p>With AWS Direct Connect plus VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections.\nThis solution combines the AWS managed benefits of the VPN solution with low latency, increased bandwidth, more consistent benefits of the AWS Direct Connect solution, and an end-to-end, secure IPsec connection. Therefore, AWS Direct Connect plus VPN is the correct solution for this use-case.</p>\n\n<p>AWS Direct Connect Plus VPN:\n<img src=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/images/image10.png\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html</a></p>\n\n<p>Incorrect options:\n<strong>Use site-to-site VPN to establish a connection between the data center and AWS Cloud</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.</p>\n\n<p>However, Site-to-site VPN cannot provide low latency and high throughput connection, therefore this option is ruled out.</p>\n\n<p><strong>Use VPC transit gateway to establish a connection between the data center and AWS Cloud</strong> - A transit gateway is a network transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. A transit gateway by itself cannot establish a low latency and high throughput connection between a data center and AWS Cloud. Hence this option is incorrect.</p>\n\n<p><strong>Use AWS Direct Connect to establish a connection between the data center and AWS Cloud</strong> - AWS Direct Connect by itself cannot provide an encrypted connection between a data center and AWS Cloud, so this option is ruled out.</p>\n\n<p>References:\n<a href=\"https://aws.amazon.com/directconnect/\">https://aws.amazon.com/directconnect/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html</a></p>\n",
        "answers": [
          "<p>Use AWS Direct Connect to establish a connection between the data center and AWS Cloud</p>",
          "<p>Use site-to-site VPN to establish a connection between the data center and AWS Cloud</p>",
          "<p>Use AWS Direct Connect along with a site-to-site VPN to establish a connection between the data center and AWS Cloud</p>",
          "<p>Use VPC transit gateway to establish a connection between the data center and AWS Cloud</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "Continuous Improvement for Existing Solutions",
      "question_plain": "The engineering team at a retail company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection.\n\nWhich of the following options represents the MOST optimal solution with the LEAST infrastructure set up required for provisioning the end to end connection?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141728,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A social learning platform allows students to connect with other students as well as experts and professionals from academic, research institutes and industry. The engineering team at the company manages 5 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for PostgreSQL DB cluster. As an AWS Certified Solutions Architect Professional, you have been asked to make the database cluster resilient from a disaster recovery perspective.</p>\n\n<p>Which of the following features will help you prepare for database disaster recovery? (Select two)</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Use cross-Region Read Replicas</strong></p>\n\n<p>In addition to using Read Replicas to reduce the load on your source DB instance, you can also use Read Replicas to implement a DR solution for your production DB environment. If the source DB instance fails, you can promote your Read Replica to a standalone source server. Read Replicas can also be created in a different Region than the source database. Using a cross-Region Read Replica can help ensure that you get back up and running if you experience a regional availability issue.</p>\n\n<p><strong>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single or multiple AWS Region(s)</strong></p>\n\n<p>Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology.</p>\n\n<p>The automated backup feature of Amazon RDS enables point-in-time recovery for your database instance. Amazon RDS will backup your database and transaction logs and store both for a user-specified retention period. If itâ€™s a Multi-AZ configuration, backups occur on the standby to reduce I/O impact on the primary. Amazon RDS supports single Region or cross-Region automated backups.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use RAID 1 configuration for the RDS DB cluster</strong> - This option has been added as a distractor. RAID configuration options can only be used for EC2 instanceâ€“hosted databases. By using EBS storage volumes with EC2 instances, you can configure volumes with any RAID levels. For example, for greater I/O performance, you can opt for RAID 0, which can stripe multiple volumes together. RAID 1 can be used for data redundancy because it mirrors two volumes together.</p>\n\n<p><strong>Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</strong> - Amazon RDS Provisioned IOPS Storage is an SSD-backed storage option designed to deliver fast, predictable, and consistent I/O performance. This storage type enhances the performance of the RDS database, but this isn't a disaster recovery option.</p>\n\n<p><strong>Use database cloning feature of the RDS DB cluster</strong> - This option has been added as a distractor. Database cloning is only available for Aurora and not for RDS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/\">https://aws.amazon.com/rds/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2021/07/amazon-rds-cross-region-automated-backups-regional-expansion/\">https://aws.amazon.com/about-aws/whats-new/2021/07/amazon-rds-cross-region-automated-backups-regional-expansion/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/best-storage-practices-for-running-production-workloads-on-hosted-databases-with-amazon-rds-or-amazon-ec2/\">https://aws.amazon.com/blogs/database/best-storage-practices-for-running-production-workloads-on-hosted-databases-with-amazon-rds-or-amazon-ec2/</a></p>\n",
        "answers": [
          "<p>Use cross-Region Read Replicas</p>",
          "<p>Use RAID 1 configuration for the RDS DB cluster</p>",
          "<p>Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</p>",
          "<p>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single or multiple AWS Region(s)</p>",
          "<p>Use database cloning feature of the RDS DB cluster</p>"
        ]
      },
      "correct_response": ["a", "d"],
      "section": "Continuous Improvement for Existing Solutions",
      "question_plain": "A social learning platform allows students to connect with other students as well as experts and professionals from academic, research institutes and industry. The engineering team at the company manages 5 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for PostgreSQL DB cluster. As an AWS Certified Solutions Architect Professional, you have been asked to make the database cluster resilient from a disaster recovery perspective.\n\nWhich of the following features will help you prepare for database disaster recovery? (Select two)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141726,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A team uses an Amazon S3 bucket to store the client data. After updating the S3 bucket with a few file deletes and some new file additions, the team has just realized that these changes have not been propagated to the AWS Storage Gateway file share.</p>\n\n<p>What is the underlying issue? Which method can be used to resolve it?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Storage Gateway doesn't automatically update the cache when you upload a file directly to Amazon S3. Perform a <code>RefreshCache</code> operation to see the changes on the file share</strong></p>\n\n<p>Storage Gateway updates the file share cache automatically when you write files to the cache locally using the file share. However, Storage Gateway doesn't automatically update the cache when you upload a file directly to Amazon S3. When you do this, you must perform a <code>RefreshCache</code> operation to see the changes on the file share. If you have more than one file share, then you must run the <code>RefreshCache</code> operation on each file share.</p>\n\n<p>You can refresh the cache using the Storage Gateway console and the AWS Command Line Interface (AWS CLI).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Uploading files from your file gateway to Amazon S3 when S3 Versioning is enabled results in cache update issues. Disable versioning on the S3 bucket</strong> - Carefully consider the use of S3 Versioning and Cross-Region Replication (CRR) in Amazon S3 when you're uploading data from your file gateway. Uploading files from your file gateway to Amazon S3 when S3 Versioning is enabled results in at least two versions of an S3 object. This option is not relevant to the given issue and has just been added as a distractor.</p>\n\n<p><strong>Storage Gateway doesn't automatically update the cache when you upload a file directly to Amazon S3. Perform a <code>ResetCache</code> operation to see the changes on the file share</strong> - 'ResetCache', resets all cache disks that have encountered an error, and make the disks available for reconfiguration as cache storage. When a cache is reset, the gateway loses its cache storage. At this point, you can reconfigure the disks as cache disks. This operation is only supported in the cached volume and tape gateway types.</p>\n\n<p><strong>Configure correct permissions in Amazon S3 bucket policy to allow automatic refresh of cache</strong> - This statement is incorrect and has just been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/filegateway/latest/files3/GettingStartedCreateFileShare.html\">https://docs.aws.amazon.com/filegateway/latest/files3/GettingStartedCreateFileShare.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/storage-gateway-s3-changes-not-showing/\">https://aws.amazon.com/premiumsupport/knowledge-center/storage-gateway-s3-changes-not-showing/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/filegateway/latest/files3/refresh-cache.html\">https://docs.aws.amazon.com/filegateway/latest/files3/refresh-cache.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html\">https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html</a></p>\n",
        "answers": [
          "<p>Uploading files from your file gateway to Amazon S3 when S3 Versioning is enabled results in cache update issues. Disable versioning on the S3 bucket</p>",
          "<p>Storage Gateway doesn't automatically update the cache when you upload a file directly to Amazon S3. Perform a <code>ResetCache</code> operation to see the changes on the file share</p>",
          "<p>Configure correct permissions in Amazon S3 bucket policy to allow automatic refresh of cache</p>",
          "<p>Storage Gateway doesn't automatically update the cache when you upload a file directly to Amazon S3. Perform a <code>RefreshCache</code> operation to see the changes on the file share</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "Design for New Solutions",
      "question_plain": "A team uses an Amazon S3 bucket to store the client data. After updating the S3 bucket with a few file deletes and some new file additions, the team has just realized that these changes have not been propagated to the AWS Storage Gateway file share.\n\nWhat is the underlying issue? Which method can be used to resolve it?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141724,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>An Amazon Simple Storage Service (Amazon S3) bucket has been configured to host a static website. While using the S3 static website endpoint, the testing team has complained that they are receiving access denied error for this website.</p>\n\n<p>What are the key points to consider while configuring an S3 bucket as a static website? (Select two)</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Correct options:</p>\n\n<p><strong>Objects can't be encrypted by AWS Key Management Service (AWS KMS)</strong> - AWS KMS doesn't support anonymous requests. As a result, any Amazon S3 bucket that allows anonymous or public access will not apply to objects that are encrypted with AWS KMS. You must remove KMS encryption from the objects that you want to serve using the Amazon S3 static website endpoint. Instead of using AWS KMS encryption, use AES-256 to encrypt your objects.</p>\n\n<p><strong>The AWS account that owns the bucket must also own the object</strong> - To allow public read access to objects, the AWS account that owns the bucket must also own the objects. A bucket or object is owned by the account of the AWS Identity and Access Management (IAM) identity that created the bucket or object. The object-ownership requirement applies to public read access granted by a bucket policy. It doesn't apply to public read access granted by the object's access control list (ACL).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Objects in the bucket must be publicly accessible. S3 bucket policy must allow access to the s3:GetObject and s3:Put Object actions</strong> - Objects in the bucket must indeed be publicly accessible. S3 bucket policy needs to just allow access to the s3:GetObject action for public access of objects in S3.</p>\n\n<p><strong>Amazon S3 Block Public Access must be disabled at the bucket level even though it is already disabled at the account level</strong> - Amazon S3 Block Public Access settings can apply to individual buckets or AWS accounts. Confirm that there aren't any Amazon S3 Block Public Access settings applied to either your S3 bucket or AWS account. These settings can override permissions that allow public read access. There is no need to disable the 'Block Public Access' feature at the bucket level even though it is already disabled at the account level.</p>\n\n<p><strong>Amazon S3 static website endpoint needs to support both publicly and privately accessible content</strong> - This statement is incorrect. Amazon S3 static website endpoint supports only publicly accessible content.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-static-website-endpoint-error/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-static-website-endpoint-error/</a></p>\n",
        "answers": [
          "<p>Objects in the bucket must be publicly accessible. S3 bucket policy must allow access to the s3:GetObject and s3:Put Object actions</p>",
          "<p>Objects can't be encrypted by AWS Key Management Service (AWS KMS)</p>",
          "<p>The AWS account that owns the bucket must also own the object</p>",
          "<p>Amazon S3 Block Public Access must be disabled at the bucket level even though it is already disabled at the account level</p>",
          "<p>Amazon S3 static website endpoint needs to support both publicly and privately accessible content</p>"
        ]
      },
      "correct_response": ["b", "c"],
      "section": "Design for New Solutions",
      "question_plain": "An Amazon Simple Storage Service (Amazon S3) bucket has been configured to host a static website. While using the S3 static website endpoint, the testing team has complained that they are receiving access denied error for this website.\n\nWhat are the key points to consider while configuring an S3 bucket as a static website? (Select two)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141722,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A data analytics company uses Amazon S3 as the data lake to store the input data that is ingested from the IoT field devices on an hourly basis. The ingested data has attributes such as the device type, ID of the device, the status of the device, the timestamp of the event, the source IP address, etc. The data runs into millions of records per day and the company wants to run complex analytical queries on this data daily for product improvements for each device type.</p>\n\n<p>Which is the most optimal way to save this data to get the best performance from the millions of data points processed daily?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the data in Apache ORC, partitioned by date and sorted by device type of the device</strong> - Apache Parquet and ORC are columnar storage formats that are optimized for fast retrieval of data and used in AWS analytical applications.</p>\n\n<p>By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but that is loaded only once per day, might partition by a data source identifier and date.</p>\n\n<p>For the given use case, as the company does daily analysis, so it only needs to look at the data generated for a given date. Hence partitioning by date offers significant performance and cost advantages. Since the company also wants to analyze product improvements for each device type, it is better to keep the data sorted by device type, so it allows for faster query execution.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the data in Apache Parquet, partitioned by device type and sorted by date</strong> -  Apache Parquet is a columnar storage format that is optimized for fast retrieval of data and used in AWS analytical applications. However, partitioning by device type is incorrect for this use case, and partitioning by date is optimal.</p>\n\n<p><strong>Store the data in compressed .csv, partitioned by date and sorted by the status of the device</strong></p>\n\n<p><strong>Store the data in compressed .csv, partitioned by date and sorted by device type</strong></p>\n\n<p>Both the above options are not columnar storage formats, they are row-based formats that are not optimal for big data retrievals for complex analytical queries.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a></p>\n",
        "answers": [
          "<p>Store the data in compressed .csv, partitioned by date and sorted by the status of the device</p>",
          "<p>Store the data in compressed .csv, partitioned by date and sorted by device type</p>",
          "<p>Store the data in Apache Parquet, partitioned by device type and sorted by date</p>",
          "<p>Store the data in Apache ORC, partitioned by date and sorted by device type of the device</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "Continuous Improvement for Existing Solutions",
      "question_plain": "A data analytics company uses Amazon S3 as the data lake to store the input data that is ingested from the IoT field devices on an hourly basis. The ingested data has attributes such as the device type, ID of the device, the status of the device, the timestamp of the event, the source IP address, etc. The data runs into millions of records per day and the company wants to run complex analytical queries on this data daily for product improvements for each device type.\n\nWhich is the most optimal way to save this data to get the best performance from the millions of data points processed daily?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141720,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>Recently, an Amazon CloudFront distribution has been configured with an Amazon S3 bucket as the origin. However, users are getting an HTTP 307 Temporary Redirect response from Amazon S3.</p>\n\n<p>What could be the reason for this behavior and how will you resolve the issue? (Select two)</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Correct options:</p>\n\n<p><strong>When a new Amazon S3 bucket is created, it takes up to 24 hours before the bucket name propagates across all AWS Regions</strong></p>\n\n<p><strong>CloudFront by default, forwards the requests to the default S3 endpoint. Change the origin domain name of the distribution to include the Regional endpoint of the bucket</strong></p>\n\n<p>After you create an Amazon S3 bucket, up to 24 hours can pass before the bucket name propagates across all AWS Regions. During this time, you might receive the 307 Temporary Redirect response for requests to Regional endpoints that aren't in the same Region as your bucket.</p>\n\n<p>To avoid the 307 Temporary Redirect response, send requests only to the Regional endpoint in the same Region as your S3 bucket. If you're using an Amazon CloudFront distribution with an Amazon S3 origin, CloudFront forwards requests to the default S3 endpoint ( s3.amazonaws.com). The default S3 endpoint is in the us-east-1 Region. If you must access Amazon S3 within the first 24 hours of creating the bucket, you can change the origin domain name of the distribution. The domain name must include the Regional endpoint of the bucket. For example, if the bucket is in us-west-2, you can change the origin domain name from awsexamplebucketname.s3.amazonaws.com to awsexamplebucket.s3.us-west-2.amazonaws.com.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Cross-Region replication for the S3 bucket so that CloudFront can retrieve the data immediately after the creation of the bucket</strong> - S3 Cross-Region Replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions. CRR can help you do the following - meet compliance requirements, minimize latency and increase operational efficiency. CRR however, cannot resolve the HTTP 307 error.</p>\n\n<p><strong>Configure CloudFront <code>Cache-Control</code> and <code>Expires</code> headers to a value of zero, to fetch new objects immediately from the S3 bucket</strong> - You can use the <code>Cache-Control</code> and <code>Expires</code> headers to control how long objects stay in the CloudFront cache. This option has been added as a distractor and is unrelated to the HTTP 307 error.</p>\n\n<p><strong>Enable Amazon S3 Transfer Acceleration to help CloudFront access data faster over long distances from the S3 bucket</strong> - Amazon S3 Transfer Acceleration is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration is designed to optimize transfer speeds from across the world into S3 buckets. This option has been added as a distractor and is unrelated to the HTTP 307 error.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-http-307-response/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-http-307-response/</a></p>\n",
        "answers": [
          "<p>When a new Amazon S3 bucket is created, it takes up to 24 hours before the bucket name propagates across all AWS Regions</p>",
          "<p>Enable Cross-Region replication for the S3 bucket so that CloudFront can retrieve the data immediately after the creation of the bucket</p>",
          "<p>Configure CloudFront <code>Cache-Control</code> and <code>Expires</code> headers to a value of zero, to fetch new objects immediately from the S3 bucket</p>",
          "<p>Enable Amazon S3 Transfer Acceleration to help CloudFront access data faster over long distances from the S3 bucket</p>",
          "<p>CloudFront by default, forwards the requests to the default S3 endpoint. Change the origin domain name of the distribution to include the Regional endpoint of the bucket</p>"
        ]
      },
      "correct_response": ["a", "e"],
      "section": "Continuous Improvement for Existing Solutions",
      "question_plain": "Recently, an Amazon CloudFront distribution has been configured with an Amazon S3 bucket as the origin. However, users are getting an HTTP 307 Temporary Redirect response from Amazon S3.\n\nWhat could be the reason for this behavior and how will you resolve the issue? (Select two)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141718,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>The security team at a company has put forth a requirement to track the external IP address when a customer or a third party uploads files to the Amazon Simple Storage Service (Amazon S3) bucket owned by the company.</p>\n\n<p>How will you track the external IP address used for each upload? (Select two)</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Correct options:</p>\n\n<p><strong>Enable Amazon S3 server access logging to capture all bucket-level and object-level events</strong></p>\n\n<p><strong>Enable AWS CloudTrail data events to enable object-level logging for S3 bucket</strong></p>\n\n<p>To find the IP addresses for object-level requests to Amazon S3 (uploads and downloads), you must first enable one of the following logging methods:</p>\n\n<ol>\n<li><p>Amazon S3 server access logging captures all bucket-level and object-level events. These logs use a format similar to Apache web server logs. After you enable server access logging, review the logs to find the IP addresses used with each upload to your bucket.</p></li>\n<li><p>AWS CloudTrail data events capture the last 90 days of bucket-level events (for example, PutBucketPolicy and DeleteBucketPolicy), and you can enable object-level logging. These logs use a JSON format. After you enable object-level logging with data events, review the logs to find the IP addresses used with each upload to your bucket.  It might take a few hours for AWS CloudTrail to start creating logs.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable VPC Flow Logs to capture all object-level events occurring on the S3 bucket</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC, so it does not apply to S3. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3.</p>\n\n<p><strong>Enable AWS Systems Manager Agent (SSM Agent) that writes information about executions, commands, scheduled actions on all AWS resources</strong> - AWS Systems Manager Agent (SSM Agent) writes information about executions, commands, scheduled actions, errors, and health statuses to log files on each managed node. You can view log files by manually connecting to a managed node, or you can automatically send logs to Amazon CloudWatch Logs. AWS Systems Manager Agent (SSM Agent) is Amazon software that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, and on-premises servers and virtual machines (VMs), so it does not apply to S3.</p>\n\n<p><strong>CloudWatch Logs centrally maintain the logs from all of your systems, applications, and AWS services that you use. Use these logs to capture the IP address at the object level for the S3 bucket</strong> - You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources. CloudWatch Logs enables you to see all of your logs, regardless of their source, as a single and consistent flow of events ordered by time, and you can query them and sort them based on other dimensions, group them by specific fields, create custom computations with a powerful query language, and visualize log data in dashboards. CloudWatch Logs cannot be used to track the external IP address used for uploads to S3.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/external-ip-address-s3-bucket/\">https://aws.amazon.com/premiumsupport/knowledge-center/external-ip-address-s3-bucket/</a></p>\n",
        "answers": [
          "<p>Enable Amazon S3 server access logging to capture all bucket-level and object-level events</p>",
          "<p>Enable AWS CloudTrail data events to enable object-level logging for S3 bucket</p>",
          "<p>Enable VPC Flow Logs to capture all object-level events occurring on the S3 bucket</p>",
          "<p>Enable AWS Systems Manager Agent (SSM Agent) that writes information about executions, commands, scheduled actions on all AWS resources</p>",
          "<p>CloudWatch Logs centrally maintain the logs from all of your systems, applications, and AWS services that you use. Use these logs to capture the IP address at the object level for the S3 bucket</p>"
        ]
      },
      "correct_response": ["a", "b"],
      "section": "Continuous Improvement for Existing Solutions",
      "question_plain": "The security team at a company has put forth a requirement to track the external IP address when a customer or a third party uploads files to the Amazon Simple Storage Service (Amazon S3) bucket owned by the company.\n\nHow will you track the external IP address used for each upload? (Select two)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141716,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>An Amazon S3 bucket is shared by three different teams (managing their own separate AWS accounts) for document uploads. Initially, the S3 bucket settings were set to default. Later, the bucket sees the following updates:</p>\n\n<p>After week 1, S3 Object Ownership bucket-level settings were used and all Access Control Lists (ACLs) were disabled. The three teams uploaded their documents to the shared bucket with this new setting.</p>\n\n<p>After week 2, S3 bucket level settings were again set back to default and the ACLs were enabled once more</p>\n\n<p>What is the outcome of these action(s) on the documents uploaded after week 1 and what are the key points of consideration for future S3 bucket configurations? (Select two)</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Correct options:</p>\n\n<p><strong>You, as the bucket owner, still own any objects that were written to the bucket while the <code>bucket owner enforced</code> setting was applied. These objects are not owned by the <code>object writer</code>, even if you re-enable ACLs</strong></p>\n\n<p><strong>If you used object ACLs for permissions management before you applied the <code>bucket owner enforced</code> setting and you didn't migrate these object ACL permissions to your bucket policy after you re-enable ACLs, these permissions are restored</strong> -</p>\n\n<p>You can re-enable ACLs by changing from the bucket owner-enforced setting to another Object Ownership setting at any time. If you used object ACLs for permissions management before you applied the bucket owner-enforced setting and you didn't migrate these object ACL permissions to your bucket policy, after you re-enable ACLs, these permissions are restored. Additionally, objects written to the bucket while the bucket owner enforced setting was applied are still owned by the bucket owner.</p>\n\n<p>For example, if you change from the bucket owner-enforced setting back to object writer, you, as the bucket owner, no longer own and have full control over objects that were previously owned by other AWS accounts. Instead, the uploading accounts again own these objects. Objects owned by other accounts use ACLs for permissions, so you can't use policies to grant permissions to these objects. However, you, as the bucket owner, still own any objects that were written to the bucket while the bucket owner-enforced setting was applied. These objects are not owned by the object writer, even if you re-enable ACLs.</p>\n\n<p>Changes introduced by disabling ACLs:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q20-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If you used object ACLs for permissions management before you applied the <code>bucket owner enforced</code> setting and you didn't migrate these object ACL permissions to your bucket policy after you re-enable ACLs, these permissions are not restored</strong> - As explained above, the permissions are restored for this scenario. So this option is incorrect.</p>\n\n<p><strong>To simplify permissions management and auditing, use the <code>Bucket owner preferred</code> S3 bucket setting</strong> - This statement is incorrect. AWS recommends that you disable ACLs by choosing the <code>bucket owner enforced</code> setting and use your bucket policy to share data with users outside of your account as needed. This approach simplifies permissions management and auditing. You can disable ACLs on both newly created and already existing buckets.</p>\n\n<p><strong>You, as the bucket owner, will not own the objects that were written to the bucket while the <code>bucket owner enforced setting</code> was applied. These objects will again be owned by the object writer, when you re-enable the ACLs</strong> - This statement is incorrect. You, as the bucket owner, still own any objects that were written to the bucket while the bucket owner-enforced setting was applied. These objects are not owned by the object writer, even if you re-enable ACLs.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html</a></p>\n",
        "answers": [
          "<p>To simplify permissions management and auditing, use the <code>Bucket owner preferred</code> S3 bucket setting</p>",
          "<p>If you used object ACLs for permissions management before you applied the <code>bucket owner enforced</code> setting and you didn't migrate these object ACL permissions to your bucket policy after you re-enable ACLs, these permissions are not restored</p>",
          "<p>You, as the bucket owner, will not own the objects that were written to the bucket while the <code>bucket owner enforced setting</code> was applied. These objects will again be owned by the object writer when you re-enable the ACLs</p>",
          "<p>You, as the bucket owner, still own any objects that were written to the bucket while the <code>bucket owner enforced</code> setting was applied. These objects are not owned by the <code>object writer</code>, even if you re-enable ACLs</p>",
          "<p>If you used object ACLs for permissions management before you applied the <code>bucket owner enforced</code> setting and you didn't migrate these object ACL permissions to your bucket policy after you re-enable ACLs, these permissions are restored</p>"
        ]
      },
      "correct_response": ["d", "e"],
      "section": "Design Solutions for Organizational Complexity",
      "question_plain": "An Amazon S3 bucket is shared by three different teams (managing their own separate AWS accounts) for document uploads. Initially, the S3 bucket settings were set to default. Later, the bucket sees the following updates:\n\nAfter week 1, S3 Object Ownership bucket-level settings were used and all Access Control Lists (ACLs) were disabled. The three teams uploaded their documents to the shared bucket with this new setting.\n\nAfter week 2, S3 bucket level settings were again set back to default and the ACLs were enabled once more\n\nWhat is the outcome of these action(s) on the documents uploaded after week 1 and what are the key points of consideration for future S3 bucket configurations? (Select two)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141714,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A bioinformatics company leverages multiple open source tools to manage data analysis workflows running on its on-premises servers to process biological data which is generated and stored on a Network Attached Storage (NAS). The existing workflow receives around 100 GB of input biological data for each job run and individual jobs can take several hours to process the data. The CTO at the company wants to re-architect its proprietary analytics workflow on AWS to meet the workload demands and reduce the turnaround time from months to days. The company has provisioned a high-speed AWS Direct Connect connection. The final result needs to be stored in Amazon S3. The company is expecting approximately 20 job requests each day.</p>\n\n<p>Which of the following options would you recommend for the given use case?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage AWS DataSync to transfer the biological data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow for orchestrating an AWS Batch job that processes the biological data</strong></p>\n\n<p>AWS DataSync is an online data movement and discovery service that simplifies and accelerates data migrations to AWS as well as moving data between on-premises storage, edge locations, other clouds, and AWS Storage. You can use DataSync to migrate active data to AWS, archive data to free up on-premises storage capacity, replicate data to AWS for business continuity, or transfer data to the cloud for analysis and processing.</p>\n\n<p>For data transfer between on-premises and AWS Storage services, a single DataSync task is capable of fully utilizing a 10 Gbps network link. Since each workflow job consumes around 100GB of data and the company sees approximately 20 runs every day, DataSync can easily handle such active data transfer workloads. For the given use case, you can then configure an S3 event to trigger an AWS Lambda function that starts an AWS Step Functions workflow for orchestrating an AWS Batch job that processes the biological data.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q19-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/datasync/faqs/\">https://aws.amazon.com/datasync/faqs/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q19-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/datasync/faqs/\">https://aws.amazon.com/datasync/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage AWS Data Pipeline to transfer the biological data to Amazon S3. Use S3 events to trigger an AWS Step Functions workflow for orchestrating an AWS Batch job that processes the biological data</strong> - You cannot trigger an AWS Step Function directly from an S3 event, so this option is incorrect.</p>\n\n<p><strong>Leverage AWS Data Pipeline to transfer the biological data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances to process the biological data</strong> - You cannot trigger an Amazon EC2 Auto Scaling group directly from an S3 event, so this option is incorrect.</p>\n\n<p><strong>Leverage AWS Storage Gateway file gateway to transfer the biological data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow for orchestrating an AWS Batch job that processes the biological data</strong> - You should use AWS DataSync to migrate existing or active data to Amazon S3 and use the File Gateway configuration of AWS Storage Gateway to retain access to the migrated data and for ongoing updates from your on-premises file-based applications. Since the data processing workflow/application is being migrated from on-premises to AWS Cloud, you no longer have any on-premises applications that need to access the processed data from AWS Cloud. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/datasync/\">https://aws.amazon.com/datasync/</a></p>\n\n<p><a href=\"https://aws.amazon.com/datasync/faqs/\">https://aws.amazon.com/datasync/faqs/</a></p>\n",
        "answers": [
          "<p>Leverage AWS Data Pipeline to transfer the biological data to Amazon S3. Use S3 events to trigger an AWS Step Functions workflow for orchestrating an AWS Batch job that processes the biological data</p>",
          "<p>Leverage AWS Data Pipeline to transfer the biological data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances to process the biological data</p>",
          "<p>Leverage AWS Storage Gateway file gateway to transfer the biological data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow for orchestrating an AWS Batch job that processes the biological data</p>",
          "<p>Leverage AWS DataSync to transfer the biological data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow for orchestrating an AWS Batch job that processes the biological data</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "Accelerate Workload Migration and Modernization",
      "question_plain": "A bioinformatics company leverages multiple open source tools to manage data analysis workflows running on its on-premises servers to process biological data which is generated and stored on a Network Attached Storage (NAS). The existing workflow receives around 100 GB of input biological data for each job run and individual jobs can take several hours to process the data. The CTO at the company wants to re-architect its proprietary analytics workflow on AWS to meet the workload demands and reduce the turnaround time from months to days. The company has provisioned a high-speed AWS Direct Connect connection. The final result needs to be stored in Amazon S3. The company is expecting approximately 20 job requests each day.\n\nWhich of the following options would you recommend for the given use case?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141712,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>The development team at a company needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The team created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose. The team created the following IAM policy and attached it to an IAM role:</p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"key-policy-1\",\n  \"Statement\": [\n    {\n      \"Sid\": \"GetPut\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::ExampleBucket/*\"\n    },\n    {\n      \"Sid\": \"KMS\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"kms:Decrypt\",\n        \"kms:Encrypt\"\n      ],\n      \"Resource\": \"arn:aws:kms:us-west-1:111122223333:key/keyid-12345\"\n    }\n  ]\n}\n</code></pre>\n\n<p>The team was able to successfully get existing objects from the S3 bucket while testing. But any attempts to upload a new object resulted in an error. The error message stated that the action was forbidden.</p>\n\n<p>Which IAM policy action should be added to the IAM policy to resolve the error?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>kms:GenerateDataKey</strong></p>\n\n<p>GenerateDataKey returns a unique symmetric data key for use outside of AWS KMS. This operation returns a plaintext copy of the data key and a copy that is encrypted under a symmetric encryption KMS key that you specify. The bytes in the plaintext key are random; they are not related to the caller or the KMS key. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q18-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>kms:GetPublicKey</strong> - This option returns the public key of an asymmetric KMS key. Unlike the private key of an asymmetric KMS key, which never leaves AWS KMS unencrypted, callers with kms:GetPublicKey permission can download the public key of an asymmetric KMS key. It cannot be used for a client-side encryption mechanism.</p>\n\n<p><strong>kms:GetKeyPolicy</strong> - This option gets a key policy attached to the specified KMS key. It cannot be used for a client-side encryption mechanism.</p>\n\n<p><strong>kms:GetDataKey</strong> - This is a made-up option that serves as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GetKeyPolicy.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GetKeyPolicy.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GetPublicKey.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GetPublicKey.html</a></p>\n",
        "answers": [
          "<p>kms:GetPublicKey</p>",
          "<p>kms:GetKeyPolicy</p>",
          "<p>kms:GetDataKey</p>",
          "<p>kms:GenerateDataKey</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "Design for New Solutions",
      "question_plain": "The development team at a company needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The team created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose. The team created the following IAM policy and attached it to an IAM role:\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"key-policy-1\",\n  \"Statement\": [\n    {\n      \"Sid\": \"GetPut\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::ExampleBucket/*\"\n    },\n    {\n      \"Sid\": \"KMS\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"kms:Decrypt\",\n        \"kms:Encrypt\"\n      ],\n      \"Resource\": \"arn:aws:kms:us-west-1:111122223333:key/keyid-12345\"\n    }\n  ]\n}\n\n\nThe team was able to successfully get existing objects from the S3 bucket while testing. But any attempts to upload a new object resulted in an error. The error message stated that the action was forbidden.\n\nWhich IAM policy action should be added to the IAM policy to resolve the error?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141710,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company has its flagship application fronted by an Application Load Balancer that is targeting several EC2 Linux instances running in an Auto Scaling group in a private subnet. AWS Systems Manager Agent is installed on all the EC2 instances. The company recently released a new version of the application, however, some of the EC2 instances are now being marked as unhealthy and are being terminated, thereby causing the application to run at reduced capacity. You have been tasked to ascertain the root cause by analyzing Amazon CloudWatch logs that are collected from the application, but you find that the logs are inconclusive.</p>\n\n<p>Which of the following options would you propose to get access to an EC2 instance to troubleshoot the issue?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Suspend the Auto Scaling group's <code>Terminate</code> process. Use Session Manager to log in to an instance that is marked as unhealthy and analyze the system logs to figure out the root cause</strong></p>\n\n<p>The <code>Terminate</code> process removes instances from the Auto Scaling group when the group scales in, or when Amazon EC2 Auto Scaling chooses to terminate instances for other reasons, such as when an instance is terminated for exceeding its maximum lifetime duration or failing a health check. You need to suspend the <code>Terminate</code> process which will allow you to get access to the instance without it being terminated even if it is marked as unhealthy. You should note that another way to prevent Amazon EC2 Auto Scaling from terminating unhealthy instances, is to suspend the <code>ReplaceUnhealthy</code> process. You can then leverage the Session Manager to log in to the instance that is marked as unhealthy and analyze the system logs to figure out the root cause.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q17-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/terminating-instances.html#Using_ChangingDisableAPITermination\">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/terminating-instances.html#Using_ChangingDisableAPITermination</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q17-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/terminating-instances.html#Using_ChangingDisableAPITermination\">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/terminating-instances.html#Using_ChangingDisableAPITermination</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Suspend the Auto Scaling group's <code>HealthCheck</code> process. Use EC2 instance connect to log in to an instance that is marked as unhealthy and analyze the system logs to figure out the root cause</strong> - The <code>HealthCheck</code> process checks the health of the instances and marks an instance as unhealthy if Amazon EC2 or Elastic Load Balancing tells Amazon EC2 Auto Scaling that the instance is unhealthy. This process can override the health status of an instance that you set manually. If you suspend the <code>HealthCheck</code> process, then none of the instances would be marked as unhealthy. Therefore, you cannot suspend the <code>HealthCheck</code> process for the given use case, since you must identify the root cause behind some of the instances being marked as unhealthy.</p>\n\n<p><strong>Suspend the Auto Scaling group's <code>Launch</code> process. Use Session Manager to log in to an instance that is marked as unhealthy and analyze the system logs to figure out the root cause</strong> - The <code>Launch</code> process adds instances to the Auto Scaling group when the group scales out, or when Amazon EC2 Auto Scaling chooses to launch instances for other reasons, such as when it adds instances to a warm pool. Suspending the <code>Launch</code> process will not help in identifying the root cause behind some instances being marked as unhealthy as those instances would still be terminated.</p>\n\n<p><strong>Enable EC2 instance termination protection. Use Session Manager to log In to an instance that is marked as unhealthy and analyze the system logs to figure out the root cause</strong> - Enabling EC2 instance termination (DisableApiTermination attribute) does not prevent Amazon EC2 Auto Scaling from terminating an instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/terminating-instances.html#Using_ChangingDisableAPITermination\">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/terminating-instances.html#Using_ChangingDisableAPITermination</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html</a></p>\n",
        "answers": [
          "<p>Suspend the Auto Scaling group's <code>HealthCheck</code> process. Use EC2 instance connect to log in to an instance that is marked as unhealthy and analyze the system logs to figure out the root cause</p>",
          "<p>Suspend the Auto Scaling group's <code>Terminate</code> process. Use Session Manager to log in to an instance that is marked as unhealthy and analyze the system logs to figure out the root cause</p>",
          "<p>Suspend the Auto Scaling group's <code>Launch</code> process. Use Session Manager to log in to an instance that is marked as unhealthy and analyze the system logs to figure out the root cause</p>",
          "<p>Enable EC2 instance termination protection. Use Session Manager to log In to an instance that is marked as unhealthy and analyze the system logs to figure out the root cause</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "Design for New Solutions",
      "question_plain": "A company has its flagship application fronted by an Application Load Balancer that is targeting several EC2 Linux instances running in an Auto Scaling group in a private subnet. AWS Systems Manager Agent is installed on all the EC2 instances. The company recently released a new version of the application, however, some of the EC2 instances are now being marked as unhealthy and are being terminated, thereby causing the application to run at reduced capacity. You have been tasked to ascertain the root cause by analyzing Amazon CloudWatch logs that are collected from the application, but you find that the logs are inconclusive.\n\nWhich of the following options would you propose to get access to an EC2 instance to troubleshoot the issue?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141678,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A financial services company had a security incident recently and wants to review the security of its two-tier server architecture. The company wants to ensure that it follows the principle of least privilege while configuring the security groups for access between the EC2 instance-based app servers and RDS MySQL database servers. The security group for the EC2 instances as well as the security group for the MySQL database servers has no inbound and outbound rules configured currently.</p>\n\n<p>As an AWS Certified Solutions Architect Professional, which of the following options would you recommend to adhere to the given requirements? (Select two)</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an outbound rule in the security group for the EC2 instance app servers using TCP protocol on port 3306. Set the destination as the security group for the MySQL DB servers</strong></p>\n\n<p><strong>Create an inbound rule in the security group for the MySQL DB servers using TCP protocol on port 3306. Set the source as the security group for the EC2 instance app servers</strong></p>\n\n<p>A security group controls the traffic that is allowed to reach and leave the resources that it is associated with. For example, after you associate a security group with an EC2 instance, it controls the inbound and outbound traffic for the instance. Security groups are stateful. For example, if you send a request from an instance, the response traffic for that request is allowed to reach the instance regardless of the inbound security group rules. Responses to allowed inbound traffic are allowed to leave the instance, regardless of the outbound rules.</p>\n\n<p>When you first create a security group, it has no inbound rules. Therefore, no inbound traffic is allowed until you add inbound rules to the security group. When you first create a security group, it has an outbound rule that allows all outbound traffic from the resource. You can remove the rule and add outbound rules that allow specific outbound traffic only. If your security group has no outbound rules, no outbound traffic is allowed.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p>\n\n<p>For the given use case, you need to set up an outbound rule in the security group for the EC2 instance app servers using TCP protocol on port 3306 and then select the destination as the security group for the MySQL DB servers. Further, you need to set up an inbound rule in the security group for the MySQL DB servers using TCP protocol on port 3306 and then select the source as the security group for the EC2 instance app servers. This combination would let the request be initiated from the EC2 instances and allowed into the DB servers. Since the security groups are stateful, the response from the DB servers would be allowed out of the DB servers (even though no outbound rules are configured in the DB security group) and further into the EC2 instances (even though no inbound rules are configured in the EC2 instance security group)</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an outbound rule in the security group for the EC2 instance app servers using TCP protocol on the ephemeral port range. Set the destination as the security group for the MySQL DB servers</strong> - As explained above, you need to set up an outbound rule in the security group for the EC2 instance app servers using TCP protocol on port 3306 and NOT on the ephemeral port range because the MySQL DB is configured to process requests on port 3306. A common use-case for ephemeral ports: these are used in NACLs to handle response traffic. Consider a custom network ACL for a VPC that supports IPv4 only. It includes rules that allow HTTP and HTTPS traffic in (inbound rules 100 and 110). There's a corresponding outbound rule that enables responses to that inbound traffic (outbound rule 140, which covers ephemeral ports 32768-65535). So this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q1-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports</a></p>\n\n<p><strong>Create an outbound rule in the security group for the MySQL DB servers using TCP protocol on the ephemeral port range. Set the destination as the security group for the EC2 instance app servers</strong></p>\n\n<p><strong>Create an outbound rule in the security group for the MySQL DB servers using TCP protocol on port 3306. Set the destination as the security group for the EC2 instance app servers</strong></p>\n\n<p>There is no need to create an outbound rule in the security group for the MySQL DB servers either on the ephemeral port range or port 3306 since the security groups are stateful therefore the response from the DB servers would be allowed out of the DB servers. Hence both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports</a></p>\n",
        "answers": [
          "<p>Create an outbound rule in the security group for the EC2 instance app servers using TCP protocol on port 3306. Set the destination as the security group for the MySQL DB servers</p>",
          "<p>Create an inbound rule in the security group for the MySQL DB servers using TCP protocol on port 3306. Set the source as the security group for the EC2 instance app servers</p>",
          "<p>Create an outbound rule in the security group for the MySQL DB servers using TCP protocol on the ephemeral port range. Set the destination as the security group for the EC2 instance app servers</p>",
          "<p>Create an outbound rule in the security group for the EC2 instance app servers using TCP protocol on the ephemeral port range. Set the destination as the security group for the MySQL DB servers</p>",
          "<p>Create an outbound rule in the security group for the MySQL DB servers using TCP protocol on port 3306. Set the destination as the security group for the EC2 instance app servers</p>"
        ]
      },
      "correct_response": ["a", "b"],
      "section": "Continuous Improvement for Existing Solutions",
      "question_plain": "A financial services company had a security incident recently and wants to review the security of its two-tier server architecture. The company wants to ensure that it follows the principle of least privilege while configuring the security groups for access between the EC2 instance-based app servers and RDS MySQL database servers. The security group for the EC2 instances as well as the security group for the MySQL database servers has no inbound and outbound rules configured currently.\n\nAs an AWS Certified Solutions Architect Professional, which of the following options would you recommend to adhere to the given requirements? (Select two)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141706,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A multi-national company operates hundreds of AWS accounts and the CTO wants to rationalize the operational costs. The CTO has mandated a centralized process for purchasing new Reserved Instances (RIs) or modifying existing RIs. Whereas earlier the business units (BUs) would directly purchase or modify RIs in their own AWS accounts independently, now all BUs must be denied independent purchase and the BUs must submit requests to a dedicated central team for purchasing RIs.</p>\n\n<p>As an AWS Certified Solutions Architect Professional, which of the following solutions would you combine to enforce the new process most efficiently? (Select two)</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Correct options:</p>\n\n<p><strong>Make sure that all AWS accounts are assigned organizational units (OUs) within an AWS Organizations structure operating in all features mode</strong></p>\n\n<p>AWS Organizations has two available feature sets:</p>\n\n<p>All features â€“ This feature set is the preferred way to work with AWS Organizations, and it includes Consolidating Billing features. When you create an organization, enabling all features is the default. With all features enabled, you can use the advanced account management features available in AWS Organizations such as integration with supported AWS services and organization management policies. Policies in AWS Organizations enable you to apply additional types of management to the AWS accounts in your organization. You can use policies when all features are enabled in your organization. Service control policies (SCPs) offer central control over the maximum available permissions for all of the accounts in your organization.</p>\n\n<p>Consolidated Billing features â€“ All organizations support this subset of features, which provides basic management tools that you can use to centrally manage the accounts in your organization. You cannot leverage SCPs in this feature mode.</p>\n\n<p><strong>Set up a Service Control Policy (SCP) that contains a deny rule to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions. Attach the SCP to each organizational unit (OU) of the AWS Organizations structure</strong></p>\n\n<p>Service control policies (SCPs) are a type of organizational policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organizationâ€™s access control guidelines. SCPs alone are not sufficient to grant permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or the resources in your accounts to actually grant permissions. SCPs don't affect users or roles in the management account. They affect only the member accounts in your organization.</p>\n\n<p>For the given use case, you can set up an SCP that contains a deny rule to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Make sure that all AWS accounts are assigned organizational units (OUs) within an AWS Organizations structure operating in the consolidated billing features mode</strong> - You cannot leverage SCPs in this feature mode, so this option is incorrect.</p>\n\n<p><strong>Leverage AWS Config to notify on the attachment of an IAM policy that allows access to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions</strong> - AWS Config cannot prevent the attachment of an IAM policy that allows access to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions. So this option is incorrect for the given requirements.</p>\n\n<p><strong>Set up an IAM policy in each AWS account with a deny rule to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions</strong> - This is a cumbersome and inefficient solution to prevent each of the member AWS accounts from purchasing the RIs. This is not the best fit solution.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_support-all-features.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_support-all-features.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n",
        "answers": [
          "<p>Make sure that all AWS accounts are assigned organizational units (OUs) within an AWS Organizations structure operating in the consolidated billing features mode</p>",
          "<p>Make sure that all AWS accounts are assigned organizational units (OUs) within an AWS Organizations structure operating in all features mode</p>",
          "<p>Set up a Service Control Policy (SCP) that contains a deny rule to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions. Attach the SCP to each organizational unit (OU) of the AWS Organizations structure</p>",
          "<p>Leverage AWS Config to notify on the attachment of an IAM policy that allows access to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions</p>",
          "<p>Set up an IAM policy in each AWS account with a deny rule to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions</p>"
        ]
      },
      "correct_response": ["b", "c"],
      "section": "Design Solutions for Organizational Complexity",
      "question_plain": "A multi-national company operates hundreds of AWS accounts and the CTO wants to rationalize the operational costs. The CTO has mandated a centralized process for purchasing new Reserved Instances (RIs) or modifying existing RIs. Whereas earlier the business units (BUs) would directly purchase or modify RIs in their own AWS accounts independently, now all BUs must be denied independent purchase and the BUs must submit requests to a dedicated central team for purchasing RIs.\n\nAs an AWS Certified Solutions Architect Professional, which of the following solutions would you combine to enforce the new process most efficiently? (Select two)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141704,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A social media company has VPC Flow Logs enabled for its NAT gateway. The security team is seeing Action = ACCEPT for inbound traffic that comes from the public IP address 198.21.200.1 destined for a private EC2 instance. The team must determine whether the traffic represents unsolicited inbound connections from the internet. The first two octets of the VPC CIDR block are 205.1.</p>\n\n<p>Which of the following options can address this requirement?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Inspect the VPC Flow Logs using the CloudWatch console and select the log group that contains the NAT gateway's ENI and the EC2 instance's ENI. Leverage a query filter with the destination address set as <code>like 205.1</code> and the source address set as <code>like 198.21.200.1</code>. Execute the stats command to filter the sum of bytes transferred by the source address and the destination address</strong></p>\n\n<p>NAT gateways managed by AWS don't accept traffic initiated from the internet. However, there are two reasons why the information in your VPC flow logs might appear to indicate that inbound traffic is being accepted from the internet.</p>\n\n<p>1: Inbound internet traffic is permitted by your security group or network access control lists (ACL)</p>\n\n<p>VPC flow logs show inbound internet traffic as accepted if the traffic is permitted by your security group or network ACLs. If network ACLs attached to a NAT gateway donâ€™t explicitly deny traffic from the internet, then the traffic to the NAT gateway appears accepted. However, the traffic isn't actually accepted by the NAT gateway and is dropped. You can use just the first two octets in the search filter to analyze all network interfaces in the VPC, like so:</p>\n\n<pre><code>filter (dstAddr like 'xxx.xxx' and srcAddr like 'public IP')\n| stats sum(bytes) as bytesTransferred by srcAddr, dstAddr\n| limit 10\n</code></pre>\n\n<p>If the query results show traffic on the NAT gateway private IP from the public IP, but not traffic on other private IPs in the VPC. These results confirm that the incoming traffic was unsolicited.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q14-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway/\">https://aws.amazon.com/premiumsupport/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q14-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway/\">https://aws.amazon.com/premiumsupport/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Inspect the VPC Flow Logs using the CloudTrail console and select the log group that contains the NAT gateway's ENI and the EC2 instance's ENI. Leverage a query filter with the destination address set as <code>like 205.1</code> and the source address set as <code>like 198.21.200.1</code>. Execute the stats command to filter the sum of bytes transferred by the source address and the destination address</strong></p>\n\n<p><strong>Inspect the VPC Flow Logs using the CloudTrail console and select the log group that contains the NAT gateway's ENI and the EC2 instance's ENI. Leverage a query filter with the source address set as <code>like 205.1</code> and the destination address set as <code>like 198.21.200.1</code>. Execute the stats command to filter the sum of bytes transferred by the source address and the destination address</strong></p>\n\n<p>You cannot use the CloudTrail console to analyze the VPC Flow Logs. You need to use the CloudWatch console for this analysis. Therefore both these options are incorrect.</p>\n\n<p><strong>Inspect the VPC Flow Logs using the CloudWatch console and select the log group that contains the NAT gateway's ENI and the EC2 instance's ENI. Leverage a query filter with the source address set as <code>like 205.1</code> and the destination address set as <code>like 198.21.200.1</code>. Execute the stats command to filter the sum of bytes transferred by the source address and the destination address</strong> - For the query filter, you need to use the destination address set as <code>like 205.1</code> with the source address set as <code>like 198.21.200.1</code> and NOT vice-versa.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway/\">https://aws.amazon.com/premiumsupport/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway/</a></p>\n",
        "answers": [
          "<p>Inspect the VPC Flow Logs using the CloudWatch console and select the log group that contains the NAT gateway's ENI and the EC2 instance's ENI. Leverage a query filter with the destination address set as <code>like 205.1</code> and the source address set as <code>like 198.21.200.1</code>. Execute the stats command to filter the sum of bytes transferred by the source address and the destination address</p>",
          "<p>Inspect the VPC Flow Logs using the CloudTrail console and select the log group that contains the NAT gateway's ENI and the EC2 instance's ENI. Leverage a query filter with the destination address set as <code>like 205.1</code> and the source address set as <code>like 198.21.200.1</code>. Execute the stats command to filter the sum of bytes transferred by the source address and the destination address</p>",
          "<p>Inspect the VPC Flow Logs using the CloudTrail console and select the log group that contains the NAT gateway's ENI and the EC2 instance's ENI. Leverage a query filter with the source address set as <code>like 205.1</code> and the destination address set as <code>like 198.21.200.1</code>. Execute the stats command to filter the sum of bytes transferred by the source address and the destination address</p>",
          "<p>Inspect the VPC Flow Logs using the CloudWatch console and select the log group that contains the NAT gateway's ENI and the EC2 instance's ENI. Leverage a query filter with the source address set as <code>like 205.1</code> and the destination address set as <code>like 198.21.200.1</code>. Execute the stats command to filter the sum of bytes transferred by the source address and the destination address</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "Design Solutions for Organizational Complexity",
      "question_plain": "A social media company has VPC Flow Logs enabled for its NAT gateway. The security team is seeing Action = ACCEPT for inbound traffic that comes from the public IP address 198.21.200.1 destined for a private EC2 instance. The team must determine whether the traffic represents unsolicited inbound connections from the internet. The first two octets of the VPC CIDR block are 205.1.\n\nWhich of the following options can address this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141702,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company is migrating its two-tier legacy application (using MongoDB as a key-value database) from its on-premises data center to AWS. The company has mandated that the EC2 instances must be hosted in a private subnet with no internet access. In addition, all connectivity between the EC2 instance-hosted application and the database must be encrypted. The database must be able to scale to meet traffic spikes from any bursty or unpredictable workloads.</p>\n\n<p>Which do you recommend?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up new Amazon DynamoDB tables for the application with on-demand capacity. Use a gateway VPC endpoint for DynamoDB so that the application can have a private and encrypted connection to the DynamoDB tables</strong></p>\n\n<p>With provisioned capacity, you pay for the provision of read and write capacity units for your DynamoDB tables. Whereas with DynamoDB on-demand you pay per request for the data reads and writes that your application performs on your tables.</p>\n\n<p>With on-demand capacity mode, DynamoDB charges you for the data reads and writes your application performs on your tables. You do not need to specify how much read and write throughput you expect your application to perform because DynamoDB instantly accommodates your workloads as they ramp up or down.</p>\n\n<p>With provisioned capacity mode, you specify the number of reads and writes per second that you expect your application to require, and you are billed based on that. Furthermore, if you can forecast your capacity requirements you can also reserve a portion of DynamoDB provisioned capacity and optimize your costs even further. With provisioned capacity, you can also use auto-scaling to automatically adjust your tableâ€™s capacity based on the specified utilization rate to ensure application performance, and also potentially reduce costs. To configure auto-scaling in DynamoDB, set the minimum and maximum levels of read and write capacity in addition to the target utilization percentage.</p>\n\n<p>It is important to note that DynamoDB auto scaling modifies provisioned throughput settings only when the actual workload stays elevated or depressed for a sustained period of several minutes. This applies to scaling up or down the provisioned capacity of a DynamoDB table. In the case that you have an occasional usage spike, auto-scaling might not be able to react in time. This sometimes can be mitigated by DynamoDB burst capacity where DynamoDB reserves a portion of the unused provisioned capacity for later bursts of throughput. The burst capacity is limited though and these extra capacity units can be consumed quickly.</p>\n\n<p>This means that provisioned capacity is probably best for you if you have relatively predictable application traffic, run applications whose traffic is consistent, and ramps up or down gradually.</p>\n\n<p>Whereas on-demand capacity mode is probably best when you have new tables with unknown workloads, unpredictable application traffic, and also if you only want to pay exactly for what you use. The on-demand pricing model is ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or minutes, and when under-provisioned capacity would impact the user experience.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster for the application with provisioned capacity with auto-scaling enabled. Use an interface VPC endpoint for DocumentDB so that the application can have a private and encrypted connection to the DocumentDB tables</strong></p>\n\n<p><strong>Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster for the application with on-demand capacity. Use a gateway VPC endpoint for DocumentDB so that the application can have a private and encrypted connection to the DocumentDB tables</strong></p>\n\n<p>Amazon DocumentDB (with MongoDB compatibility) clusters are deployed within an Amazon Virtual Private Cloud (Amazon VPC). They can be accessed directly by Amazon EC2 instances or other AWS services that are deployed in the same Amazon VPC. Additionally, Amazon DocumentDB can be accessed by EC2 instances or other AWS services in different VPCs in the same AWS Region or other Regions via VPC peering. Therefore, neither the interface nor gateway VPC endpoint is supported for DocumentDB. So both these options are incorrect.</p>\n\n<p><strong>Set up new Amazon DynamoDB tables for the application with on-demand capacity. Use an interface VPC endpoint for DynamoDB so that the application can have a private and encrypted connection to the DynamoDB tables</strong> - Only gateway VPC endpoint is supported for DynamoDB, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html\">https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/documentdb/latest/developerguide/connect-from-outside-a-vpc.html\">https://docs.aws.amazon.com/documentdb/latest/developerguide/connect-from-outside-a-vpc.html</a></p>\n",
        "answers": [
          "<p>Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster for the application with provisioned capacity with auto-scaling enabled. Use an interface VPC endpoint for DocumentDB so that the application can have a private and encrypted connection to the DocumentDB tables</p>",
          "<p>Set up new Amazon DynamoDB tables for the application with on-demand capacity. Use an interface VPC endpoint for DynamoDB so that the application can have a private and encrypted connection to the DynamoDB tables</p>",
          "<p>Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster for the application with on-demand capacity. Use a gateway VPC endpoint for DocumentDB so that the application can have a private and encrypted connection to the DocumentDB tables</p>",
          "<p>Set up new Amazon DynamoDB tables for the application with on-demand capacity. Use a gateway VPC endpoint for DynamoDB so that the application can have a private and encrypted connection to the DynamoDB tables</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "Design for New Solutions",
      "question_plain": "A company is migrating its two-tier legacy application (using MongoDB as a key-value database) from its on-premises data center to AWS. The company has mandated that the EC2 instances must be hosted in a private subnet with no internet access. In addition, all connectivity between the EC2 instance-hosted application and the database must be encrypted. The database must be able to scale to meet traffic spikes from any bursty or unpredictable workloads.\n\nWhich do you recommend?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141700,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>An e-commerce company is investigating user reports of its Java-based web application errors on the day of the Thanksgiving sale. The development team recovered the logs created by the EC2 instance-hosted web servers and reviewed Aurora DB cluster performance metrics. Some of the web servers were terminated before logs could be collected and the Aurora metrics were inadequate for query performance analysis.</p>\n\n<p>Which of the following steps would you recommend to make the monitoring process more reliable to troubleshoot any future events due to traffic spikes? (Select three)</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", "", ""],
        "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs</strong></p>\n\n<p>You can configure your Aurora MySQL DB cluster to publish general, slow, audit, and error log data to a log group in Amazon CloudWatch Logs. With CloudWatch Logs, you can perform real-time analysis of the log data, and use CloudWatch to create alarms and view metrics. You can use CloudWatch Logs to store your log records in highly durable storage.</p>\n\n<p>To publish logs to CloudWatch Logs, the respective logs must be enabled. Error logs are enabled by default, but you must enable the other types of logs explicitly. The slow query logs and error logs can be used to identify the root cause behind the given issue.</p>\n\n<p><strong>Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the application logs to CloudWatch Logs</strong></p>\n\n<p>You can collect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch agent. The unified CloudWatch agent enables you to collect internal system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. You can collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server. The application logs (via the CloudWatch logs) can be used to identify the root cause behind the given issue.</p>\n\n<p><strong>Set up the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances as well as set up tracing of SQL queries with the X-Ray SDK for Java</strong></p>\n\n<p>You can use the X-Ray SDK to trace incoming HTTP requests that your application serves on an EC2 instance. Use a Filter to instrument incoming HTTP requests. When you add the X-Ray servlet filter to your application, the X-Ray SDK for Java creates a segment for each sampled request. This segment includes timing, method, and disposition of the HTTP request. You can also instrument your SQL database queries by adding the X-Ray SDK for Java JDBC interceptor to your data source configuration. X-Ray tracing for the HTTP requests as well as the SQL queries can help in identifying the root cause behind the given issue.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudTrail and configure a trail to deliver Amazon Aurora query activity to an Amazon S3 bucket. Process and analyze these real-time log streams using Amazon Kinesis Data Streams</strong> - You can use CloudTrail to view, search, download, archive, analyze, and respond to account activity across your AWS infrastructure. You can identify who or what took which action, what resources were acted upon, when the event occurred, and other details to help you analyze and respond to activity in your AWS account. CloudTrail provides a record of actions taken by a user, role, or AWS service in Amazon Aurora. However, CloudTrail does not capture any query activity in Aurora, so this option is incorrect.</p>\n\n<p><strong>Enable <code>detailed monitoring</code> for Amazon EC2 instances to send data points to CloudWatch every minute. Track the metric 'CPUUtilization' to know when the auto-scaling process can kick in</strong> - Tracking the 'CPUUtilization' parameter is irrelevant to the given use case as it would not point to the root cause behind the given issue.</p>\n\n<p><strong>Enable <code>Aurora lab mode</code> which will then publish all logs and activity on Aurora DB to CloudWatch logs</strong> - Aurora lab mode is used to enable Aurora features that are available in the current Aurora database version but are not enabled by default. These features are tested in development/test environments. <code>Aurora lab mode</code> is not relevant for capturing the log activity of Aurora DB. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.CloudWatch.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.CloudWatch.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-filters.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-filters.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-sqlclients.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-sqlclients.html</a></p>\n",
        "answers": [
          "<p>Use CloudTrail and configure a trail to deliver Amazon Aurora query activity to an Amazon S3 bucket. Process and analyze these real-time log streams using Amazon Kinesis Data Streams</p>",
          "<p>Enable <code>Aurora lab mode</code> which will then publish all logs and activity on Aurora DB to CloudWatch logs</p>",
          "<p>Enable <code>detailed monitoring</code> for Amazon EC2 instances to send data points to CloudWatch every minute. Track the metric 'CPUUtilization' to know when the auto-scaling process can kick in</p>",
          "<p>Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs</p>",
          "<p>Set up the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances as well as set up tracing of SQL queries with the X-Ray SDK for Java</p>",
          "<p>Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the application logs to CloudWatch Logs</p>"
        ]
      },
      "correct_response": ["d", "e", "f"],
      "section": "Continuous Improvement for Existing Solutions",
      "question_plain": "An e-commerce company is investigating user reports of its Java-based web application errors on the day of the Thanksgiving sale. The development team recovered the logs created by the EC2 instance-hosted web servers and reviewed Aurora DB cluster performance metrics. Some of the web servers were terminated before logs could be collected and the Aurora metrics were inadequate for query performance analysis.\n\nWhich of the following steps would you recommend to make the monitoring process more reliable to troubleshoot any future events due to traffic spikes? (Select three)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141698,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A team has recently created a secret using AWS Secrets Manager to access their private Amazon Relational Database Service (Amazon RDS) instance. When the team tried to rotate the AWS Secrets Manager secret in an Amazon Virtual Private Cloud (Amazon VPC), the operation failed. On analyzing the Amazon CloudWatch Logs, the team realized that the AWS Lambda task timed out.</p>\n\n<p>Which of the following solutions needs to be implemented for rotating the secret successfully?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure an Amazon VPC interface endpoint to access your Secrets Manager Lambda rotation function and private Amazon Relational Database Service (Amazon RDS) instance</strong></p>\n\n<p>Secrets Manager can't rotate secrets for AWS services running in Amazon VPC private subnets because these subnets don't have internet access. To rotate the keys successfully you need to configure an Amazon VPC interface endpoint to access your Secrets Manager Lambda function and private Amazon Relational Database Service (Amazon RDS) instance.</p>\n\n<p>Steps that need to be followed:\n1. Create security groups for the Secrets Manager VPC endpoint, Amazon RDS instance, and the Lambda rotation function\n2. Add rules to Amazon VPC endpoint and Amazon RDS instance security groups\n3. Attach security groups to AWS resources\n4. Create an Amazon VPC interface endpoint for the Secrets Manager service and associate it with a security group\n5. Verify that the Secrets Manager can rotate the secret</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure an Amazon VPC interface endpoint for the Lambda service to enable access for your Secrets Manager Lambda rotation function and private Amazon Relational Database Service (Amazon RDS) instance</strong> - As explained above, you need to create an Amazon VPC interface endpoint for the Secrets Manager and not for the Lambda service. This option has been added as a distractor.</p>\n\n<p><strong>Interface VPC endpoints support traffic only over HTTP. If this is incorrectly configured, the AWS Lambda function can timeout</strong> - This statement is incorrect. Interface VPC endpoints support traffic only over TCP.</p>\n\n<p><strong>Your Lambda rotation function might be based on an older template that doesn't support SSL/TLS. To support connections that use SSL/TLS, you must recreate your Lambda rotation function</strong> - Rotation functions for Amazon RDS (except Amazon RDS for Oracle) and Amazon DocumentDB automatically use SSL/TLS to connect to your database if it's available. If you set up secret rotation before December 20, 2021, then your rotation function might be based on an older template that doesn't support SSL/TLS. To support connections that use SSL/TLS, you must recreate your rotation function. If this is the issue then the following error crops up \": setSecret: Unable to log into the database with previous, current, or the pending secret of secret\".</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rotate-secrets-manager-secret-vpc/\">https://aws.amazon.com/premiumsupport/knowledge-center/rotate-secrets-manager-secret-vpc/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html#vpce-interface-limitations\">https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html#vpce-interface-limitations</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rotate-secret-db-ssl/\">https://aws.amazon.com/premiumsupport/knowledge-center/rotate-secret-db-ssl/</a></p>\n",
        "answers": [
          "<p>Interface VPC endpoints support traffic only over HTTP. If this is incorrectly configured, the AWS Lambda function can timeout</p>",
          "<p>Configure an Amazon VPC interface endpoint for the Secrets Manager service to enable access for your Secrets Manager Lambda rotation function and private Amazon Relational Database Service (Amazon RDS) instance</p>",
          "<p>Your Lambda rotation function might be based on an older template that doesn't support SSL/TLS. To support connections that use SSL/TLS, you must recreate your Lambda rotation function</p>",
          "<p>Configure an Amazon VPC interface endpoint for the Lambda service to enable access for your Secrets Manager Lambda rotation function and private Amazon Relational Database Service (Amazon RDS) instance</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "Accelerate Workload Migration and Modernization",
      "question_plain": "A team has recently created a secret using AWS Secrets Manager to access their private Amazon Relational Database Service (Amazon RDS) instance. When the team tried to rotate the AWS Secrets Manager secret in an Amazon Virtual Private Cloud (Amazon VPC), the operation failed. On analyzing the Amazon CloudWatch Logs, the team realized that the AWS Lambda task timed out.\n\nWhich of the following solutions needs to be implemented for rotating the secret successfully?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141696,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An ed-tech company needs to deliver its video-on-demand (VOD) content to approximately 1 million users in a cost-effective way. The learning material is in the form of videos with a maximum size of 10 GB each. The videos are highly watched when initially uploaded and subsequently have very less views after 6-8 months. While the old videos might not be accessed regularly, they need to be immediately accessible when needed. With trainers and material doubling every few months, the number of videos has exploded over the last few months, dramatically increasing the cost of storage for the company.</p>\n\n<p>Which is the most cost-effective way of storing these videos to address the given use case?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 Intelligent-Tiering storage class to store the video files. Configure this S3 bucket as the origin of an Amazon CloudFront distribution for delivering the contents to the customers</strong></p>\n\n<p>S3 Intelligent-Tiering is the only cloud storage class that delivers automatic storage cost savings when data access patterns change, without performance impact or operational overhead. The Amazon S3 Intelligent-Tiering storage class is designed to optimize storage costs by automatically moving data to the most cost-effective access tier when access patterns change. For a small monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access patterns and automatically moves objects that have not been accessed to lower-cost access tiers.</p>\n\n<p>S3 Intelligent-Tiering is the ideal storage class for data with unknown, changing, or unpredictable access patterns, independent of object size or retention period. You can use S3 Intelligent-Tiering as the default storage class for virtually any workload, especially data lakes, data analytics, new applications, and user-generated content.</p>\n\n<p>S3 Intelligent-Tiering storage class:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q10-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/s3/storage-classes/intelligent-tiering/\">https://aws.amazon.com/s3/storage-classes/intelligent-tiering/</a></p>\n\n<p>When you store your objects in an Amazon S3 bucket, you can either have users get your objects directly from S3, or you can configure CloudFront to get your objects from S3 and then distribute them to your users. Using CloudFront can be more cost-effective if your users access your objects frequently because, at higher usage, the price for CloudFront data transfer is lower than the price for Amazon S3 data transfer. In addition, downloads are faster with CloudFront than with Amazon S3 alone since the cached content is served from the edge locations that are closer to the end users.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Elastic File System (Amazon EFS) standard storage class to store the video files. Move these video files to EFS Standardâ€“Infrequent Access (Standard-IA) through lifecycle management configuration. Configure a CloudFront custom distribution to deliver content from the EFS origin</strong> - Amazon Elastic File System (Amazon EFS) cannot be configured as an origin for a CloudFront distribution.</p>\n\n<p><strong>Use AWS Elemental MediaConvert and store the transcoded videos in S3. Configure an AWS Elemental MediaPackage endpoint to deliver the content from S3</strong> - The main use case for AWS Media Services is to deliver live content to a global audience. For the given scenario, the content is primarily served via video on demand (VOD), so there is no need to incur extra costs for using the suite of AWS Media Services, so this option is incorrect.</p>\n\n<p><strong>Use Amazon Elastic File System (Amazon EFS) Intelligent-Tiering storage class to store the video files. Configure an Amazon EC2 instance to deliver this content from EFS to viewers through an Amazon CloudFront distribution</strong> - EFS is about three times costlier than S3. In addition, using EC2 as the origin for Cloudfront imposes additional costs for running the infrastructure. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html\">https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/live-streaming.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/live-streaming.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/mediapackage/latest/ug/what-is-terms.html\">https://docs.aws.amazon.com/mediapackage/latest/ug/what-is-terms.html</a></p>\n",
        "answers": [
          "<p>Use Amazon Elastic File System (Amazon EFS) Standard storage class to store the video files. Move these video files to EFS Standardâ€“Infrequent Access (Standard-IA) through lifecycle management configuration. Configure a CloudFront custom distribution to deliver content from the EFS origin</p>",
          "<p>Use Amazon S3 Intelligent-Tiering storage class to store the video files. Configure this S3 bucket as the origin of an Amazon CloudFront distribution for delivering the contents to the customers</p>",
          "<p>Use AWS Elemental MediaConvert and store the transcoded videos in S3. Configure an AWS Elemental MediaPackage endpoint to deliver the content from S3</p>",
          "<p>Use Amazon Elastic File System (Amazon EFS) Intelligent-Tiering storage class to store the video files. Configure an Amazon EC2 instance to deliver this content from EFS to viewers through an Amazon CloudFront distribution</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "Design for New Solutions",
      "question_plain": "An ed-tech company needs to deliver its video-on-demand (VOD) content to approximately 1 million users in a cost-effective way. The learning material is in the form of videos with a maximum size of 10 GB each. The videos are highly watched when initially uploaded and subsequently have very less views after 6-8 months. While the old videos might not be accessed regularly, they need to be immediately accessible when needed. With trainers and material doubling every few months, the number of videos has exploded over the last few months, dramatically increasing the cost of storage for the company.\n\nWhich is the most cost-effective way of storing these videos to address the given use case?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141694,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A retail company offers its services to the customers via APIs that leverage Amazon API Gateway and Lambda functions. The company also has a legacy API hosted on an Amazon EC2 instance that is used by the company's supply chain partners. The security and audit team at the company has raised concerns over the use of these APIs and wants a solution to secure them all from any vulnerabilities, DDoS attacks, and malicious exploits.</p>\n\n<p>Which of the following options would you use to address the security requirements of the company?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Web Application Firewall (WAF) as the first line of defense to protect the API Gateway APIs against malicious exploits and DDoS attacks. Install Amazon Inspector on the EC2 instance to check for vulnerabilities. Configure Amazon GuardDuty to monitor any malicious attempts to access the APIs illegally</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect web applications and APIs from attacks. It enables you to configure a set of rules (called a web access control list (web ACL)) that allow, block, or count web requests based on customizable web security rules and conditions that you define. You can protect the following resource types:</p>\n\n<ol>\n<li>Amazon CloudFront distribution</li>\n<li>Amazon API Gateway REST API</li>\n<li>Application Load Balancer</li>\n<li>AWS AppSync GraphQL API</li>\n<li>Amazon Cognito user pool</li>\n</ol>\n\n<p>You can use AWS WAF to protect your API Gateway API from common web exploits, such as SQL injection and cross-site scripting (XSS) attacks. These could affect API availability and performance, compromise security, or consume excessive resources. For example, you can create rules to allow or block requests from specified IP address ranges, requests from CIDR blocks, requests that originate from a specific country or region, requests that contain malicious SQL code, or requests that contain malicious scripts.</p>\n\n<p>DDoS attacks are attempts by an attacker to disrupt the availability of targeted systems. For infrastructure layer attacks, you can use AWS services such as Amazon CloudFront and Elastic Load Balancing (ELB) to provide automatic DDoS protection. For application layer attacks, you can use AWS WAF as the primary mitigation. AWS WAF web access control lists (web ACLs) minimize the effects of a DDoS attack at the application layer.</p>\n\n<p>How WAF works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q9-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p>\n\n<p>GuardDuty is an intelligent threat detection service that continuously monitors your AWS accounts, Amazon Elastic Compute Cloud (EC2) instances, Amazon Elastic Kubernetes Service (EKS) clusters, and data stored in Amazon Simple Storage Service (S3) for malicious activity without the use of security software or agents. If potential malicious activity, such as anomalous behavior, credential exfiltration, or command and control infrastructure (C2) communication is detected, GuardDuty generates detailed security findings that can be used for security visibility and assisting in remediation. GuardDuty can monitor reconnaissance activities by an attacker such as unusual API activity, intra-VPC port scanning, unusual patterns of failed login requests, or unblocked port probing from a known bad IP.</p>\n\n<p>How GuardDuty works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q9-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n\n<p>Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure.</p>\n\n<p>How Amazon Inspector works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q9-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/inspector\">https://aws.amazon.com/inspector</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon CloudFront in front of the APIs to protect against malicious exploits and DDoS attacks. Install Amazon GuardDuty on the EC2 instances to assess any vulnerabilities</strong> - This statement is incorrect. GuardDuty cannot assess vulnerabilities in the EC2 instances. Amazon Inspector is the automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure.</p>\n\n<p><strong>Enable AWS Network Firewall on API Gateway as well as the Amazon EC2 instances to check for vulnerabilities and protect against DDoS attacks as well as malicious exploits</strong> - AWS Network Firewall is a managed service that makes it easy to deploy essential network protections for all of your Amazon Virtual Private Clouds (VPCs). AWS Network Firewall works with AWS Firewall Manager to centrally manage security policies and automatically enforce mandatory security policies across existing and newly created accounts and VPCs. This service works at the VPC level and not at the API Gateway or the EC2 instance level.</p>\n\n<p>How AWS Network Firewall works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q9-i4.jpg\">\nvia - <a href=\"https://aws.amazon.com/network-firewall/\">https://aws.amazon.com/network-firewall/</a></p>\n\n<p><strong>Use AWS Web Application Firewall (WAF) as the first line of defense to protect the API Gateway APIs against malicious exploits and DDoS attacks. Install Amazon Inspector on the EC2 instance to check for vulnerabilities. Configure Amazon GuardDuty to block any malicious attempts to access the APIs illegally</strong> - GuardDuty cannot block any malicious attempts to access the APIs illegally. Rather, it can only monitor/detect such attempts.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/guardduty/faqs/\">https://aws.amazon.com/guardduty/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/waf-mitigate-ddos-attacks/\">https://aws.amazon.com/premiumsupport/knowledge-center/waf-mitigate-ddos-attacks/</a></p>\n",
        "answers": [
          "<p>Use AWS Web Application Firewall (WAF) as the first line of defense to protect the API Gateway APIs against malicious exploits and DDoS attacks. Install Amazon Inspector on the EC2 instance to check for vulnerabilities. Configure Amazon GuardDuty to monitor any malicious attempts to access the APIs illegally</p>",
          "<p>Enable AWS Network Firewall on API Gateway as well as the Amazon EC2 instances to check for vulnerabilities and protect against DDoS attacks as well as malicious exploits</p>",
          "<p>Configure Amazon CloudFront in front of the APIs to protect against malicious exploits and DDoS attacks. Install Amazon GuardDuty on the EC2 instances to assess any vulnerabilities</p>",
          "<p>Use AWS Web Application Firewall (WAF) as the first line of defense to protect the API Gateway APIs against malicious exploits and DDoS attacks. Install Amazon Inspector on the EC2 instance to check for vulnerabilities. Configure Amazon GuardDuty to block any malicious attempts to access the APIs illegally</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "Design for New Solutions",
      "question_plain": "A retail company offers its services to the customers via APIs that leverage Amazon API Gateway and Lambda functions. The company also has a legacy API hosted on an Amazon EC2 instance that is used by the company's supply chain partners. The security and audit team at the company has raised concerns over the use of these APIs and wants a solution to secure them all from any vulnerabilities, DDoS attacks, and malicious exploits.\n\nWhich of the following options would you use to address the security requirements of the company?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141692,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company manages a healthcare diagnostics application that writes thousands of lab images to a mounted NFS file system each night from 10 PM - 5 AM. The company wants to migrate this application from its on-premises data center to AWS Cloud over a private network. The company has already established an AWS Direct Connect connection to AWS to facilitate this migration. This application is slated to be moved to Amazon EC2 instances with the Elastic File System (Amazon EFS) file system as the storage service.</p>\n\n<p>Which of the following represents the MOST optimal way of replicating all images to the cloud before the application is fully migrated to the cloud?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Configure a DataSync scheduled task to send the images to the EFS file system every night</strong></p>\n\n<p>You can use VPC endpoints to ensure data transferred between your AWS DataSync agent, either deployed on-premises or in-cloud, doesn't traverse the public internet or need public IP addresses. Using VPC endpoints increases the security of your data by keeping network traffic within your Amazon Virtual Private Cloud (Amazon VPC). VPC endpoints for DataSync are powered by AWS PrivateLink, a highly available, scalable technology that enables you to privately connect your VPC to supported AWS services.</p>\n\n<p>The DataSync agent transfers data between your storage and AWS. In most situations, you deploy the agent as a virtual machine in the same local network as your source storage. This approach minimizes network overhead associated with transferring data by using network protocols such as Network File System (NFS) and Server Message Block (SMB) or when accessing your object storage that's compatible with the Amazon S3 API. This setup is common regardless of the endpoint type you use to connect your agent to AWS.</p>\n\n<p>When you use a VPC endpoint, your DataSync agent communicates directly with AWS without crossing the public internet. Data is transferred using AWS Direct Connect or a virtual private network (VPN). The private IP addresses that DataSync creates for the transfer are accessible only from inside your VPC.</p>\n\n<p>Reference architecture of using DataSync with VPC endpoints:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/datasync/latest/userguide/datasync-in-vpc.html\">https://docs.aws.amazon.com/datasync/latest/userguide/datasync-in-vpc.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an NFS file share using the AWS Storage Gateway file gateway. Mount your NFS file share on a drive on your client and map it to your Amazon S3 bucket. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system</strong> - AWS Storage Gateway provides seamless access to data in hybrid architectures. When the entire application is being moved to AWS Cloud, AWS Storage Gateway is not the best fit for the given use case. In addition, the data is initially copied to S3 and then replicated into EFS, thereby making the process inefficient.</p>\n\n<p><strong>Define a cron job on the on-premises system to run the AWS s3 sync command from the on-premises file system to Amazon S3. Use the Amazon S3 Event Notifications to call a Lambda function that will copy the images from the S3 bucket to the EFS file system</strong> - The data is initially copied to S3 and then replicated into EFS, thereby making the process inefficient.</p>\n\n<p><strong>Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Connect to AWS VPC endpoint for EFS over a public VIF of the Direct Connect connection. Configure a DataSync scheduled task to send the images to the EFS file system every night</strong> - A VPC endpoint allows you to privately connect your VPC to supported AWS services without requiring an internet gateway or a NAT device, VPN connection, or AWS Direct Connect connection. A public virtual interface (VIF) can access all AWS public services using public IP addresses. You cannot leverage public VIF to access the VPC endpoint for EFS. Therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/datasync/latest/userguide/datasync-in-vpc.html\">https://docs.aws.amazon.com/datasync/latest/userguide/datasync-in-vpc.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/storage/transferring-files-from-on-premises-to-aws-and-back-without-leaving-your-vpc-using-aws-datasync/\">https://aws.amazon.com/blogs/storage/transferring-files-from-on-premises-to-aws-and-back-without-leaving-your-vpc-using-aws-datasync/</a></p>\n",
        "answers": [
          "<p>Create an NFS file share using AWS Storage Gateway file gateway. Mount your NFS file share on a drive on your client and map it to your Amazon S3 bucket. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system</p>",
          "<p>Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Configure a DataSync scheduled task to send the images to the EFS file system every night</p>",
          "<p>Define a cron job on the on-premises system to run the AWS s3 sync command from the on-premises file system to Amazon S3. Use the Amazon S3 Event Notifications to call a Lambda function that will copy the images from the S3 bucket to the EFS file system</p>",
          "<p>Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Connect to AWS VPC endpoint for EFS over a public VIF of the Direct Connect connection. Configure a DataSync scheduled task to send the images to the EFS file system every night</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "Accelerate Workload Migration and Modernization",
      "question_plain": "A company manages a healthcare diagnostics application that writes thousands of lab images to a mounted NFS file system each night from 10 PM - 5 AM. The company wants to migrate this application from its on-premises data center to AWS Cloud over a private network. The company has already established an AWS Direct Connect connection to AWS to facilitate this migration. This application is slated to be moved to Amazon EC2 instances with the Elastic File System (Amazon EFS) file system as the storage service.\n\nWhich of the following represents the MOST optimal way of replicating all images to the cloud before the application is fully migrated to the cloud?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141690,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An e-commerce company manages its flagship application on a load-balanced EC2 instance fleet for web hosting, database API services, and business logic. This tightly coupled architecture makes it inflexible for new feature additions while also making the architecture less scalable.</p>\n\n<p>Which of the following options can be used to decouple the architecture, improve scalability and provide the ability to track the failed orders?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon S3 for hosting the web application while using AWS AppSync for database access services. Use Amazon Simple Queue Service (Amazon SQS) for queuing orders and AWS Lambda for business logic. Use Amazon SQS dead-letter queue for tracking and re-processing failed orders</strong></p>\n\n<p>Amazon S3 can be configured to host a web application.</p>\n\n<p>AWS AppSync creates serverless GraphQL and Pub/Sub APIs that simplify application development through a single endpoint to securely query, update, or publish data. AWS AppSync creates serverless GraphQL and Pub/Sub APIs that simplify application development through a single endpoint to securely query, update, or publish data.</p>\n\n<p>How AWS AppSync works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q7-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/appsync/\">https://aws.amazon.com/appsync/</a></p>\n\n<p>Amazon SQS supports dead-letter queues (DLQ), which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate unconsumed messages to determine why their processing doesn't succeed.</p>\n\n<p>SQS dead-letter queues:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q7-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon CloudFront for hosting the website and Amazon API Gateway for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing and AWS Lambda for business logic. Use Amazon SQS long polling for retaining failed orders</strong> - You cannot use Amazon CloudFront for hosting a website as the website is hosted on the Cloudfront distribution's underlying origin (such as S3 or an EC2 instance). You cannot use Amazon SQS long polling for retaining failed orders. When the wait time for the <code>ReceiveMessage</code> API action is greater than 0, long polling is in effect. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses and false empty responses. Long polling is a configurable parameter of SQS queues and not a temporary storage space to hold failed orders.</p>\n\n<p><strong>Use Amazon Lightsail for web hosting with AWS AppSync for database API services. Use Simple Queue Service (Amazon SQS) for order queuing. Use Amazon Elastic Container Service (Amazon ECS) for business logic and use the <code>visibility timeout</code> parameter of Amazon SQS to retain the failed orders</strong> - You cannot use the <code>visibility timeout</code> parameter of Amazon SQS to retain the failed orders. Immediately after a message is received in an SQS queue, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a <code>visibility timeout</code>, a period during which Amazon SQS prevents other consumers from receiving and processing the message. <code>Visibility timeout</code> is a configurable parameter of SQS queues and not a temporary storage space to hold failed orders.</p>\n\n<p><strong>Use AWS Elastic Beanstalk for hosting the web application and Amazon API Gateway for database API services. Use Kinesis Data Streams for queuing orders and AWS Lambda to build business logic. Configure an Amazon S3 bucket for retaining failed orders on an hourly basis</strong> - Amazon Kinesis Streams allows real-time processing of streaming big data and the ability to read and replay records to multiple Amazon Kinesis Applications. Amazon SQS offers a reliable, highly-scalable hosted queue for storing messages as they travel between applications or microservices. It moves data between distributed application components and helps you decouple these components.</p>\n\n<p>You should not use S3 to retain failed orders on an hourly basis. This would result in too many small objects (1 object for each failed order) on S3 which need to be written and read multiple times. In addition, it would be cumbersome to keep track of the failed orders and do the root cause analysis. You could run SQL queries via Athena on this underlying data in S3. However, it would turn out to be costly and inefficient while querying small objects via Athena. Therefore, this use case is an anti-pattern for S3.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/appsync/\">https://aws.amazon.com/appsync/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-design-patterns.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-design-patterns.html</a></p>\n",
        "answers": [
          "<p>Configure Amazon CloudFront for hosting the website and Amazon API Gateway for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing and AWS Lambda for business logic. Use Amazon SQS long polling for retaining failed orders</p>",
          "<p>Use Amazon Lightsail for web hosting with AWS AppSync for database API services. Use Simple Queue Service (Amazon SQS) for order queuing. Use Amazon Elastic Container Service (Amazon ECS) for business logic and use the <code>visibility timeout</code> parameter of Amazon SQS to retain the failed orders</p>",
          "<p>Use AWS Elastic Beanstalk for hosting the web application and Amazon API Gateway for database API services. Use Kinesis Data Streams for queuing orders and AWS Lambda to build business logic. Configure an Amazon S3 bucket for retaining failed orders on an hourly basis</p>",
          "<p>Configure Amazon S3 for hosting the web application while using AWS AppSync for database access services. Use Amazon Simple Queue Service (Amazon SQS) for queuing orders and AWS Lambda for business logic. Use Amazon SQS dead-letter queue for tracking and re-processing failed orders</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "Design for New Solutions",
      "question_plain": "An e-commerce company manages its flagship application on a load-balanced EC2 instance fleet for web hosting, database API services, and business logic. This tightly coupled architecture makes it inflexible for new feature additions while also making the architecture less scalable.\n\nWhich of the following options can be used to decouple the architecture, improve scalability and provide the ability to track the failed orders?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141688,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company provides a web-based business-management platform for IT service companies across the globe to manage help desk, customer service, sales and marketing, and other critical business functions. More than 50,000 people use the company's platform, so the company must respond quickly to any reported problems. However, the company has issues with not having enough visibility into its systems to discover any issues. Multiple logs and monitoring systems are needed to understand the root cause of problems thereby taking hours to resolve. Even as the company is slowly moving towards serverless architecture using AWS Lambda/Amazon API Gateway/Amazon Elastic Container Service (Amazon ECS), the company wants to monitor the microservices and gain deeper insights into its serverless resources.</p>\n\n<p>Which of the following will you recommend to address the given requirements?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS X-Ray to analyze the microservices applications through request tracing. Configure Amazon CloudWatch for monitoring containers, latency, web server requests, and incoming load-balancer requests and create CloudWatch alarms to send out notifications if system latency is increasing</strong></p>\n\n<p>AWS X-Ray helps developers analyze and debug production, and distributed applications, such as those built using a microservices architecture.</p>\n\n<p>Analyze and debug using X-Ray:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q6-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p>AWS X-Ray creates a map of services used by your application with trace data that you can use to drill into specific services or issues. This provides a view of connections between services in your application and aggregated data for each service, including average latency and failure rates.</p>\n\n<p>X-Ray service map:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q6-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/xray/features/\">https://aws.amazon.com/xray/features/</a></p>\n\n<p>X-Ray Traces:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q6-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/xray/features/\">https://aws.amazon.com/xray/features/</a></p>\n\n<p>Amazon CloudWatch allows you to collect infrastructure metrics from more than 70 AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon DynamoDB, Amazon Simple Storage Service (Amazon S3), Amazon ECS, AWS Lambda, Amazon API Gateway, with no action on your part. For example, Amazon EC2 instances automatically publish CPU utilization, data transfer, and disk usage metrics to help you understand state changes. You can use built-in metrics for API Gateway to detect latency or use built-in metrics for AWS Lambda to detect errors or throttles. Likewise, Amazon CloudWatch also allows you to collect application metrics (such as user activity, error metrics or memory used) from your applications to monitor operational performance, troubleshoot issues, and spot trends.</p>\n\n<p>Amazon CloudWatch for monitoring:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q6-i4.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS X-Ray to analyze the microservices applications through request tracing. Configure Amazon EventBridge for monitoring containers, latency, web server requests, and incoming load-balancer requests and create alarms to send out notifications if system latency is increasing</strong></p>\n\n<p><strong>Configure Amazon EventBridge for monitoring containers, latency, web server requests, and incoming load-balancer requests and create alarms to send out notifications if system latency is increasing. Use AWS Config to continually assesses, audit, and evaluate the configurations and relationships of your resources and trigger alarms when needed</strong></p>\n\n<p>Amazon EventBridge is a serverless event bus service that uses the Amazon CloudWatch Events API, but also includes more functionality, like the ability to ingest events from SaaS apps. EventBridge is designed to extend the event model beyond AWS, bringing data from software-as-a-service (SaaS) providers into your AWS environment. This means you can consume events from popular providers such as Zendesk, PagerDuty, and Auth0. You can use these in your applications with the same ease as any AWS-generated event.</p>\n\n<p>For the given use case, you can use Cloudwatch for monitoring containers, latency, web server requests, and incoming load-balancer requests and create CloudWatch alarms to send out notifications if system latency is increasing. Therefore, both these options are incorrect.</p>\n\n<p><strong>Configure Amazon CloudWatch to monitor and analyze all microservices through request tracing. Enable CloudTrail to log all user activity</strong> - X-Ray can be used to monitor and analyze AWS microservices through request tracing, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/features/\">https://aws.amazon.com/cloudwatch/features/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContainerInsights.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContainerInsights.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/solutions/case-studies/connectwise/\">https://aws.amazon.com/solutions/case-studies/connectwise/</a></p>\n",
        "answers": [
          "<p>Use AWS X-Ray to analyze the microservices applications through request tracing. Configure Amazon EventBridge for monitoring containers, latency, web server requests, and incoming load-balancer requests and create alarms to send out notifications if system latency is increasing</p>",
          "<p>Configure Amazon CloudWatch to monitor and analyze all microservices through request tracing. Enable CloudTrail to log all user activity</p>",
          "<p>Use AWS X-Ray to analyze the microservices applications through request tracing. Configure Amazon CloudWatch for monitoring containers, latency, web server requests, and incoming load-balancer requests and create CloudWatch alarms to send out notifications if system latency is increasing</p>",
          "<p>Configure Amazon EventBridge for monitoring containers, latency, web server requests, and incoming load-balancer requests and create alarms to send out notifications if system latency is increasing. Use AWS Config to continually assesses, audit, and evaluate the configurations and relationships of your resources and trigger alarms when needed</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "Design for New Solutions",
      "question_plain": "A company provides a web-based business-management platform for IT service companies across the globe to manage help desk, customer service, sales and marketing, and other critical business functions. More than 50,000 people use the company's platform, so the company must respond quickly to any reported problems. However, the company has issues with not having enough visibility into its systems to discover any issues. Multiple logs and monitoring systems are needed to understand the root cause of problems thereby taking hours to resolve. Even as the company is slowly moving towards serverless architecture using AWS Lambda/Amazon API Gateway/Amazon Elastic Container Service (Amazon ECS), the company wants to monitor the microservices and gain deeper insights into its serverless resources.\n\nWhich of the following will you recommend to address the given requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141686,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A company uses Amazon FSx for Windows File Server with deployment type of Single-AZ 2 as its file storage service for its non-core functions. With a change in the company's policy that mandates high availability of data for all its functions, the company needs to change the existing configuration. The company also needs to monitor the file system activity as well as the end-user actions on the Amazon FSx file server.</p>\n\n<p>Which solutions will you combine to implement these requirements? (Select two)</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure a new Amazon FSx for Windows file system with a deployment type of Multi-AZ. Transfer data to the newly created file system using the AWS DataSync service. Point all the file system users to the new location. You can test the failover of your Multi-AZ file system by modifying its throughput capacity</strong></p>\n\n<p>In a Multi-AZ deployment, Amazon FSx automatically provisions and maintains a standby file server in a different Availability Zone. Any changes written to disk in your file system are synchronously replicated across Availability Zones to the standby. If there is planned file system maintenance or unplanned service disruption, Amazon FSx automatically fails over to the secondary file server, allowing you to continue accessing your data without manual intervention. Multi-AZ file systems are recommended for most production workloads that require high availability of shared Windows file data.</p>\n\n<p>To migrate your existing files to FSx for Windows File Server file systems, AWS recommends using AWS DataSync, an online data transfer service designed to simplify, automate, and accelerate copying large amounts of data to and from AWS storage services. DataSync copies data over the internet or AWS Direct Connect.</p>\n\n<p>You can test the failover of your Multi-AZ file system by modifying its throughput capacity. When you modify your file system's throughput capacity, Amazon FSx switches out the file system's file server. Multi-AZ file systems automatically fail over to the secondary server while Amazon FSx replaces the preferred server file server first. Then the file system automatically fails back to the new primary server and Amazon FSx replaces the secondary file server.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multiAZ.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multiAZ.html</a></p>\n\n<p><strong>You can monitor storage capacity and file system activity using Amazon CloudWatch, and monitor end-user actions with file access auditing using Amazon CloudWatch Logs and Amazon Kinesis Data Firehose</strong></p>\n\n<p>You can monitor storage capacity and file system activity using Amazon CloudWatch. You can monitor end-user actions with file access auditing at any time (during or after the creation of a file system) via the AWS Management Console or the Amazon FSx CLI or API. You can also change the destination for publishing user access events by logging these events to CloudWatch Logs or streaming to Kinesis Data Firehose.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a new Amazon FSx for Windows file system with a deployment type of Single-AZ 1. Transfer data to the newly created file system using the AWS DataSync service. Point all the file system users to the new location</strong> - With Single-AZ file systems, Amazon FSx automatically replicates your data within an Availability Zone (AZ) to protect it from component failure. It continuously monitors for hardware failures and automatically replaces infrastructure components in the event of a failure. Single-AZ 2 is the latest generation of Single-AZ file systems, and it supports both SSD and HDD storage. Single-AZ 1 file systems support SSD storage, Microsoft Distributed File System Replication (DFSR), and the use of custom DNS names. Single-AZ file systems will experience unavailability during file system maintenance, infrastructure component replacement, and when an Availability Zone is unavailable.</p>\n\n<p>Multi-AZ file systems are recommended for most production workloads that require high availability of shared Windows file data. Single-AZ file systems offer a lower price point for workloads that donâ€™t require the high availability of a Multi-AZ solution and that can recover from the most recent file system backup if data is lost.</p>\n\n<p><strong>You can monitor the file system activity using AWS CloudTrail and monitor end-user actions with file access auditing using Amazon CloudWatch Logs</strong> - This statement is incorrect. You can monitor storage capacity and file system activity using Amazon CloudWatch, monitor all Amazon FSx API calls using AWS CloudTrail, and monitor end-user actions with file access auditing using Amazon CloudWatch Logs and Amazon Kinesis Data Firehose.</p>\n\n<p><strong>Configure a new Amazon FSx for Windows file system with a deployment type of Multi-AZ. Transfer data to the newly created file system using the AWS DataSync service. Point all the file system users to the new location. You can test the failover of your Multi-AZ file system by modifying the elastic network interfaces associated with your file system</strong> - You must not modify or delete the elastic network interfaces associated with your file system. Modifying or deleting the network interface can cause a permanent loss of connection between your VPC and your file system. Therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multiAZ.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multiAZ.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-files-fsx.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-files-fsx.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/fsx/windows/faqs/\">https://aws.amazon.com/fsx/windows/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/limit-access-security-groups.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/limit-access-security-groups.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/file-access-auditing-is-now-available-for-amazon-fsx-for-windows-file-server/\">https://aws.amazon.com/blogs/aws/file-access-auditing-is-now-available-for-amazon-fsx-for-windows-file-server/</a></p>\n",
        "answers": [
          "<p>Configure a new Amazon FSx for Windows file system with a deployment type of Multi-AZ. Transfer data to the newly created file system using the AWS DataSync service. Point all the file system users to the new location. You can test the failover of your Multi-AZ file system by modifying its throughput capacity</p>",
          "<p>Configure a new Amazon FSx for Windows file system with a deployment type of Single-AZ 1. Transfer data to the newly created file system using the AWS DataSync service. Point all the file system users to the new location</p>",
          "<p>You can monitor the file system activity using AWS CloudTrail and monitor end-user actions with file access auditing using Amazon CloudWatch Logs</p>",
          "<p>Configure a new Amazon FSx for Windows file system with a deployment type of Multi-AZ. Transfer data to the newly created file system using the AWS DataSync service. Point all the file system users to the new location. You can test the failover of your Multi-AZ file system by modifying the elastic network interfaces associated with your file system</p>",
          "<p>You can monitor storage capacity and file system activity using Amazon CloudWatch, and monitor end-user actions with file access auditing using Amazon CloudWatch Logs and Amazon Kinesis Data Firehose</p>"
        ]
      },
      "correct_response": ["a", "e"],
      "section": "Continuous Improvement for Existing Solutions",
      "question_plain": "A company uses Amazon FSx for Windows File Server with deployment type of Single-AZ 2 as its file storage service for its non-core functions. With a change in the company's policy that mandates high availability of data for all its functions, the company needs to change the existing configuration. The company also needs to monitor the file system activity as well as the end-user actions on the Amazon FSx file server.\n\nWhich solutions will you combine to implement these requirements? (Select two)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141684,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company manages a stateful web application that persists data on a MySQL database. The application stack is hosted in the company's on-premises data center using a single server. The company is looking at increasing its market presence through promotions and campaigns. While the user experience has been good so far, the current application architecture will not support the growth that the company envisages. The company has hired you as an AWS Certified Solutions Architect Professional to migrate the current architecture to AWS which should continue to support SQL-based queries. The proposed solution should offer maximum reliability with better performance.</p>\n\n<p>What would you recommend?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up database migration to Amazon Aurora MySQL. Deploy the application in an Auto Scaling group for Amazon EC2 instances that are fronted by an Application Load Balancer. Store sessions in an Amazon ElastiCache for Redis with replication group</strong></p>\n\n<p>Amazon Aurora is designed for unparalleled high performance and availability at a global scale with full MySQL and PostgreSQL compatibility.</p>\n\n<p>Amazon ElastiCache for Redis is a Redis-compatible in-memory service that delivers the ease of use and power of Redis along with the availability, reliability, and performance suitable for the most demanding applications. Both single-node and up to 15-shard clusters are available, enabling scalability to up to 3.55 TiB of in-memory data. Amazon ElastiCache for Redis is fully managed, scalable, and secure. This makes it an ideal candidate to power high-performance use cases such as web, mobile apps, gaming, ad tech, and IoT.</p>\n\n<p>Redis and Memcached are popular, open-source, in-memory data stores. Although they are both easy to use and offer high performance, there are important differences to consider when choosing an engine. Memcached is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases.</p>\n\n<p>Redis lets you create multiple replicas of a Redis primary. This allows you to scale database reads and to have highly available clusters. The replication support makes Redis a more reliable solution than Memcached.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q4-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up database migration to an Amazon RDS MySQL Multi-AZ DB instance. Deploy the application in an Auto Scaling group for Amazon EC2 instances that are fronted by an Application Load Balancer. Store sessions in Amazon ElastiCache for Memcached</strong> - As explained above, the replication support makes Redis a more reliable solution than Memcached. So this option is not the best fit.</p>\n\n<p><strong>Set up database migration to an Amazon RDS MySQL DB instance using read replicas. Deploy the application in an Auto Scaling group for Amazon EC2 instances that are fronted by an Application Load Balancer. Store sessions using Amazon Neptune</strong> - Amazon Neptune is a fast, fully managed database service powering graph use cases such as identity graphs, knowledge graphs, and fraud detection. You cannot use Neptune as a caching layer for storing user sessions.</p>\n\n<p><strong>Set up database migration to an Amazon DocumentDB instance. Deploy the application in an Auto Scaling group for Amazon EC2 instances that are fronted by a Network Load Balancer. Store sessions in an Amazon ElastiCache for Redis with replication group</strong> - Amazon DocumentDB is a scalable, highly durable, and fully managed database service for operating mission-critical MongoDB workloads. For the given use case, you cannot migrate the relational database (MySQL) into a document database (DocumentDB) as DocumentDB does not support SQL-based queries.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-overview/database.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-overview/database.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis.Groups.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis.Groups.html</a></p>\n",
        "answers": [
          "<p>Set up database migration to an Amazon RDS MySQL DB instance using read replicas. Deploy the application in an Auto Scaling group for Amazon EC2 instances that are fronted by an Application Load Balancer. Store sessions using Amazon Neptune</p>",
          "<p>Set up database migration to an Amazon DocumentDB instance. Deploy the application in an Auto Scaling group for Amazon EC2 instances that are fronted by a Network Load Balancer. Store sessions in an Amazon ElastiCache for Redis with replication group</p>",
          "<p>Set up database migration to Amazon Aurora MySQL. Deploy the application in an Auto Scaling group for Amazon EC2 instances that are fronted by an Application Load Balancer. Store sessions in an Amazon ElastiCache for Redis with replication group</p>",
          "<p>Set up database migration to an Amazon RDS MySQL Multi-AZ DB instance. Deploy the application in an Auto Scaling group for Amazon EC2 instances that are fronted by an Application Load Balancer. Store sessions in Amazon ElastiCache for Memcached</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "Accelerate Workload Migration and Modernization",
      "question_plain": "A company manages a stateful web application that persists data on a MySQL database. The application stack is hosted in the company's on-premises data center using a single server. The company is looking at increasing its market presence through promotions and campaigns. While the user experience has been good so far, the current application architecture will not support the growth that the company envisages. The company has hired you as an AWS Certified Solutions Architect Professional to migrate the current architecture to AWS which should continue to support SQL-based queries. The proposed solution should offer maximum reliability with better performance.\n\nWhat would you recommend?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141682,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A retail company is introducing multiple business units as part of its expansion plans. To implement this change, the company will be building several new business-unit-specific workloads by leveraging a variety of AWS services. The company wants to track the expenses of each business unit and limit the spending to a pre-defined threshold. In addition, the solution should allow the security team to identify and respond to threats as quickly as possible for all the workloads across the business units. Also, workload accounts may need to be pulled off into a temporary holding area due to resource audit reasons.</p>\n\n<p>Which of the following can be combined to build a solution for the given requirements? (Select three)</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", "", ""],
        "explanation": "<p>Correct options:</p>\n\n<p><strong>Use AWS Organizations to set up a multi-account environment. Organize the accounts into the following Organizational Units (OUs): Security, Infrastructure, Workloads, Suspended and Exceptions</strong> - AWS categorizes the Security OU and the Infrastructure OU as foundational. The foundational OUs contain accounts, workloads, and other AWS resources that provide common security and infrastructure capabilities to secure and support your overall AWS environment.</p>\n\n<p>The Suspended OU is used as a temporary holding area for accounts that are required to have their use suspended either temporarily or permanently.</p>\n\n<p>The Exceptions OU houses an account that requires an exception to the security policies that are applied to your Workloads OU.</p>\n\n<p>AWS recommended OUs and accounts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/recommended-ous-and-accounts.html\">https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/recommended-ous-and-accounts.html</a></p>\n\n<p><strong>Configure an AWS Budget alert to move an AWS account to Exceptions OU if the account reaches a predefined budget threshold. Use Service Control Policies (SCPs) to limit/block resource usage in the Exceptions OU. Configure a Suspended OU to hold workload accounts with retired resources. Use Service Control Policies (SCPs) to limit/block resource usage in the Suspended OU</strong> - AWS Budgets provides the capability to configure cost-saving controls, or actions, that run either automatically on your behalf or by using a workflow approval process. You can use actions to define an explicit response that you want to take when a budget exceeds its action threshold. You can trigger these alerts on actual or forecasted cost and usage budgets.</p>\n\n<p>For the given scenario, the management account can move the member account to restrictive OU (Exceptions OU) after the budget threshold for the member account is met.</p>\n\n<p>Using AWS Budgets actions to move an AWS account to an OU:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q3-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/manage-cost-overruns-part-1/\">https://aws.amazon.com/blogs/mt/manage-cost-overruns-part-1/</a></p>\n\n<p><strong>Designate an account within the AWS Organizations organization to be the GuardDuty delegated administrator. Create an SNS topic in this account. Subscribe the security team to the topic so that the security team can receive alerts from GuardDuty via SNS</strong></p>\n\n<p>When you use GuardDuty with an AWS Organizations organization, you can designate any account within the organization to be the GuardDuty delegated administrator. Only the organization management account can designate GuardDuty delegated administrators.</p>\n\n<p>An account that is designated as a delegated administrator becomes a GuardDuty administrator account, has GuardDuty automatically enabled in the designated Region and is granted permission to enable and manage GuardDuty for all accounts in the organization within that Region. The other accounts in the organization can be viewed and added as GuardDuty member accounts associated with the delegated administrator account.</p>\n\n<p>For the given use case, you can set up an SNS topic in this account and then subscribe the security team to the topic so that the security team can receive alerts from GuardDuty.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Organizations to set up a multi-account environment. Organize the accounts into the following Service Control Policies (SCPs): Security, Infrastructure, Workloads, Suspended, and Exceptions. Grant necessary permissions to the accounts by using the SCP guardrails</strong> - Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs alone are not sufficient to grant permissions to the accounts in your organization. No permissions are granted by an SCP. You cannot organize AWS accounts into Service Control Policies (SCPs). You organize AWS accounts into Organization Units by using AWS Organizations.</p>\n\n<p><strong>Configure an AWS Cost Explorer alert to move an AWS account to Exceptions OU if the account reaches a predefined budget threshold. Use Service Control Policies (SCPs) to limit/block resource usage in the Exceptions OU. Configure a Suspended OU to hold workload accounts with retired resources. Use Service Control Policies (SCPs) to limit/block resource usage in the Suspended OU</strong> - AWS Cost Explorer lets you explore your AWS costs and usage at both a high level and at a detailed level of analysis, empowering you to dive deeper using several filtering dimensions (e.g., AWS Service, Region, Member Account, etc.). It is not possible to define actionable alerts in AWS Cost Explorer.</p>\n\n<p><strong>Configure GuardDuty in all member accounts within the AWS Organizations organization. Create an SNS topic in each account. Subscribe the security team to the topic so that the security team can receive alerts from GuardDuty via SNS</strong> - It is inefficient and cumbersome to configure GuardDuty in all member accounts within the AWS Organizations organization. It's better to centrally manage GuardDuty for all AWS accounts by using a GuardDuty delegated administrator account within the AWS Organizations organization.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/suspended-ou.html\">https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/suspended-ou.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/benefits-of-using-ous.html\">https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/benefits-of-using-ous.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html</a></p>\n",
        "answers": [
          "<p>Use AWS Organizations to set up a multi-account environment. Organize the accounts into the following Service Control Policies (SCPs): Security, Infrastructure, Workloads, Suspended, and Exceptions. Grant necessary permissions to the accounts by using the SCP guardrails</p>",
          "<p>Configure an AWS Cost Explorer alert to move an AWS account to Exceptions OU if the account reaches a predefined budget threshold. Use Service Control Policies (SCPs) to limit/block resource usage in the Exceptions OU. Configure a Suspended OU to hold workload accounts with retired resources. Use Service Control Policies (SCPs) to limit/block resource usage in the Suspended OU</p>",
          "<p>Use AWS Organizations to set up a multi-account environment. Organize the accounts into the following Organizational Units (OUs): Security, Infrastructure, Workloads, Suspended and Exceptions</p>",
          "<p>Configure an AWS Budget alert to move an AWS account to Exceptions OU if the account reaches a predefined budget threshold. Use Service Control Policies (SCPs) to limit/block resource usage in the Exceptions OU. Configure a Suspended OU to hold workload accounts with retired resources. Use Service Control Policies (SCPs) to limit/block resource usage in the Suspended OU</p>",
          "<p>Configure GuardDuty in all member accounts within the AWS Organizations organization. Create an SNS topic in each account. Subscribe the security team to the topic so that the security team can receive alerts from GuardDuty via SNS</p>",
          "<p>Designate an account within the AWS Organizations organization to be the GuardDuty delegated administrator. Create an SNS topic in this account. Subscribe the security team to the topic so that the security team can receive alerts from GuardDuty via SNS</p>"
        ]
      },
      "correct_response": ["c", "d", "f"],
      "section": "Design Solutions for Organizational Complexity",
      "question_plain": "A retail company is introducing multiple business units as part of its expansion plans. To implement this change, the company will be building several new business-unit-specific workloads by leveraging a variety of AWS services. The company wants to track the expenses of each business unit and limit the spending to a pre-defined threshold. In addition, the solution should allow the security team to identify and respond to threats as quickly as possible for all the workloads across the business units. Also, workload accounts may need to be pulled off into a temporary holding area due to resource audit reasons.\n\nWhich of the following can be combined to build a solution for the given requirements? (Select three)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 57141680,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A social media company manages a multi-AZ VPC environment consisting of public subnets and private subnets. Each public subnet contains a NAT Gateway as well as an Internet Gateway. Most of the company's applications are deployed in the private subnets and these applications read and write data to Kinesis Data Streams. The company has hired you as an AWS Certified Solutions Architect Professional to reduce costs and optimize the applications. Upon analysis in the AWS Cost Explorer, you notice that the cost in the EC2-Other category is consistently high due to the increasing NAT Gateway data transfer charges.</p>\n\n<p>What do you recommend to address this requirement?</p>\n",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an interface VPC endpoint for Kinesis Data Streams in the VPC. Ensure that the VPC endpoint policy allows traffic from the applications</strong></p>\n\n<p>You can use an interface VPC endpoint to keep traffic between your Amazon VPC and Kinesis Data Streams from leaving the Amazon network. Interface VPC endpoints don't require an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Interface VPC endpoints are powered by AWS PrivateLink, an AWS technology that enables private communication between AWS services using an elastic network interface with private IPs in your Amazon VPC. You do not need to change the settings for your streams, producers, or consumers. Simply create an interface VPC endpoint for your Kinesis Data Streams traffic from and to your Amazon VPC-based applications to start flowing through the interface VPC endpoint.</p>\n\n<p>VPC endpoint policies enable you to control access by either attaching a policy to a VPC endpoint or by using additional fields in a policy that is attached to an IAM user, group, or role to restrict access to only occur via the specified VPC endpoint. These policies can be used to restrict access to specific streams to a specified VPC endpoint when used in conjunction with the IAM policies to only grant access to Kinesis data stream actions via the specified VPC endpoint.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-sap-pt/assets/pt3-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/vpc.html\">https://docs.aws.amazon.com/streams/latest/dev/vpc.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an interface VPC endpoint for Kinesis Data Streams in the VPC. Ensure that the applications have the required IAM permissions to use the interface VPC endpoint</strong> - Although you can use an IAM policy to restrict access to specific streams to a specified VPC endpoint by only granting access to Kinesis data stream actions via the specified VPC endpoint. However, you need to make changes in the code for the different applications to assume the relevant IAM role and then make changes in the permissions policy attached to the IAM role to grant access to Kinesis Data Streams via the VPC endpoint. This is not an elegant solution when compared to just using the VPC endpoint policy.</p>\n\n<p><strong>Set up a gateway VPC endpoint for Kinesis Data Streams in the VPC. Ensure that the VPC endpoint policy allows traffic from the applications</strong></p>\n\n<p><strong>Set up a gateway VPC endpoint for Kinesis Data Streams in the VPC. Ensure that the applications have the required IAM permissions to use the gateway VPC endpoint</strong></p>\n\n<p>There are three types of VPC endpoints. You must create the type of VPC endpoint that's required by the endpoint service.</p>\n\n<p>Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS.</p>\n\n<p>GatewayLoadBalancer - Create a Gateway Load Balancer endpoint to send traffic to a fleet of virtual appliances using private IP addresses. You route traffic from your VPC to the Gateway Load Balancer endpoint using route tables. The Gateway Load Balancer distributes traffic to the virtual appliances and can scale with demand.</p>\n\n<p>Gateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses. You route traffic from your VPC to the gateway endpoint using route tables. Gateway endpoints do not enable AWS PrivateLink.</p>\n\n<p>You cannot set up a gateway VPC endpoint for Kinesis Data Streams. Gateway VPC endpoint is only supported for S3 and DynamoDB. Therefore, both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/vpc.html\">https://docs.aws.amazon.com/streams/latest/dev/vpc.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html</a></p>\n",
        "answers": [
          "<p>Set up a gateway VPC endpoint for Kinesis Data Streams in the VPC. Ensure that the VPC endpoint policy allows traffic from the applications</p>",
          "<p>Set up an interface VPC endpoint for Kinesis Data Streams in the VPC. Ensure that the applications have the required IAM permissions to use the interface VPC endpoint</p>",
          "<p>Set up an interface VPC endpoint for Kinesis Data Streams in the VPC. Ensure that the VPC endpoint policy allows traffic from the applications</p>",
          "<p>Set up a gateway VPC endpoint for Kinesis Data Streams in the VPC. Ensure that the applications have the required IAM permissions to use the gateway VPC endpoint</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "Continuous Improvement for Existing Solutions",
      "question_plain": "A social media company manages a multi-AZ VPC environment consisting of public subnets and private subnets. Each public subnet contains a NAT Gateway as well as an Internet Gateway. Most of the company's applications are deployed in the private subnets and these applications read and write data to Kinesis Data Streams. The company has hired you as an AWS Certified Solutions Architect Professional to reduce costs and optimize the applications. Upon analysis in the AWS Cost Explorer, you notice that the cost in the EC2-Other category is consistently high due to the increasing NAT Gateway data transfer charges.\n\nWhat do you recommend to address this requirement?",
      "related_lectures": []
    }
  ]
}
