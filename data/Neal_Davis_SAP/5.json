{
  "count": 35,
  "next": null,
  "previous": null,
  "results": [
    {
      "_class": "assessment",
      "id": 85080513,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company has a security policy that requires that all internal application connectivity must use private IP addresses. A Solutions Architect has created interface endpoints in private subnets to connect to AWS public services. The Solutions Architect tested the configuration and the connectivity failed.</p><p>Which configuration change should the Solutions Architect make to resolve the issue?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>You must ensure that the security group that's associated with the endpoint network interface allows communication between the endpoint network interface and the resources in your VPC that communicate with the service.</p><p>When you create an interface endpoint, endpoint-specific DNS hostnames are generated. For AWS services and AWS Marketplace Partner services, the private DNS option (enabled by default) associates a private hosted zone with your VPC.</p><p>The hosted zone contains a record set for the default DNS name for the service (for example, ec2.us-east-1.amazonaws.com) that resolves to the private IP addresses of the endpoint network interfaces in your VPC. This enables you to make requests to the service using its default DNS hostname instead of the endpoint-specific DNS hostnames.</p><p>With Private DNS enabled on the endpoint, instances can send requests AWS services through the interface endpoint using either the default DNS hostname or the endpoint-specific DNS hostname.</p><p><strong>CORRECT: </strong>\"Configure the security group on the interface endpoint to allow connectivity to the AWS services\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable the private DNS option on the VPC attributes\" is incorrect. Private DNS is enabled by default for endpoints created for AWS services and AWS Marketplace Partner services.</p><p><strong>INCORRECT:</strong> \"Update the route table for the subnets with a route to the interface endpoint\" is incorrect. You do not need to update the route table for interface endpoints.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon Route 53 private hosted zone with a conditional forwarder for the internal application\" is incorrect. This is unnecessary, a private hosted zone is a private hosted zone is associated with the VPC by default when the private DNS option is enabled (which is a default).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/verify-domains.html\">https://docs.aws.amazon.com/vpc/latest/userguide/verify-domains.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-networking-content-delivery-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-networking-content-delivery-sap/</a></p>",
        "answers": [
          "<p>Update the route table for the subnets with a route to the interface endpoint.</p>",
          "<p>Enable the private DNS option on the VPC attributes.</p>",
          "<p>Configure the security group on the interface endpoint to allow connectivity to the AWS services.</p>",
          "<p>Configure an Amazon Route 53 private hosted zone with a conditional forwarder for the internal application.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A company has a security policy that requires that all internal application connectivity must use private IP addresses. A Solutions Architect has created interface endpoints in private subnets to connect to AWS public services. The Solutions Architect tested the configuration and the connectivity failed.Which configuration change should the Solutions Architect make to resolve the issue?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080515,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A company runs a high performance computing (HPC) application in an on-premises data center. The solution consists of a 10-node cluster running Linux with high-speed inter-node connectivity. The company is planning to migrate the application to the AWS Cloud. A Solutions Architect needs to design the solution architecture on AWS to ensure optimum performance for the HPC cluster.</p><p>Which combination of steps will meet these requirements? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>High performance computing applications require that the nodes have very high-bandwidth, low-latency connections between the nodes in the cluster. AWS recommend that the instances are placed in a placement group.</p><p>A cluster placement group should be used as it packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.</p><p>Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communications at scale on AWS. Its custom-built operating system (OS) bypass hardware interface enhances the performance of inter-instance communications, which is critical to scaling these applications.</p><p><strong>CORRECT: </strong>\"Deploy Amazon EC2 instances in a placement group\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Use Amazon EC2 instances that support Elastic Fabric Adapter (EFA)\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Deploy instances across multiple Availability Zones\" is incorrect. To ensure the best inter-node connectivity the instances should be deployed in a single AZ.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 instances that support burstable performance\" is incorrect. This is not a recommended solution for optimum performance of HPC applications.</p><p><strong>INCORRECT:</strong> \"Deploy Amazon EC2 instances in an Auto Scaling group\" is incorrect. This does not enable better performance.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/hpc/efa/\">https://aws.amazon.com/hpc/efa/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p><p><a href=\"https://aws.amazon.com/hpc/\">https://aws.amazon.com/hpc/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-compute-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-compute-sap/</a></p>",
        "answers": [
          "<p>Deploy instances across multiple Availability Zones.</p>",
          "<p>Deploy Amazon EC2 instances in a placement group.</p>",
          "<p>Use Amazon EC2 instances that support Elastic Fabric Adapter (EFA).</p>",
          "<p>Use Amazon EC2 instances that support burstable performance.</p>",
          "<p>Deploy Amazon EC2 instances in an Auto Scaling group.</p>"
        ]
      },
      "correct_response": ["b", "c"],
      "section": "AWS Compute",
      "question_plain": "A company runs a high performance computing (HPC) application in an on-premises data center. The solution consists of a 10-node cluster running Linux with high-speed inter-node connectivity. The company is planning to migrate the application to the AWS Cloud. A Solutions Architect needs to design the solution architecture on AWS to ensure optimum performance for the HPC cluster.Which combination of steps will meet these requirements? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080517,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company has an application that generates data exports which are saved as CSV files in an Amazon S3 bucket. The data is generally confidential and only accessed by IAM users. An individual CSV file must be shared with an external organization. A Solutions Architect used an IAM user account to attempt to perform a PUT Object call to enable a public ACL on the object and it failed with “insufficient permissions”.</p><p>What is the most likely cause of this issue?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The Amazon S3 Block Public Access feature provides settings for access points, buckets, and accounts to help you manage public access to Amazon S3 resources. By default, new buckets, access points, and objects don't allow public access.</p><p>S3 Block Public Access provides four settings. You can apply these settings in any combination to individual access points, buckets, or entire AWS accounts. If you apply a setting to an account, it applies to all buckets and access points that are owned by that account. Similarly, if you apply a setting to a bucket, it applies to all access points associated with that bucket.</p><p>The settings you can configure with the Block Public Access Feature are:</p><p>- IgnorePublicAcls - causes Amazon S3 to ignore all public ACLs on a bucket and any objects that it contains.</p><p>- BlockPublicAcls – PUT bucket ACL and PUT objects requests are blocked if granting public access.</p><p>- BlockPublicPolicy – Rejects requests to PUT a bucket policy if granting public access.</p><p>- RestrictPublicBuckets – Restricts access to principles in the bucket owners’ AWS account.</p><p><strong>CORRECT: </strong>\"The bucket has the BlockPublicAcls setting set to TRUE\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The object ACL does not allow write permissions for the IAM user account\" is incorrect. IAM user accounts are not specified on object ACLs. The object owner (AWS account) should have full permissions by default.</p><p><strong>INCORRECT:</strong> \"The bucket has the BlockPublicPolicy setting set to TRUE\" is incorrect. This would reject any attempts to add a bucket policy that enables public access.</p><p><strong>INCORRECT:</strong> \"The object has a policy assigned that blocks all public access\" is incorrect. You cannot assign policies to objects, they are assigned to buckets.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-storage-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-storage-sap/</a></p>",
        "answers": [
          "<p>The object ACL does not allow write permissions for the IAM user account.</p>",
          "<p>The bucket has the BlockPublicAcls setting set to TRUE.</p>",
          "<p>The bucket has the BlockPublicPolicy setting set to TRUE.</p>",
          "<p>The object has a policy assigned that blocks all public access.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Storage",
      "question_plain": "A company has an application that generates data exports which are saved as CSV files in an Amazon S3 bucket. The data is generally confidential and only accessed by IAM users. An individual CSV file must be shared with an external organization. A Solutions Architect used an IAM user account to attempt to perform a PUT Object call to enable a public ACL on the object and it failed with “insufficient permissions”.What is the most likely cause of this issue?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080519,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company has a line of business (LOB) application that is used for storing sales data for an eCommerce platform. The data is unstructured and stored in an Oracle database running on a single Amazon EC2 instance. The application front end consists of six EC2 instances in three Availability Zones (AZs). Each week the application receives bursts of traffic and application performance suffers. A Solutions Architect must design a solution to address scalability and reliability. The solutions should also eliminate licensing costs.</p><p>Which set of steps should the Solutions Architect take?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The front end can be served by a combination of reserved instances and spot instances to reduce cost. As this is a LOB application we can assume that it will be around for a long time so an RI is suitable.</p><p>For the database layer this can be migrated to DynamoDB. In the case of Oracle not all features are supported when migrating to DynamoDB but for hosting unstructured data and eliminating licensing costs this could be a good solution. AWS DMS can be used to migrate the database and convert the tables.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-24_01-20-20-4655aa26c7ba5de26894a2b6bc1fe00e.jpg\"></p><p><strong>CORRECT: </strong>\"Create an Auto Scaling group for the front end with a combination of Reserved instances and Spot Instances to reduce costs. Convert the tables in the Oracle database into Amazon DynamoDB tables\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group for the front end with a combination of On-Demand and Spot Instances to reduce costs. Migrate the Oracle database into a single Amazon RDS reserved DB instance\" is incorrect. This will not eliminate the licensing costs as Oracle on RDS is still subject to licensing.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group for the front end with a combination of Reserved instances and Spot Instances to reduce costs. Migrate the Oracle database into an Amazon RDS multi-AZ deployment\" is incorrect. As above, Oracle on RDS does not eliminate licensing.</p><p><strong>INCORRECT:</strong> \"Use Spot Instances for the front end to reduce costs. Convert the tables in the Oracle database into Amazon DynamoDB tables\" is incorrect. Spot instances reduce cost but do not increase reliability as they are subject to termination of the maximum configured price is exceeded.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-from-an-on-premises-oracle-database-or-amazon-rds-for-oracle-to-amazon-dynamodb-using-aws-dms-and-aws-sct.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-from-an-on-premises-oracle-database-or-amazon-rds-for-oracle-to-amazon-dynamodb-using-aws-dms-and-aws-sct.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-database-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-database-sap/</a></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-migration-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-migration-sap/</a></p>",
        "answers": [
          "<p>Create an Auto Scaling group for the front end with a combination of On-Demand and Spot Instances to reduce costs. Migrate the Oracle database into a single Amazon RDS reserved DB instance.</p>",
          "<p>Create an Auto Scaling group for the front end with a combination of Reserved instances and Spot Instances to reduce costs. Convert the tables in the Oracle database into Amazon DynamoDB tables.</p>",
          "<p>Create an Auto Scaling group for the front end with a combination of Reserved instances and Spot Instances to reduce costs. Migrate the Oracle database into an Amazon RDS multi-AZ deployment.</p>",
          "<p>Use Spot Instances for the front end to reduce costs. Convert the tables in the Oracle database into Amazon DynamoDB tables.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "A company has a line of business (LOB) application that is used for storing sales data for an eCommerce platform. The data is unstructured and stored in an Oracle database running on a single Amazon EC2 instance. The application front end consists of six EC2 instances in three Availability Zones (AZs). Each week the application receives bursts of traffic and application performance suffers. A Solutions Architect must design a solution to address scalability and reliability. The solutions should also eliminate licensing costs.Which set of steps should the Solutions Architect take?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080521,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A manufacturing company collects data from IoT devices in JSON format. The data is collected, transformed and stored in a data warehouse for analysis using an analytics tool that uses ODBC. The performance of the current solution suffers under high loads due to insufficient compute capacity and incoming data is often lost.</p><p>The application will be migrated to AWS. The solution must support the current analytics tool, resolve the compute constraints, and be cost-effective.</p><p>Which solution meets these requirements?</p>",
        "relatedLectureIds": [],
        "links": [],
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>There are multiple options that work here but we must select the most cost-effective and resilient solution that also resolves the performance constraints. Using a fully serverless architecture meets all of these goals.</p><p>The JSON data can be initially stored in Amazon S3 and then AWS Glue can be directed at the source bucket to run transformation on the data and store the results in another S3 bucket. The Glue Data Catalog is used to store metadata about the data in the S3 data lake.</p><p>Athena can be used to query the transformed data in the S3 bucket. The Amazon Athena ODBC driver can be used to connect to Athena from the existing analytics tool.</p><p>The architecture in the diagram below shows a similar solution using an AWS IoT Analytics channel. This also includes AWS Glue and Athena. Instead of using QuickSight to visualize data, the solution in the question scenario uses a third-party analytics tool connecting over ODBC.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-24_01-21-58-c9c401fb2f74acaf7577a330dac7d026.jpg\"></p><p><strong>CORRECT: </strong>\"Re-architect the application. Load the data into Amazon S3. Use AWS Glue to transform the data. Store the table schema in an AWS Glue Data Catalog. Use Amazon Athena to query the data\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Replatform the application. Use Amazon API Gateway for data ingestion. Use AWS Lambda to transform the JSON data. Create an Amazon Aurora PostgreSQL DB cluster with an Aurora Replica in another Availability Zone. Use Amazon QuickSight to generate reports and visualize data\" is incorrect. This is a workable solution but is not as cost-effective as using a fully serverless architecture.</p><p><strong>INCORRECT:</strong> \"Re-architect the application. Load the data into Amazon S3. Use AWS Lambda to transform the data. Create an Amazon DynamoDB global table across two Regions to store the data and use Amazon Elasticsearch to query the data\" is incorrect. This is another workable solution and you can connect to Elasticsearch using ODBC. However, the DynamoDB global table introduces additional cost and is unnecessary in this solution.</p><p><strong>INCORRECT:</strong> \"Re-architect the application. Load the data into Amazon S3. Use Amazon Kinesis Data Analytics to transform the data. Create an external schema in an AWS Glue Data Catalog. Use Amazon Redshift Spectrum to query the data\" is incorrect. There is no mention of where the data would be output to, and this would not the most cost-effective solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/connect-with-odbc.html\">https://docs.aws.amazon.com/athena/latest/ug/connect-with-odbc.html</a></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html\">https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-analytics-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-analytics-sap/</a></p>",
        "answers": [
          "<p>Re-architect the application. Load the data into Amazon S3. Use AWS Lambda to transform the data. Create an Amazon DynamoDB global table across two Regions to store the data and use Amazon Elasticsearch to query the data.</p>",
          "<p>Replatform the application. Use Amazon API Gateway for data ingestion. Use AWS Lambda to transform the JSON data. Create an Amazon Aurora PostgreSQL DB cluster with an Aurora Replica in another Availability Zone. Use Amazon QuickSight to generate reports and visualize data.</p>",
          "<p>Re-architect the application. Load the data into Amazon S3. Use AWS Glue to transform the data. Store the table schema in an AWS Glue Data Catalog. Use Amazon Athena to query the data.</p>",
          "<p>Re-architect the application. Load the data into Amazon S3. Use Amazon Kinesis Data Analytics to transform the data. Create an external schema in an AWS Glue Data Catalog. Use Amazon Redshift Spectrum to query the data.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Management & Governance",
      "question_plain": "A manufacturing company collects data from IoT devices in JSON format. The data is collected, transformed and stored in a data warehouse for analysis using an analytics tool that uses ODBC. The performance of the current solution suffers under high loads due to insufficient compute capacity and incoming data is often lost.The application will be migrated to AWS. The solution must support the current analytics tool, resolve the compute constraints, and be cost-effective.Which solution meets these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080523,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A Solutions Architect is designing a publicly accessible web application that runs from an Amazon S3 website endpoint. The S3 website is the origin for an Amazon CloudFront distribution. After deploying the solution the operations team ran some tests and received an “Error 403: Access Denied message” when attempting to connect.</p><p>What should the Solutions Architect check to determine the root cause of the issue? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>If your CloudFront distribution is using a website endpoint, verify the following requirements to avoid Access Denied errors:</p><p> • Objects in the bucket must be publicly accessible.</p><p> • Objects in the bucket can't be encrypted by AWS Key Management Service (AWS KMS).</p><p> • The bucket policy must allow access to s3:GetObject.</p><p> • If the bucket policy grants public read access, then the AWS account that owns the bucket must also own the object.</p><p> • The requested objects must exist in the bucket.</p><p> • Amazon S3 Block Public Access must be disabled on the bucket.</p><p> • If Requester Pays is enabled, then the request must include the request-payer parameter.</p><p> • If you're using a Referer header to restrict access from CloudFront to your S3 origin, then review the custom header.</p><p>From this list of potential causes for Access Denied errors we can ascertain that the Solutions Architect should verify if the S3 block public access option is enabled, and whether KMS encryption is enabled on the bucket.</p><p><strong>CORRECT: </strong>\"Check if the S3 block public access option is enabled on the S3 bucket\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Check if the S3 bucket is encrypted using AWS KMS\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Check if object lock is enabled for the objects in the S3 bucket\" is incorrect. Object Lock helps prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. This is not a potential cause of an Access Denied error when trying to connect to the website endpoint.</p><p><strong>INCORRECT:</strong> \"Check if the storage class for objects in the S3 Standard\" is incorrect. The storage class does not need to be S3 Standard. There are other storage classes that would work with this solution.</p><p><strong>INCORRECT:</strong> \"Check the object versioning status of the objects in the S3 bucket\" is incorrect. The versioning status is not relevant here.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-website-cloudfront-error-403/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-website-cloudfront-error-403/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-storage-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-storage-sap/</a></p>",
        "answers": [
          "<p>Check if the S3 block public access option is enabled on the S3 bucket.</p>",
          "<p>Check if object lock is enabled for the objects in the S3 bucket.</p>",
          "<p>Check if the storage class for objects in the S3 Standard.</p>",
          "<p>Check the object versioning status of the objects in the S3 bucket.</p>",
          "<p>Check if the S3 bucket is encrypted using AWS KMS.</p>"
        ]
      },
      "correct_response": ["a", "e"],
      "section": "AWS Storage",
      "question_plain": "A Solutions Architect is designing a publicly accessible web application that runs from an Amazon S3 website endpoint. The S3 website is the origin for an Amazon CloudFront distribution. After deploying the solution the operations team ran some tests and received an “Error 403: Access Denied message” when attempting to connect.What should the Solutions Architect check to determine the root cause of the issue? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080525,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company runs a Java application on Amazon EC2 instances. The DevOps team uses a combination of Amazon CloudFormation and AWS OpsWorks to update the infrastructure and application stacks respectively. During recent updates the DevOps team reported service disruption issues that affected the Java application running on the Amazon EC2 instances.</p><p>Which solution will increase the reliability of application updates?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS OpsWorks is performing the updates to the application. There is no report of the infrastructure stack having any errors and therefore CloudFormation is not relevant.</p><p>AWS OpsWorks supports a blue-green deployment strategy. It does not support a canary deployment strategy.</p><p>A <em>blue-green</em> deployment strategy is a common way to efficiently use separate stacks to deploy an application update to production.</p><p> • The blue environment is the production stack, which hosts the current application.</p><p> • The green environment is the staging stack, which hosts the updated application.</p><p>When you are ready to deploy the updated app to production, you switch user traffic from the blue stack to the green stack, which becomes the new production stack. You then retire the old blue stack.</p><p>This deployment strategy offers the ability to perform testing on the green environment before fully cutting over and this can improve reliability for updates.</p><p><strong>CORRECT: </strong>\"Implement a blue/green deployment strategy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement the canary release strategy\" is incorrect. OpsWorks does not support a canary release methodology.</p><p><strong>INCORRECT:</strong> \"Implement CloudFormation change sets\" is incorrect. CloudFormation is used to deploy the underlying infrastructure stack (e.g. Amazon EC2 and related infrastructure), OpsWorks is being used to deploy the application updates.</p><p><strong>INCORRECT:</strong> \"Implement CloudFormation Stack sets\" is incorrect. As above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/best-deploy.html#best-deploy-environments-blue-green\">https://docs.aws.amazon.com/opsworks/latest/userguide/best-deploy.html#best-deploy-environments-blue-green</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-management-tools-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-management-tools-sap/</a></p>",
        "answers": [
          "<p>Implement a blue/green deployment strategy.</p>",
          "<p>Implement the canary release strategy.</p>",
          "<p>Implement CloudFormation change sets.</p>",
          "<p>Implement CloudFormation Stack sets.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Compute",
      "question_plain": "A company runs a Java application on Amazon EC2 instances. The DevOps team uses a combination of Amazon CloudFormation and AWS OpsWorks to update the infrastructure and application stacks respectively. During recent updates the DevOps team reported service disruption issues that affected the Java application running on the Amazon EC2 instances.Which solution will increase the reliability of application updates?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080527,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application runs across a fleet of Amazon EC2 instances in an Auto Scaling group. Application logs are collected from the EC2 instances using a cron job that is scheduled to run every 30 minutes. The cron job saves the log files to an Amazon S3 bucket. Failures and scaling events have caused some logs to be lost as the instances have been lost before the cron job collected the log files.</p><p>Which of the following options is the MOST reliable way of collecting and preserving the log files?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>You can use the CloudWatch Logs agent to collect application and system logs form an EC2 instance and send them to CloudWatch Logs. Logs automatically flow from the instance to the log stream you create. The batch_count parameter specifies the max number of log events in a batch, up to 10000. Using a value of 1 will result in every log entry being immediately streamed to CloudWatch Logs.</p><p>This is the most reliable method of collecting the logs as by using a streaming solution the logs are immediately saved.</p><p><strong>CORRECT: </strong>\"Use the Amazon CloudWatch Logs agent to stream log messages directly to CloudWatch Logs. Configure the batch_count parameter to 1\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Update the cron job to run every 5 minutes instead of every 30 minutes to reduce the possibility of log files being lost\" is incorrect. This will still result in log entries being lost with up to 5 minutes of data.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events to trigger Amazon Systems Manager Session Manager to run a batch script that collects the log files\" is incorrect. Session Manager is used instead of SSH for connecting securely to EC2 and running commands. It is not well suited to this scenario for collecting log files automatically.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events to trigger and AWS Lambda function that collects the log files using an SSH connection\" is incorrect. You cannot configure CloudWatch Events to trigger for application log entries so this would not work.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartEC2Instance.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartEC2Instance.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-management-tools-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-management-tools-sap/</a></p>",
        "answers": [
          "<p>Use the Amazon CloudWatch Logs agent to stream log messages directly to CloudWatch Logs. Configure the batch_count parameter to 1.</p>",
          "<p>Update the cron job to run every 5 minutes instead of every 30 minutes to reduce the possibility of log files being lost.</p>",
          "<p>Use Amazon CloudWatch Events to trigger Amazon Systems Manager Session Manager to run a batch script that collects the log files.</p>",
          "<p>Use Amazon CloudWatch Events to trigger and AWS Lambda function that collects the log files using an SSH connection.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Management & Governance",
      "question_plain": "An application runs across a fleet of Amazon EC2 instances in an Auto Scaling group. Application logs are collected from the EC2 instances using a cron job that is scheduled to run every 30 minutes. The cron job saves the log files to an Amazon S3 bucket. Failures and scaling events have caused some logs to be lost as the instances have been lost before the cron job collected the log files.Which of the following options is the MOST reliable way of collecting and preserving the log files?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080529,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An on-premises analytics database running on Oracle will be migrated to the cloud. The database runs on a single virtual machine (VM) and multiple client VMs running a Java-based web application that is used to perform SQL queries on the database. All virtual machines will be migrated to the cloud. The database uses 2 TB of storage and each client VM has a different configuration and saves stored procedures and query results in the local file system. There is a 10 Gbit AWS Direct Connect (DX) connection established and the application can be migrated over a scheduled 48-hour change window.</p><p>Which strategy will reduce the operational overhead on the database and have the LEAST impact on the operations staff after the migration?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>There are two key requirements for this migration: reduce the operational overhead associated with the DB; minimize the impact on operations staff following the completion of the migration.</p><p>To reduce the operational overhead of the analytics database it can be migrated to an Amazon RDS Oracle DB. The migration can use the AWS Database Migration Service (DMS).</p><p>The client VMs these can be migrated using AWS Server Migration Service (DMS) which will replicate the existing operating systems into AWS. Amazon EC2 instances can then be created from the AMIs that SMS creates. This solution ensures that the stored procedures and query results in the file system is not lost and this will reduce the operational burden on staff after the migration.</p><p>The client instances should not be behind a ELB as they have different configurations therefore Route 53 A records can be created to connect directly to each instance.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-24_01-29-18-a0a1c0f69fa4fe5a49c4b69168f7e4f0.jpg\"></p><p><strong>CORRECT: </strong>\"Use AWS DMS to migrate the database to Amazon RDS. Replicate the client VMs into AWS using AWS SMS. Create Route 53 A records for each client VM\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS SMS to replicate the database to AWS and create an Amazon EC2 instance. Migrate the Java-based web application to an AWS Elastic Beanstalk environment behind an Application Load Balancer (ALB)\" is incorrect. To lower operational overhead on the DB it can be migrated to RDS.</p><p><strong>INCORRECT:</strong> \"Use AWS DMS to migrate the database to Amazon RedShift. Migrate the Java-based web application to an AWS Elastic Beanstalk environment behind an Application Load Balancer (ALB)\" is incorrect. Oracle Data Warehouses can be migrated to RedShift using DMS and the Schema Conversion Tool (SCT). The question does not explicitly state that this is a data warehouse nor that the SCT should be used.</p><p><strong>INCORRECT:</strong> \"Use AWS SMS to replicate the database to AWS and create an Amazon EC2 instance. Replicate the client VMs into AWS using AWS SMS. Place the EC2 instances behind an Application Load Balancer (ALB)\" is incorrect. The client EC2 instances should not be behind an ALB as they each have different configurations.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p><p><a href=\"https://aws.amazon.com/dms/schema-conversion-tool/\">https://aws.amazon.com/dms/schema-conversion-tool/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-migration-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-migration-sap/</a></p>",
        "answers": [
          "<p>Use AWS DMS to migrate the database to Amazon RDS. Replicate the client VMs into AWS using AWS SMS. Create Route 53 A records for each client VM.</p>",
          "<p>Use AWS SMS to replicate the database to AWS and create an Amazon EC2 instance. Migrate the Java-based web application to an AWS Elastic Beanstalk environment behind an Application Load Balancer (ALB).</p>",
          "<p>Use AWS DMS to migrate the database to Amazon RedShift. Migrate the Java-based web application to an AWS Elastic Beanstalk environment behind an Application Load Balancer (ALB).</p>",
          "<p>Use AWS SMS to replicate the database to AWS and create an Amazon EC2 instance. Replicate the client VMs into AWS using AWS SMS. Place the EC2 instances behind an Application Load Balancer (ALB).</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Migration & Transfer",
      "question_plain": "An on-premises analytics database running on Oracle will be migrated to the cloud. The database runs on a single virtual machine (VM) and multiple client VMs running a Java-based web application that is used to perform SQL queries on the database. All virtual machines will be migrated to the cloud. The database uses 2 TB of storage and each client VM has a different configuration and saves stored procedures and query results in the local file system. There is a 10 Gbit AWS Direct Connect (DX) connection established and the application can be migrated over a scheduled 48-hour change window.Which strategy will reduce the operational overhead on the database and have the LEAST impact on the operations staff after the migration?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080531,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company uses an AWS account with resources deployed in multiple Regions globally. Operations teams deploy and manage resources within each Region. Some Region-specific service quotas have been reached causing an inability for the local operations teams to deploy resources. A centralized cloud team is responsible for monitoring and updating service quotas. The cloud team needs to create an automated and operationally efficient solution to proactively monitor service quotas. Monitoring should occur every 15 minutes and send alerts when a team exceeds 80% utilization.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon EventBridge (formerly Amazon CloudWatch Events) can be used to run a function on a schedule. The function can retrieve the most current utilization and service limit data. The function can then trigger an SNS topic if the utilization is above 80%. This solution meets all requirements.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule that triggers an AWS Lambda function to use AWS Trusted Advisor to retrieve the most current utilization and service limit data. If the current utilization is above 80%, publish a message to an Amazon SNS topic to alert the cloud team\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule that triggers an AWS Lambda function to use AWS Trusted Advisor to retrieve the most current utilization and service limit data. If the current utilization is above 80%, use AWS Budgets to send an alert to the cloud team\" is incorrect.</p><p>AWS Budgets is used for tracking and alerting on spend on your AWS account, it is not a messaging service for alerting.</p><p><strong>INCORRECT:</strong> \"Create a scheduled AWS Config rule to trigger an AWS Lambda function to call the GetServiceQuota API. If any service utilization is above 80%, publish a message to an Amazon SNS topic to alert the cloud team\" is incorrect.</p><p>The GetServiceQuota API will simply list the current quota, it does not return data showing how much of that quota has been utilized.</p><p><strong>INCORRECT:</strong> \"Create a scheduled AWS Config rule to trigger an AWS Lambda function to call the ListServiceQuotas API. If any service utilization is above 80%, publish a message to an Amazon SNS topic to alert the cloud team\" is incorrect.</p><p>The ListServiceQuotas API lists the service quota applied to a specific service. It also does not return data showing how much of that quota has been utilized.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awssupport/latest/user/trustedadvisor.html\">https://docs.aws.amazon.com/awssupport/latest/user/trustedadvisor.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-management-tools-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-management-tools-sap/</a></p>",
        "answers": [
          "<p>Create a scheduled AWS Config rule to trigger an AWS Lambda function to call the GetServiceQuota API. If any service utilization is above 80%, publish a message to an Amazon SNS topic to alert the cloud team.</p>",
          "<p>Create a scheduled AWS Config rule to trigger an AWS Lambda function to call the ListServiceQuotas API. If any service utilization is above 80%, publish a message to an Amazon SNS topic to alert the cloud team.</p>",
          "<p>Create an Amazon EventBridge rule that triggers an AWS Lambda function to use AWS Trusted Advisor to retrieve the most current utilization and service limit data. If the current utilization is above 80%, use AWS Budgets to send an alert to the cloud team.</p>",
          "<p>Create an Amazon EventBridge rule that triggers an AWS Lambda function to use AWS Trusted Advisor to retrieve the most current utilization and service limit data. If the current utilization is above 80%, publish a message to an Amazon SNS topic to alert the cloud team.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Management & Governance",
      "question_plain": "A company uses an AWS account with resources deployed in multiple Regions globally. Operations teams deploy and manage resources within each Region. Some Region-specific service quotas have been reached causing an inability for the local operations teams to deploy resources. A centralized cloud team is responsible for monitoring and updating service quotas. The cloud team needs to create an automated and operationally efficient solution to proactively monitor service quotas. Monitoring should occur every 15 minutes and send alerts when a team exceeds 80% utilization.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080533,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A security team has discovered that developers have been storing IAM secret access keys in AWS CodeCommit repositories. The security team requires that measures are put in place to automatically find and remediate all instances of this vulnerability on an ongoing basis.</p><p>Which solution meets these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>You can configure a CodeCommit repository so that code pushes or other events trigger actions, such as sending a notification from Amazon Simple Notification Service (Amazon SNS) or invoking a function in AWS Lambda. You can create up to 10 triggers for each CodeCommit repository. In this case you can trigger AWS Lambda to scan the code for access keys</p><p><strong>CORRECT:</strong> \"Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If any credentials are found, disable them and notify the user\" is correct.</p><p><strong>INCORRECT: </strong>\"Create an Amazon Macie job that scans AWS CodeCommit repositories for credentials. If any credentials are found an AWS Lambda function should be triggered that disables the credentials\" is incorrect. Macie scans Amazon S3 buckets but you cannot see the S3 bucket assigned to CodeCommit as it’s an AWS managed service bucket.</p><p><strong>INCORRECT:</strong> \"Use AWS Trusted Advisor to check for unsecured AWS credentials. If any unsecured credentials are found, use AWS Secrets Manager to rotate the credentials\" is incorrect. Secrets Manager cannot be used for rotating secret access keys.</p><p><strong>INCORRECT:</strong> \"Run a cron job on an Amazon EC2 instance to check the CodeCommit repositories for unsecured credentials. If any unsecured credentials are found, generate new credentials and store them in AWS KMS\" is incorrect. EC2 is not a very cost-effective option for running this task and KMS is used for storing encryption keys, not secret access keys.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/security-identity-compliance-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/security-identity-compliance-sap/</a></p>",
        "answers": [
          "<p>Create an Amazon Macie job that scans AWS CodeCommit repositories for credentials. If any credentials are found an AWS Lambda function should be triggered that disables the credentials.</p>",
          "<p>Use AWS Trusted Advisor to check for unsecured AWS credentials. If any unsecured credentials are found, use AWS Secrets Manager to rotate the credentials.</p>",
          "<p>Run a cron job on an Amazon EC2 instance to check the CodeCommit repositories for unsecured credentials. If any unsecured credentials are found, generate new credentials and store them in AWS KMS.</p>",
          "<p>Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If any credentials are found, disable them and notify the user.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A security team has discovered that developers have been storing IAM secret access keys in AWS CodeCommit repositories. The security team requires that measures are put in place to automatically find and remediate all instances of this vulnerability on an ongoing basis.Which solution meets these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080535,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>An eCommerce website consists of a two-tier architecture. Amazon EC2 instances in an Auto Scaling group are used for the web server layer behind an Application Load Balancer (ALB). The web servers run a PHP application on Apache Tomcat. The database layer runs on an Aurora MySQL database instance.</p><p>Recently, a large sales event caused some errors to occur for customers when placing orders on the website. The operations team collected logs from the web servers and reviewed Aurora DB cluster performance metrics. Several web servers were terminated by the ASG before the logs could be collected and the Aurora metrics were not sufficient for query performance analysis.</p><p>Which combination of steps should a Solutions Architect take to improve application performance visibility during peak traffic events? (Select THREE.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", "", ""],
        "explanation": "<p>For Amazon Aurora you can monitor the MySQL error log, slow query log, and the general log. The MySQL error log is generated by default; you can generate the slow query and general logs by setting parameters in your DB parameter group. This data will help troubleshoot any issues with the Aurora database layer.</p><p>AWS X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. You can use the AWS SDK to develop programs that use the X-Ray API. In this case the application runs on PHP so the X-Ray SDK for PHP should be used.</p><p>Some instances were terminated by the ASG before the logs could be collected. To prevent this issue from reoccurring, the CloudWatch Logs agent can be installed on instances. The agent streams logs straight to CloudWatch logs at regular intervals (configurable) so the logs will not be lost next time.</p><p><strong>CORRECT: </strong>\"Configure the Aurora MySQL DB cluster to generate slow query logs by setting parameters in the parameter group\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for PHP\" is also a correct answer.</p><p><strong>CORRECT: </strong>\"Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Configure the Aurora MySQL DB cluster to generate error logs by setting parameters in the parameter group\" is incorrect. Error logs are generated by default; however the slow query logs can be added by configuring parameters.</p><p><strong>INCORRECT:</strong> \"Configure AWS CloudTrail to collect API activity from Amazon EC2 and Aurora and analyze with Amazon Athena\" is incorrect. API activity tells us who did what API action and when. In this case we are attempting to troubleshoot performance issues so auditing data is not as useful as performance metrics, tracing, and logs.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon EventBridge rule that triggers Lambda upon Aurora error events and saves logs to Amazon S3 for analysis with Amazon Athena\" is incorrect. EventBridge is not used for triggering on error events occurring in Aurora.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_LogAccess.Concepts.MySQL.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_LogAccess.Concepts.MySQL.html</a></p><p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-api.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-api.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_GettingStarted.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_GettingStarted.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-database-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-database-sap/</a></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-application-integration-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-application-integration-sap/</a></p>",
        "answers": [
          "<p>Configure the Aurora MySQL DB cluster to generate slow query logs by setting parameters in the parameter group.</p>",
          "<p>Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for PHP.</p>",
          "<p>Configure the Aurora MySQL DB cluster to generate error logs by setting parameters in the parameter group.</p>",
          "<p>Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs.</p>",
          "<p>Configure AWS CloudTrail to collect API activity from Amazon EC2 and Aurora and analyze with Amazon Athena.</p>",
          "<p>Configure an Amazon EventBridge rule that triggers Lambda upon Aurora error events and saves logs to Amazon S3 for analysis with Amazon Athena.</p>"
        ]
      },
      "correct_response": ["a", "b", "d"],
      "section": "AWS Database",
      "question_plain": "An eCommerce website consists of a two-tier architecture. Amazon EC2 instances in an Auto Scaling group are used for the web server layer behind an Application Load Balancer (ALB). The web servers run a PHP application on Apache Tomcat. The database layer runs on an Aurora MySQL database instance.Recently, a large sales event caused some errors to occur for customers when placing orders on the website. The operations team collected logs from the web servers and reviewed Aurora DB cluster performance metrics. Several web servers were terminated by the ASG before the logs could be collected and the Aurora metrics were not sufficient for query performance analysis.Which combination of steps should a Solutions Architect take to improve application performance visibility during peak traffic events? (Select THREE.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080537,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application runs on Amazon EC2 instances in a private subnet within an Amazon VPC. The application stores files in a specific Amazon S3 bucket. The files should not traverse the internet and only the application instances should be granted access to save files to the S3 bucket. A gateway endpoint has been created for Amazon S3 and connected to the Amazon VPC.</p><p>What additional steps should a Solutions Architect take to meet the stated requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>An endpoint policy can be attached to a gateway endpoint that will restrict access to the specific S3 bucket. This means the gateway endpoint can only be used to access that one particular bucket. To restrict access to the bucket to the EC2 instances only an IAM role can be assigned to the instances and then a bucket policy can be added to the bucket that restricts access only to the IAM role. This solution meets all requirements.</p><p>The diagram below depicts this configuration:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-24_01-34-55-22156f7acc16d98f79ee6103fa7a2ab5.jpg\"></p><p><strong>CORRECT: </strong>\"Attach an endpoint policy to the gateway endpoint that restricts access to the specific S3 bucket. Assign an IAM role to the EC2 instances and attach a policy to the S3 bucket that grants access only to this role\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Attach an endpoint policy to the gateway endpoint that restricts access to the specific S3 bucket. Attach a bucket policy to the S3 bucket that grants access only to the VPC endpoint\" is incorrect. This does not restrict access to the specific EC2 instances running the application. This will allow any EC2 instances in the private subnet to access the S3 bucket (if they have S3 permissions).</p><p><strong>INCORRECT:</strong> \"Attach a bucket policy to the S3 bucket that grants access to the EC2 instances only using the aws:SourceIp condition. Update the VPC route table so only the application EC2 instances can access the VPC endpoint\" is incorrect. The aws:SourceIp condition is used for restricting access based on public IP addresses, not private IP addresses.</p><p><strong>INCORRECT:</strong> \"Attach an endpoint policy to the gateway endpoint that restricts access to S3 in the current Region. Attach a bucket policy to the S3 bucket that grants access to ec2.amazonaws.com service only\" is incorrect. This solution does not lock down the specific bucket or the specific application EC2 instances and fails to meet all requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-access.html#vpc-endpoint-policies\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-access.html#vpc-endpoint-policies</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-storage-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-storage-sap/</a></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-networking-content-delivery-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-networking-content-delivery-sap/</a></p>",
        "answers": [
          "<p>Attach an endpoint policy to the gateway endpoint that restricts access to the specific S3 bucket. Attach a bucket policy to the S3 bucket that grants access only to the VPC endpoint.</p>",
          "<p>Attach a bucket policy to the S3 bucket that grants access to the EC2 instances only using the aws:SourceIp condition. Update the VPC route table so only the application EC2 instances can access the VPC endpoint.</p>",
          "<p>Attach an endpoint policy to the gateway endpoint that restricts access to the specific S3 bucket. Assign an IAM role to the EC2 instances and attach a policy to the S3 bucket that grants access only to this role.</p>",
          "<p>Attach an endpoint policy to the gateway endpoint that restricts access to S3 in the current Region. Attach a bucket policy to the S3 bucket that grants access to ec2.amazonaws.com service only.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "An application runs on Amazon EC2 instances in a private subnet within an Amazon VPC. The application stores files in a specific Amazon S3 bucket. The files should not traverse the internet and only the application instances should be granted access to save files to the S3 bucket. A gateway endpoint has been created for Amazon S3 and connected to the Amazon VPC.What additional steps should a Solutions Architect take to meet the stated requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080539,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A fleet of EC2 instances generate a large quantity of data and store the data on an Amazon EFS file system. The EC2 instances also backup the data by uploading to an Amazon S3 bucket in another Region on a daily basis. Some S3 uploads have been failing and the storage costs have significantly increased.</p><p>The operations team has removed the failed uploads. How can a Solutions Architect configure the backup jobs to efficiently backup data to S3 while reducing storage costs?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>S3’s multipart upload feature accelerates the uploading of large objects by allowing you to split them up into logical parts that can be uploaded in parallel. If you initiate a multipart upload but never finish it, the in-progress upload occupies some storage space and will incur storage charges.</p><p>However, these uploads are not visible when you list the contents of a bucket. You can use a lifecycle rule to clean up incomplete multipart uploads which will reduce storage space.</p><p>The rule configuration can be seen in the image below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-24_01-36-30-e025680a4f84e975b796ecfba1d8f325.jpg\"></p><p><strong>CORRECT: </strong>\"Use multipart upload for the backup jobs. Create a lifecycle policy for the incomplete multipart uploads on the S3 bucket to prevent new failed uploads from accumulating\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use S3 Transfer Acceleration for the backup jobs. Use the Multi-Object Delete operation to remove the old uploads on a daily basis\" is incorrect. S3 transfer acceleration is for optimizing the speed of uploads, multipart upload is a better solution for running uploads of parts of objects in parallel. Using multi-object delete is not an automated process, it will be much better to use a lifecycle rule for efficiency.</p><p><strong>INCORRECT:</strong> \"Use multipart upload for the backup jobs. Create a lifecycle policy that archives data to Amazon S3 Glacier on a daily basis\" is incorrect. The failed uploads should not be archived, they should be deleted.</p><p><strong>INCORRECT:</strong> \"Use S3 Transfer Acceleration for the backup jobs. Create a lifecycle policy for the incomplete multipart uploads on the S3 bucket to prevent new failed uploads from accumulating\" is incorrect. As explained above for S3 transfer acceleration.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/s3-lifecycle-management-update-support-for-multipart-uploads-and-delete-markers/\">https://aws.amazon.com/blogs/aws/s3-lifecycle-management-update-support-for-multipart-uploads-and-delete-markers/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-storage-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-storage-sap/</a></p>",
        "answers": [
          "<p>Use S3 Transfer Acceleration for the backup jobs. Use the Multi-Object Delete operation to remove the old uploads on a daily basis.</p>",
          "<p>Use multipart upload for the backup jobs. Create a lifecycle policy that archives data to Amazon S3 Glacier on a daily basis.</p>",
          "<p>Use S3 Transfer Acceleration for the backup jobs. Create a lifecycle policy for the incomplete multipart uploads on the S3 bucket to prevent new failed uploads from accumulating.</p>",
          "<p>Use multipart upload for the backup jobs. Create a lifecycle policy for the incomplete multipart uploads on the S3 bucket to prevent new failed uploads from accumulating.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Storage",
      "question_plain": "A fleet of EC2 instances generate a large quantity of data and store the data on an Amazon EFS file system. The EC2 instances also backup the data by uploading to an Amazon S3 bucket in another Region on a daily basis. Some S3 uploads have been failing and the storage costs have significantly increased.The operations team has removed the failed uploads. How can a Solutions Architect configure the backup jobs to efficiently backup data to S3 while reducing storage costs?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080541,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A development team created a service that uses an AWS Lambda function to store information in an Amazon RDS Database. The database credentials are stored in clear text in the Lambda function code. A Solutions Architect is advising the development team on how to better secure the service. Which of the following should the Solutions Architect recommend? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>AWS Secrets Manager can be used to rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. You can configure Secrets Manager to rotate secrets automatically, which can help you meet your security and compliance needs. Secrets Manager offers built-in integrations for MySQL, PostgreSQL, and Amazon Aurora on Amazon RDS, and can rotate credentials for these databases natively.</p><p>The steps to configure Secrets Manger for this scenario are to first create the database credentials in Secrets Manager (without enabling rotation), and then to update the Lambda function to use Secrets Manager to obtain the DB credentials. At that point secret rotation can be enabled.</p><p>The image below shows the screen in Secrets Manager where you can select an existing database for which you will store credentials:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-24_01-38-11-44e6ccc0b2af02fa139f311b63bafb66.jpg\"></p><p><strong>CORRECT: </strong>\"Create encrypted database credentials in AWS Secrets Manager for the Amazon RDS database\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Configure Lambda to use the stored database credentials in AWS Secrets Manager and enable automatic rotation\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create a Lambda function to rotate the credentials every hour by deploying a new Lambda version with the updated credentials\" is incorrect. The credentials can be rotated in Secrets Manager and a new Lambda function is not required.</p><p><strong>INCORRECT:</strong> \"Configure Lambda to use the stored database credentials in AWS KMS and enabled automatic key rotation\" is incorrect. KMS is used for storing encryption keys, not credentials.</p><p><strong>INCORRECT:</strong> \"Store the Amazon RDS database credentials in AWS KMS using imported key material\" is incorrect. You cannot import database credentials into KMS, you can only import key material for a CMK.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets-manager/\">https://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets-manager/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/security-identity-compliance-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/security-identity-compliance-sap/</a></p>",
        "answers": [
          "<p>Create encrypted database credentials in AWS Secrets Manager for the Amazon RDS database.</p>",
          "<p>Configure Lambda to use the stored database credentials in AWS Secrets Manager and enable automatic rotation.</p>",
          "<p>Create a Lambda function to rotate the credentials every hour by deploying a new Lambda version with the updated credentials.</p>",
          "<p>Configure Lambda to use the stored database credentials in AWS KMS and enabled automatic key rotation.</p>",
          "<p>Store the Amazon RDS database credentials in AWS KMS using imported key material.</p>"
        ]
      },
      "correct_response": ["a", "b"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A development team created a service that uses an AWS Lambda function to store information in an Amazon RDS Database. The database credentials are stored in clear text in the Lambda function code. A Solutions Architect is advising the development team on how to better secure the service. Which of the following should the Solutions Architect recommend? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080543,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A serverless application uses an AWS Lambda function behind and Amazon API Gateway REST API. During busy periods thousands of simultaneous invocations are required and requests fail multiple times before succeeding. The operations team has checked for AWS Lambda errors and did not find any. A Solutions Architect must investigate the root cause of the issue. What is the most likely cause of this problem?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Throttling can be configured for API Gateway at the stage or usage plan level. Amazon API Gateway provides two basic types of throttling-related settings:</p><p><em> </em>• <em>Server-side throttling limits</em> are applied across all clients. These limit settings exist to prevent your API—and your account—from being overwhelmed by too many requests.</p><p><em> </em>• <em>Per-client throttling limits</em> are applied to clients that use API keys associated with your usage policy as client identifier.</p><p>In this case it is likely that the server-side throttling limits are configured to a value that is too low for the periods of peak usage. This is causing requests to fail repeatedly before they finally make it through to the Lambda function.</p><p>This is the most likely cause of the issue as it is clear the requests are not reaching Lambda or there would be errors logged by the function.</p><p><strong>CORRECT: </strong>\"The throttle limit on the REST API is configured too low. During busy periods some requests are being throttled and are not reaching the Lambda function\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The Lambda is configured with too little memory causing the function to fail at peak load\" is incorrect. If this was the case we would see errors being logged by Lambda but there aren’t any.</p><p><strong>INCORRECT:</strong> \"The Lambda function is set to use synchronous invocation and the REST API is calling the function using asynchronous invocation\" is incorrect. API Gateway can be configured to invoke a Lambda function both synchronously and asynchronously.</p><p><strong>INCORRECT:</strong> \"The API is using the non-proxy integration with Lambda when it should be using proxy integration\" is incorrect. API Gateway can be configured to connect to Lambda using both proxy and non-proxy integrations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-networking-content-delivery-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-networking-content-delivery-sap/</a></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-compute-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-compute-sap/</a></p>",
        "answers": [
          "<p>The Lambda is configured with too little memory causing the function to fail at peak load.</p>",
          "<p>The Lambda function is set to use synchronous invocation and the REST API is calling the function using asynchronous invocation.</p>",
          "<p>The throttle limit on the REST API is configured too low. During busy periods some requests are being throttled and are not reaching the Lambda function.</p>",
          "<p>The API is using the non-proxy integration with Lambda when it should be using proxy integration.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A serverless application uses an AWS Lambda function behind and Amazon API Gateway REST API. During busy periods thousands of simultaneous invocations are required and requests fail multiple times before succeeding. The operations team has checked for AWS Lambda errors and did not find any. A Solutions Architect must investigate the root cause of the issue. What is the most likely cause of this problem?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080545,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company is planning to migrate a containerized application to Amazon ECS. The company wishes to reduce instance costs as much as possible whilst reducing the probability of service interruptions. How should a Solutions Architect configure the solution?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Spot instances are the lowest cost option but can be terminated by AWS at any time if they need the capacity back or the maximum configured price is exceeded. This can lead to service interruptions as the underlying container instances are terminated.</p><p>Automated Spot Instance Draining will automatically place Spot instances in “DRAINING” state upon the receipt of two minute interruption notice. ECS tasks running on Spot instances will automatically be triggered for shutdown before the instance terminates and replacement tasks will be scheduled elsewhere on the cluster. No new ECS service tasks will be started on the instances once the termination process has begun.</p><p>ECS takes over the coordination of termination of tasks with the termination of the underlying EC2 instance using the inherent instance “DRAINING” functionality. The managed termination, scheduling of replacement tasks and graceful termination of the LB connections reduces the probability of service interruptions. This makes it easier for customers to use Spot instances as part of their ECS cluster.</p><p><strong>CORRECT: </strong>\"Use Amazon ECS Spot instances and configure Spot Instance Draining\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon ECS Cluster Auto Scaling (CAS) and configure a warm-up time\" is incorrect. The warm-up time defines the number of seconds it takes for a newly launched instance to warm up and is not relevant here.</p><p><strong>INCORRECT:</strong> \"Use Amazon ECS Reserved instances and configure termination protection\" is incorrect. Termination protection with RIs will provide cost savings and protection from service interruptions but it is not the lowest cost option.</p><p><strong>INCORRECT:</strong> \"Use Amazon ECS with Application Auto Scaling and suspend dynamic scaling\" is incorrect. This answer does not provide a solution for lowering instance costs and suspension of dynamic scaling is going to prevent scale-in and scale-out events from occurring.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/09/amazon-ecs-supports-automated-draining-for-spot-instances-running-ecs-services/\">https://aws.amazon.com/about-aws/whats-new/2019/09/amazon-ecs-supports-automated-draining-for-spot-instances-running-ecs-services/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-compute-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-compute-sap/</a></p>",
        "answers": [
          "<p>Use Amazon ECS Spot instances and configure Spot Instance Draining.</p>",
          "<p>Use Amazon ECS Cluster Auto Scaling (CAS) and configure a warm-up time.</p>",
          "<p>Use Amazon ECS Reserved instances and configure termination protection.</p>",
          "<p>Use Amazon ECS with Application Auto Scaling and suspend dynamic scaling.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Compute",
      "question_plain": "A company is planning to migrate a containerized application to Amazon ECS. The company wishes to reduce instance costs as much as possible whilst reducing the probability of service interruptions. How should a Solutions Architect configure the solution?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080547,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect wants to make sure that only IAM users with appropriate permissions can access a new Amazon API Gateway endpoint. How can the Solutions Architect design the API Gateway access control to meet this requirement?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>To allow an API caller to invoke the API or refresh its caching, you must create IAM policies that permit a specified API caller to invoke the API method for which the IAM user authentication is enabled. The API developer sets the method's authorizationType property to AWS_IAM to require that the caller submit the IAM user's access keys to be authenticated. Then, you attach the policy to an IAM user representing the API caller, to an IAM group containing the user, or to an IAM role assumed by the user.</p><p>In this IAM permissions policy statement, the IAM Resource element contains a list of deployed API methods identified by given HTTP verbs and API Gateway resource paths. The IAM Action element contains the required API Gateway API executing actions. These actions include execute-api:Invoke or execute-api:InvalidateCache, where execute-api designates the underlying API execution component of API Gateway.</p><p><strong>CORRECT: </strong>\"Set the authorization to AWS_IAM for the API Gateway method. Create a permissions policy that grants execute-api:Invoke permission on the REST API resource and attach it to a group containing the IAM user accounts\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function as a custom authorizer and ask the API client to pass the key and secret when making the call, and then use Lambda to validate the key/secret pair against the IAM system\" is incorrect. This solution does is not a valid method of integrating the REST API with IAM.</p><p><strong>INCORRECT:</strong> \"Set the authorization to AWS_IAM for the API Gateway method. Create a permissions policy that grants lambda:InvokeFunction permission on the REST API resource and attach it to a group containing the IAM user accounts\" is incorrect. The permission that is being granted here is for invoking a Lambda function, not a REST API.</p><p><strong>INCORRECT:</strong> \"Create a client certificate for API Gateway. Distribute the certificate to the AWS users that need to access the endpoint. Enable the API caller to pass the client certificate when accessing the endpoint\" is incorrect. Client certificates are a different way to authenticate, this is not using IAM user accounts.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-networking-content-delivery-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-networking-content-delivery-sap/</a></p>",
        "answers": [
          "<p>Set the authorization to AWS_IAM for the API Gateway method. Create a permissions policy that grants execute-api:Invoke permission on the REST API resource and attach it to a group containing the IAM user accounts.</p>",
          "<p>Create an AWS Lambda function as a custom authorizer and ask the API client to pass the key and secret when making the call, and then use Lambda to validate the key/secret pair against the IAM system.</p>",
          "<p>Set the authorization to AWS_IAM for the API Gateway method. Create a permissions policy that grants lambda:InvokeFunction permission on the REST API resource and attach it to a group containing the IAM user accounts.</p>",
          "<p>Create a client certificate for API Gateway. Distribute the certificate to the AWS users that need to access the endpoint. Enable the API caller to pass the client certificate when accessing the endpoint.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A Solutions Architect wants to make sure that only IAM users with appropriate permissions can access a new Amazon API Gateway endpoint. How can the Solutions Architect design the API Gateway access control to meet this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080549,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A web application is being deployed on Amazon EC2 instances and requires that users authenticate before they can access content. The solution needs to be configured so that it is highly available. Once authenticated, users should remain connected even if an underlying instance fails.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>This solution can be easily solved by using a combination of Auto Scaling and load balancing to ensure high availability of the web application. To ensure users remain connected even if the instance they are on fails, the authenticated connection details can be stored in Amazon DynamoDB. This helps reduce the need for users to re-authenticate. DynamoDB is often used for session state data and ElastiCache is another service that can be used for this use case.</p><p><strong>CORRECT: </strong>\"Create an Auto Scaling group for the EC2 instances and use an Application Load Balancer to direct incoming requests. Use Amazon DynamoDB to save the authenticated connection details\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS Global Accelerator to forward requests to the EC2 instances. Use Amazon DynamoDB to save the authenticated connection details\" is incorrect. Global Accelerator is used for directing connections to different implementations of an application in different Regions. It is not suitable for the requirements in this scenario.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group for the EC2 instances and use an Application Load Balancer to direct incoming requests. Save the authenticated connection details on Amazon EBS volumes\" is incorrect. EBS volumes are locally attached to EC2 instances and will be lost if the instance is terminated. A serverless service that is suitable for storing session state data should be used instead (e.g. DynamoDB or ElastiCache).</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group for the EC2 instances and use an Application Load Balancer to direct incoming requests. Use AWS Secrets Manager to save the authenticated connection details\" is incorrect. Secrets Manager is used for storing credentials that are used by applications but would not be used for storing session state data for users of the application.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/features/\">https://aws.amazon.com/dynamodb/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-database-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-database-sap/</a></p>",
        "answers": [
          "<p>Use AWS Global Accelerator to forward requests to the EC2 instances. Use Amazon DynamoDB to save the authenticated connection details.</p>",
          "<p>Create an Auto Scaling group for the EC2 instances and use an Application Load Balancer to direct incoming requests. Save the authenticated connection details on Amazon EBS volumes.</p>",
          "<p>Create an Auto Scaling group for the EC2 instances and use an Application Load Balancer to direct incoming requests. Use AWS Secrets Manager to save the authenticated connection details.</p>",
          "<p>Create an Auto Scaling group for the EC2 instances and use an Application Load Balancer to direct incoming requests. Use Amazon DynamoDB to save the authenticated connection details.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Compute",
      "question_plain": "A web application is being deployed on Amazon EC2 instances and requires that users authenticate before they can access content. The solution needs to be configured so that it is highly available. Once authenticated, users should remain connected even if an underlying instance fails.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080551,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company stores highly confidential information in an Amazon S3 bucket. The security team have evaluated the security of the configuration and have come up with some new requirements that must be met. The security team now requires the ability to identify the IP addresses that make requests to the bucket to be able to identify malicious actors. They additionally require that any changes to the bucket policy are automatically remediated and alerts of these changes are sent to their team members.</p><p>Which strategies should a Solutions Architect use to meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon S3 server access logging provides detailed records for the requests that are made to a bucket. This includes the IP addresses that issued the requests. S3 stores server access logs as objects in an S3 bucket. Athena can then be used to query Amazon S3 access logs using SQL queries.</p><p>The AWS Config Auto Remediation feature automatically remediates non-compliant resources evaluated by AWS Config rules. You can associate remediation actions with AWS Config rules and choose to execute them automatically to address non-compliant resources without manual intervention. An AWS Config rule can be applied to identify and remediate any unauthorized changes to the policy associated with the S3 bucket. Amazon SNS can be integrated as a destination for alerts.</p><p><strong>CORRECT: </strong>\"Identify the IP addresses in Amazon S3 requests with Amazon S3 access logs and Amazon Athena. Use AWS Config with Auto Remediation to remediate any changes to S3 bucket policies. Configure alerting with AWS Config and Amazon SNS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Logs with the Amazon Athena connector to identify the IP addresses in Amazon S3 requests. Use CloudWatch Events rules with AWS Lambda to automatically remediate S3 bucket policy changes. Configure alerting with Amazon SNS\" is incorrect. CloudWatch Logs will not contain information about the IP addresses that issued requests to Amazon S3, AWS CloudTrail should be used instead.</p><p><strong>INCORRECT:</strong> \"Use Amazon Macie with to identify the IP addresses in Amazon S3 requests. Use AWS Lambda with Macie to automatically remediate S3 bucket policy changes. Use Macie automatic alerting capabilities for alerts\" is incorrect. Macie is used to identify personally identifiable information rather than IP addresses.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudTrail trail and log management events. Use CloudWatch Events rules with AWS Lambda to automatically remediate S3 bucket policy changes. Configure alerting with Amazon SNS\" is incorrect. CloudTrail can be used to identify object-level actions but you must enable object-level events, not management events.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/using-s3-access-logs-to-identify-requests.html#querying-s3-access-logs-for-requests\">https://docs.aws.amazon.com/AmazonS3/latest/dev/using-s3-access-logs-to-identify-requests.html#querying-s3-access-logs-for-requests</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-storage-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-storage-sap/</a></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-management-tools-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-management-tools-sap/</a></p>",
        "answers": [
          "<p>Use Amazon CloudWatch Logs with the Amazon Athena connector to identify the IP addresses in Amazon S3 requests. Use CloudWatch Events rules with AWS Lambda to automatically remediate S3 bucket policy changes. Configure alerting with Amazon SNS.</p>",
          "<p>Use Amazon Macie with to identify the IP addresses in Amazon S3 requests. Use AWS Lambda with Macie to automatically remediate S3 bucket policy changes. Use Macie automatic alerting capabilities for alerts.</p>",
          "<p>Create an AWS CloudTrail trail and log management events. Use CloudWatch Events rules with AWS Lambda to automatically remediate S3 bucket policy changes. Configure alerting with Amazon SNS.</p>",
          "<p>Identify the IP addresses in Amazon S3 requests with Amazon S3 access logs and Amazon Athena. Use AWS Config with Auto Remediation to remediate any changes to S3 bucket policies. Configure alerting with AWS Config and Amazon SNS.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Analytics",
      "question_plain": "A company stores highly confidential information in an Amazon S3 bucket. The security team have evaluated the security of the configuration and have come up with some new requirements that must be met. The security team now requires the ability to identify the IP addresses that make requests to the bucket to be able to identify malicious actors. They additionally require that any changes to the bucket policy are automatically remediated and alerts of these changes are sent to their team members.Which strategies should a Solutions Architect use to meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080553,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company needs to deploy an application into an AWS Region across multiple Availability Zones and has several requirements for the deployment. The application requires access to 100 GB of static data before the application starts and must be able to scale up and down quickly. Startup time must be minimized as much as possible. The Operations team must be able to install critical OS patches within 48 hours of release. The solution should also be cost-effective.</p><p>Which deployment strategy meets these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>An Auto Scaling group should be used across multiple AZs. The AMI included should include the latest OS patches and the launch config / template should be replaced with a new AMI that includes updated OS patches whenever the critical patches are released. An Amazon EFS file system can be mounted to the instances that has the 100 GB of static data.</p><p><strong>CORRECT: </strong>\"Use Amazon EC2 Auto Scaling with an AMI that includes the latest OS patches. Mount an Amazon EFS file system with the static data to the EC2 instances at launch time\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 Auto Scaling with an AMI that includes the latest OS patches. Mount a shared Amazon EBS volume with the static data to the EC2 instances at launch time\" is incorrect. You can mount a shared EBS volume but with certain constraints including that the instances must be in the same AZ.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 Auto Scaling with a standard AMI. Use a user data script to download the static data from an Amazon S3 bucket. Update the OS patches with AWS Systems Manager\" is incorrect. Downloading the data from S3 will take time and also will mean each instance has a copy in its EBS volume increasing data storage costs.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 Auto Scaling with an AMI that includes the static data. Update the OS patches with AWS Systems Manager\" is incorrect. This will increase data storage costs as the data will be included in the EBS volumes for every instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-walk-patch-windows-ami-autoscaling.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-walk-patch-windows-ami-autoscaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-storage-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-storage-sap/</a></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-compute-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-compute-sap/</a></p>",
        "answers": [
          "<p>Use Amazon EC2 Auto Scaling with an AMI that includes the latest OS patches. Mount an Amazon EFS file system with the static data to the EC2 instances at launch time.</p>",
          "<p>Use Amazon EC2 Auto Scaling with an AMI that includes the latest OS patches. Mount a shared Amazon EBS volume with the static data to the EC2 instances at launch time.</p>",
          "<p>Use Amazon EC2 Auto Scaling with a standard AMI. Use a user data script to download the static data from an Amazon S3 bucket. Update the OS patches with AWS Systems Manager.</p>",
          "<p>Use Amazon EC2 Auto Scaling with an AMI that includes the static data. Update the OS patches with AWS Systems Manager.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Compute",
      "question_plain": "A company needs to deploy an application into an AWS Region across multiple Availability Zones and has several requirements for the deployment. The application requires access to 100 GB of static data before the application starts and must be able to scale up and down quickly. Startup time must be minimized as much as possible. The Operations team must be able to install critical OS patches within 48 hours of release. The solution should also be cost-effective.Which deployment strategy meets these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080555,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company has several Amazon RDS databases each with over 50 TB of data. Management have requested that ability to generate a weekly business report from the databases. The system should support ad-hoc SQL queries.</p><p>What is the MOST cost-effective solution for the Business Intelligence platform?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The lowest cost option is to extract the data to Amazon S3 using a Glue ETL job. Tables can be created in the Glue Data Catalog and Amazon Athena can be used to run the queries. This is a fully serverless solution that meets all requirements.</p><p><strong>CORRECT: </strong>\"Configure an AWS Glue crawler to crawl the databases and create tables in the AWS Glue Data Catalog. Create an AWS Glue ETL job that loads data from the RDS databases to Amazon S3. Use Amazon Athena to run the queries\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Redshift cluster. Create an AWS Glue ETL job to copy data from the RDS databases to the Amazon Redshift cluster. Use Amazon Redshift to run the query\" is incorrect. This solution requires instances to be running in the RedShift cluster so it will be more costly.</p><p><strong>INCORRECT:</strong> \"Create an AWS Glue ETL job that copies data from the RDS databases to a single Amazon Aurora MySQL database. Run SQL queries on the Aurora MySQL database\" is incorrect. This solution requires instances to be running in the Aurora cluster so it will be more costly.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EMR cluster. Create an AWS Glue ETL job to copy data from the RDS databases to the Amazon Redshift cluster. Use Amazon QuickSight to run the query\" is incorrect. This solution requires instances to be running in the EMR cluster so it will be more costly.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/glue-best-practices.html\">https://docs.aws.amazon.com/athena/latest/ug/glue-best-practices.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-analytics-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-analytics-sap/</a></p>",
        "answers": [
          "<p>Create an Amazon Redshift cluster. Create an AWS Glue ETL job to copy data from the RDS databases to the Amazon Redshift cluster. Use Amazon Redshift to run the query.</p>",
          "<p>Configure an AWS Glue crawler to crawl the databases and create tables in the AWS Glue Data Catalog. Create an AWS Glue ETL job that loads data from the RDS databases to Amazon S3. Use Amazon Athena to run the queries.</p>",
          "<p>Create an AWS Glue ETL job that copies data from the RDS databases to a single Amazon Aurora MySQL database. Run SQL queries on the Aurora MySQL database.</p>",
          "<p>Create an Amazon EMR cluster. Create an AWS Glue ETL job to copy data from the RDS databases to the Amazon Redshift cluster. Use Amazon QuickSight to run the query.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Analytics",
      "question_plain": "A company has several Amazon RDS databases each with over 50 TB of data. Management have requested that ability to generate a weekly business report from the databases. The system should support ad-hoc SQL queries.What is the MOST cost-effective solution for the Business Intelligence platform?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080557,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application runs on a fleet of Amazon ECS instances and stores data in an Amazon S3 bucket. Until recently the application had been working well and then started to fail to upload objects to the S3 bucket. Server access logging has been enabled and 403 errors have been identified since the time of the fault. The ECS cluster has been setup according to best practices and no changes have been made to the S3 bucket policy or IAM roles used to access the bucket.</p><p>What is the most LIKELY cause of the failure?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>A 403 error is an access denied error. Therefore, there must be an issue relating to authorization to access the S3 bucket. In this scenario the bucket policy and IAM roles have not been modified. The next thing to check is that the IAM role is correctly assigned to the ECS tasks.</p><p>The most likely cause of this issue is that the ECS task role has been changed and the tasks no longer have the permissions to access S3.</p><p><strong>CORRECT: </strong>\"The ECS task IAM role was modified\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The ECS container instance IAM role was modified\" is incorrect. According to best practice the permissions that containers (tasks) require should be specified in task IAM roles not in container instance IAM roles. As the cluster has been setup according to best practice the permissions to S3 should not be specified in the container instance IAM role.</p><p><strong>INCORRECT:</strong> \"The ECS service in inaccessible\" is incorrect. This would not result in an access denied error.</p><p><strong>INCORRECT:</strong> \"The ECS tasks have insufficient memory assigned\" is incorrect. This would not result in an access denied error.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-compute-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-compute-sap/</a></p>",
        "answers": [
          "<p>The ECS service in inaccessible.</p>",
          "<p>The ECS container instance IAM role was modified.</p>",
          "<p>The ECS tasks have insufficient memory assigned.</p>",
          "<p>The ECS task IAM role was modified.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Compute",
      "question_plain": "An application runs on a fleet of Amazon ECS instances and stores data in an Amazon S3 bucket. Until recently the application had been working well and then started to fail to upload objects to the S3 bucket. Server access logging has been enabled and 403 errors have been identified since the time of the fault. The ECS cluster has been setup according to best practices and no changes have been made to the S3 bucket policy or IAM roles used to access the bucket.What is the most LIKELY cause of the failure?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080559,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application publishes data continuously to Amazon DynamoDB using an AWS Lambda function. The DynamoDB table has an auto scaling policy enabled with the target utilization set to 70%. There are short predictable periods in which a large volume of data is received and this can exceed the typical load by up to 300%. The AWS Lambda function writes<br>ProvisionedThroughputExceededException messages to Amazon CloudWatch Logs during these times, and some records are redirected to the dead letter queue.<br>What change should the company make to resolve this issue?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The error message received will include the following statement:</p><p><em>You exceeded your maximum allowed provisioned throughput for a table or for one or more global secondary indexes.</em></p><p>This indicates that the table’s capacity is insufficient for the load. The best way to resolve this for a predictable spike in traffic is to configure Application Auto Scaling to scale based on a defined schedule.</p><p>Scaling based on a schedule allows you to set your own scaling schedule for predictable load changes. For example, every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can configure Application Auto Scaling to increase capacity on Wednesday and decrease capacity on Friday.</p><p><strong>CORRECT: </strong>\"Use Application Auto Scaling to scale out write capacity on the DynamoDB table based on a schedule\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Application Auto Scaling to set a step scaling policy to scale out write capacity on the DynamoDB table when load spikes reach a defined threshold\" is incorrect. Step scaling is a dynamic scaling type that will scale when the load increases and scales according to the size of the alarm breach. However, this method must respond to alarm breaches and therefore may still result in errors. Scheduled scaling is preferable as it ensures the additional capacity is enabled before the increase in load occurs.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events to monitor the dead letter queue and invoke a Lambda function to automatically retry failed records\" is incorrect. This does not solve the original problem and the records may still fail to be written if the load has not decreased.</p><p><strong>INCORRECT:</strong> \"Reduce the DynamoDB table auto scaling policy's target utilization to 50% to provide more resources for peak traffic periods\" is incorrect. This will still result in an alarm breach if the utilization goes above 50% and is less cost-effective.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-database-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-database-sap/</a></p>",
        "answers": [
          "<p>Use Amazon CloudWatch Events to monitor the dead letter queue and invoke a Lambda function to automatically retry failed records.</p>",
          "<p>Reduce the DynamoDB table auto scaling policy's target utilization to 50% to provide more resources for peak traffic periods.</p>",
          "<p>Use Application Auto Scaling to set a step scaling policy to scale out write capacity on the DynamoDB table when load spikes reach a defined threshold.</p>",
          "<p>Use Application Auto Scaling to scale out write capacity on the DynamoDB table based on a schedule.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Database",
      "question_plain": "An application publishes data continuously to Amazon DynamoDB using an AWS Lambda function. The DynamoDB table has an auto scaling policy enabled with the target utilization set to 70%. There are short predictable periods in which a large volume of data is received and this can exceed the typical load by up to 300%. The AWS Lambda function writesProvisionedThroughputExceededException messages to Amazon CloudWatch Logs during these times, and some records are redirected to the dead letter queue.What change should the company make to resolve this issue?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080561,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A secure web application runs in an Amazon VPC that has a public subnet and a private subnet. An Application Load Balancer is deployed into the public subnet. Each subnet has a separate Network ACL. The public subnet CIDR range is 10.1.0.0/24 and the private subnet CIDR range is 10.1.1.0/24. The web application is deployed on Amazon EC2 instances in the private subnet. Which combination of rules should be defined on the private subnet’s Network ACL to allow access from internet-based clients?</p><p>(Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Network ACLs are stateless firewalls. This means you must define rules for the inbound and outbound connections. In this case the source address of packets coming towards the instances will be the IP addresses of the ALB nodes in 10.1.0.0/24. An inbound rule allowing 443 from this source network should be created.</p><p>The outbound connection will be made to the source port of the ALB node which is a dynamically defined high number port between 1024 and 65535. Therefore, an outbound rule must be created that allows this port range to destination 10.1.0.0/24.</p><p><strong>CORRECT: </strong>\"An inbound rule for port 443 from source 10.1.0.0/24.\" is a correct answer.</p><p><strong>CORRECT: </strong>\"An outbound rule for ports 1024 through 65535 to destination 10.1.0.0/24.\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"An inbound rule for port 443 from source 0.0.0.0/0.\" is incorrect. This would allow any source IP range but should be locked down to the ALB nodes in 10.1.0.0/24</p><p><strong>INCORRECT:</strong> \"An outbound rule for port 443 to destination 0.0.0.0/0.\" is incorrect. The outbound connection will not have a destination port of 443, the destination port will be a high numbered port between 1024 and 65535.</p><p><strong>INCORRECT:</strong> \"An outbound rule for port 443 to destination 10.1.0.0/24.\" is incorrect. The outbound connection will not have a destination port of 443, the destination port will be a high numbered port between 1024 and 65535.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-networking-content-delivery-sap/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-professional/aws-networking-content-delivery-sap/</a></p>",
        "answers": [
          "<p>An inbound rule for port 443 from source 0.0.0.0/0.</p>",
          "<p>An inbound rule for port 443 from source 10.1.0.0/24.</p>",
          "<p>An outbound rule for port 443 to destination 0.0.0.0/0.</p>",
          "<p>An outbound rule for port 443 to destination 10.1.0.0/24.</p>",
          "<p>An outbound rule for ports 1024 through 65535 to destination 10.1.0.0/24.</p>"
        ]
      },
      "correct_response": ["b", "e"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A secure web application runs in an Amazon VPC that has a public subnet and a private subnet. An Application Load Balancer is deployed into the public subnet. Each subnet has a separate Network ACL. The public subnet CIDR range is 10.1.0.0/24 and the private subnet CIDR range is 10.1.1.0/24. The web application is deployed on Amazon EC2 instances in the private subnet. Which combination of rules should be defined on the private subnet’s Network ACL to allow access from internet-based clients?(Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080563,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A start-up company has created a new serverless application which includes an AWS Lambda function which sits behind an Amazon API gateway and an Amazon CloudFront CDN. The development team is currently using AWS CLI scripts to update the versions of Lambda functions. In case an error is detected, a different CLI script is used to roll back the version to the previous stable one.</p><p>A solutions architect needs to optimize this process and reduce the time taken to switch versions and detect the error in Lambda functions.</p><p>How can this be accomplished?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p><strong>Explanation:</strong></p><p>AWS CodeDeploy leverages Lambda’s traffic shifting capabilities to automate the gradual rollout of new function versions. It can also help you use deployment best practices, such as testing a new change on a small portion of traffic before deploying it to all customers. For example, CodeDeploy lets you automate pre-deployment tests that a function must pass before it begins taking traffic. You can also set alarms that automatically trigger rollbacks in the event of errors.</p><p><strong>CORRECT: </strong>\"Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback in case Amazon CloudWatch alarms is triggered\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add the AWS CloudFront distribution and API Gateway in the parent stack and AWS Lambda in a child stack. Within these nested stacks, when changes are to be introduced to AWS Lambda create an AWS CloudFormation change set and deploy. Revert the AWS CloudFormation change set to the previous version in case of failure\" is incorrect.</p><p>Change sets don’t indicate whether AWS CloudFormation will successfully update a stack. It needs manual intervention to check the error and provide a resolution hence this is not a correct option.</p><p><strong>INCORRECT:</strong> \"Merge the AWS CLI scripts into a single script that deploys the new Lambda version. Execute these scripts when deployment is completed. If errors are detected, revert to the previous Lambda version\" is incorrect.</p><p>Even though CLI provides finer grain control to the AWS resources, this is more suitable for a development environment since debugging is tougher and needs manual effort. Therefore, the CLI does not reduce the amount of effort and is not a preferable option.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudFormation stack with API Gateway endpoint resource that references the new Lambda version. Change the CloudFront origin to the new API Gateway endpoint in case of success, else rollback to the previous version\" is incorrect.</p><p>While this is feasible, to detect the error, the CloudFormation output needs to be checked, lambda logs need to be checked and then CloudFront origin needs to be changed. Since the question is asking about reducing the time, this is also not a correct option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
        "answers": [
          "<p>Add the AWS CloudFront distribution and API Gateway in the parent stack and AWS Lambda in a child stack. Within these nested stacks, when changes are to be introduced to AWS Lambda create an AWS CloudFormation change set and deploy. Revert the AWS CloudFormation change set to the previous version in case of failure.</p>",
          "<p>Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback in case Amazon CloudWatch alarms is triggered.</p>",
          "<p>Merge the AWS CLI scripts into a single script that deploys the new Lambda version. Execute these scripts when deployment is completed. If errors are detected, revert to the previous Lambda version.</p>",
          "<p>Create an AWS CloudFormation stack with API Gateway endpoint resource that references the new Lambda version. Change the CloudFront origin to the new API Gateway endpoint in case of success, else rollback to the previous version.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Developer Tools",
      "question_plain": "A start-up company has created a new serverless application which includes an AWS Lambda function which sits behind an Amazon API gateway and an Amazon CloudFront CDN. The development team is currently using AWS CLI scripts to update the versions of Lambda functions. In case an error is detected, a different CLI script is used to roll back the version to the previous stable one.A solutions architect needs to optimize this process and reduce the time taken to switch versions and detect the error in Lambda functions.How can this be accomplished?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080565,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A media company runs an application that uses a static website configured in an Amazon S3 bucket and an Amazon CloudFront distribution. The website calls an Amazon API Gateway REST API, and an AWS Lambda function backs each API method.</p><p>The company wants to generate a CSV report every 2 weeks that records the following for each Lambda function:</p><p>· Recommended configured memory.</p><p>· Recommended cost.</p><p>· Price difference between current configurations and the recommendations.</p><p>Which solution will meet these requirements with the LEAST development time?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p><strong>Explanation:</strong></p><p>You can export your recommendations to record them over time and share the data with others. Recommendations are exported in a comma-separated values (.csv) file, and its metadata in a JavaScript Object Notation (.json) file, to an existing Amazon Simple Storage Service (Amazon S3) bucket that you specify.</p><p>Recommendations are exported in a comma-separated values (.csv) file, and its metadata in a JavaScript Object Notation (JSON) (.json) file, to an existing Amazon Simple Storage Service (Amazon S3) bucket that you specify. For more information, see <a href=\"https://docs.aws.amazon.com/compute-optimizer/latest/ug/exporting-recommendations.html\">Exporting Recommendations</a> in the Compute Optimizer User Guide.</p><p><strong>CORRECT: </strong>\"Use AWS Compute Optimizer. Call the “ExportLambdaFunctionRecommendations” operation for the Lambda functions. Export the .csv file to an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an additional Lambda function that extracts metrics data from Amazon CloudWatch Logs for each application Lambda function. Collate the data into tabular format. Store the data as a .csv file in an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks\" is incorrect.</p><p>This option involves a lot of manual steps and additional effort for building the Lambda function, this would also add the cost of function invocation and so is not a suitable option.</p><p><strong>INCORRECT:</strong> \"Use AWS Compute Optimizer. Set up enhanced infrastructure metrics. Within the Compute Optimizer console, schedule a job to export the Lambda recommendations to a .csv file. Store the file in an S3 bucket every 2 weeks\" is incorrect.</p><p>The question ask specifically about the Lambda function's recommended configured memory, recommended cost, and the price difference between current configurations and the recommendations. Therefore, this is not the most suitable solution.</p><p><strong>INCORRECT:</strong> \"Purchase the AWS Business Support plan for the production account. Opt into AWS Compute Optimizer for AWS Trusted Advisor checks. In the Trusted Advisor console, schedule a job to export the cost optimization checks to a .csv file. Store the file in an S3 bucket every 2 weeks\" is incorrect.</p><p>AWS Compute Optimizer provides many inbuilt checks, and it is not necessary to have an AWS Business support plan. Also implementing the steps mentioned in this option would involve lot of steps initially and trusted advisor wouldn’t be able to deliver exact data in the format required.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/compute-optimizer/latest/APIReference/API_ExportLambdaFunctionRecommendations.html\">https://docs.aws.amazon.com/compute-optimizer/latest/APIReference/API_ExportLambdaFunctionRecommendations.html</a></p><p><a href=\"https://docs.aws.amazon.com/compute-optimizer/latest/ug/exporting-recommendations.html\">https://docs.aws.amazon.com/compute-optimizer/latest/ug/exporting-recommendations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
        "answers": [
          "<p>Create an additional Lambda function that extracts metrics data from Amazon CloudWatch Logs for each application Lambda function. Collate the data into tabular format. Store the data as a .csv file in an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.</p>",
          "<p>Use AWS Compute Optimizer. Call the “ExportLambdaFunctionRecommendations” operation for the Lambda functions. Export the .csv file to an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.</p>",
          "<p>Use AWS Compute Optimizer. Set up enhanced infrastructure metrics. Within the Compute Optimizer console, schedule a job to export the Lambda recommendations to a .csv file. Store the file in an S3 bucket every 2 weeks.</p>",
          "<p>Purchase the AWS Business Support plan for the production account. Opt into AWS Compute Optimizer for AWS Trusted Advisor checks. In the Trusted Advisor console, schedule a job to export the cost optimization checks to a .csv file. Store the file in an S3 bucket every 2 weeks.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "A media company runs an application that uses a static website configured in an Amazon S3 bucket and an Amazon CloudFront distribution. The website calls an Amazon API Gateway REST API, and an AWS Lambda function backs each API method.The company wants to generate a CSV report every 2 weeks that records the following for each Lambda function:· Recommended configured memory.· Recommended cost.· Price difference between current configurations and the recommendations.Which solution will meet these requirements with the LEAST development time?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080567,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A financial company stores personally identifiable information (PII) in an Amazon S3 bucket which currently does not have versioning enabled. The current configuration has server-side encryption with S3 managed encryption keys (SSE-S3) enabled to encrypt the objects. According to a new requirement, all current and future objects in the S3 bucket must be encrypted by keys that the company's security team manages.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>You can configure your bucket to use an S3 Bucket Key for SSE-KMS on new objects by using the Amazon S3 console, REST API, Amazon SDK, Amazon CLI, or Amazon CloudFormation. If you want to enable or disable an S3 Bucket Key for existing objects, you can use a COPY operation.</p><p>Switching to AWS KMS from AWS SSE-S3 can provide some additional benefits, including protection from policies that may be overly permissive. For example, adding a bucket policy allowing overly broad access to the data instead of individual users or roles. By implementing encryption using KMS keys, the accessor of the resources would need Amazon S3 policy access and access to a KMS key to decrypt data.</p><p>The KMS key is managed by the company. To encrypt existing objects with this key they would need to be uploaded again and this operation can be performed using the AWS CLI.</p><p><strong>CORRECT: </strong>\"Change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS) in S3 bucket. Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"In the S3 bucket properties, change the default encryption to SSE-S3 with a customer managed key. Use the AWS CLI to re-upload all objects in the S3 bucket. Set an S3 bucket policy to deny unencrypted PutObject requests\" is incorrect.</p><p>You cannot manage your own encryption key with SSE-S3.</p><p><strong>INCORRECT:</strong> \"Change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS) in S3 bucket. Set an S3 bucket policy to automatically encrypt objects on GetObject and PutObject requests\" is incorrect.</p><p>You cannot encrypt using GetObject requests as this is downloading objects and they are first decrypted. The bucket policy should be setup to enforce encryption for PutObject requests only.</p><p><strong>INCORRECT:</strong> \"Change the default encryption to AES-256 with a customer managed key. Attach a policy to deny unencrypted PutObject requests to any entities that access the S3 bucket. Use the AWS CLI to re-upload all objects in the S3 bucket\" is incorrect.</p><p>Server-Side Encryption in S3 is always AES256, whether you are using SSE-S3 or SSE-KMS.</p><p>When changing the encryption type, we choose among AWS/ S3 managed or customer managed but the algorithm remains the same hence this is also incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/replication-config-for-kms-objects.html\">https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/replication-config-for-kms-objects.html</a></p><p><a href=\"https://aws.amazon.com/blogs/storage/changing-your-amazon-s3-encryption-from-s3-managed-encryption-sse-s3-to-aws-key-management-service-sse-kms/\">https://aws.amazon.com/blogs/storage/changing-your-amazon-s3-encryption-from-s3-managed-encryption-sse-s3-to-aws-key-management-service-sse-kms/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
        "answers": [
          "<p>Change the default encryption to SSE-S3 with a customer managed key. Use the AWS CLI to re-upload all objects in the S3 bucket. Set an S3 bucket policy to deny unencrypted PutObject requests.</p>",
          "<p>Change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS) in S3 bucket. Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.</p>",
          "<p>In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to automatically encrypt objects on GetObject and PutObject requests.</p>",
          "<p>Change the default encryption to AES-256 with a customer managed key. Attach a policy to deny unencrypted PutObject requests to any entities that access the S3 bucket. Use the AWS CLI to re-upload all objects in the S3 bucket.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Storage",
      "question_plain": "A financial company stores personally identifiable information (PII) in an Amazon S3 bucket which currently does not have versioning enabled. The current configuration has server-side encryption with S3 managed encryption keys (SSE-S3) enabled to encrypt the objects. According to a new requirement, all current and future objects in the S3 bucket must be encrypted by keys that the company's security team manages.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080569,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A start-up company has been using bastion hosts to connect to EC2 instances which are based on the latest Amazon Linux 2 AMI. They use these bastion hosts to SSH into EC2 instances to view logs and other troubleshooting activities.</p><p>So far, they have configured a VPC with private and public subnets, and a NAT gateway. Also, they have a Site-to-Site VPN for connectivity with the on-premises environment and EC2 security groups with direct SSH access from the on-premises environment</p><p>To increase security control and comply with auditing requirements around access to instances, which strategy should a solutions architect use?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Session Manager is a fully managed AWS Systems Manager capability. With Session Manager, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, and on-premises servers and virtual machines (VMs).</p><p>You can use either an interactive one-click browser-based shell or the AWS Command Line Interface (AWS CLI). Session Manager provides secure and auditable node management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.</p><p>To use Session Manager you must have the client installed, and this is installed by default on instances launched form the Amazon Linux 2 AMI, and you must have permissions to communicate with AWS Systems Manager. The permissions should be assigned using an IAM role attached using an instance profile.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-12-19_21-16-40-b5873d2505b7cf1281f345458d46f382.jpg\"><p><strong>CORRECT: </strong>\"Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances. Remove all security group rules attached to the EC2 instances that allow inbound TCP on port 22. Have the engineers install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install and configure EC2 Instance Connect on the fleet of EC2 instances. Remove all security group rules attached to EC2 instances that allow inbound TCP on port 22. Advise the engineers to remotely access the instances by using the EC2 Instance Connect CLI\" is incorrect.</p><p>With this option, you’d need to install Instance connect on each EC2 instance and SSH client with client &amp; key on each of the machines intending to connect with instance.</p><p>If number of terminals increase, this would mean maintaining a lot of keys which could become a security risk, hence this is not an appropriate option in above scenario.</p><p><strong>INCORRECT:</strong> \"Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer's devices. Install the Amazon CloudWatch agent on all EC2 instances and send operating system audit logs to CloudWatch Logs\" is incorrect.</p><p>Security groups should be generic in nature and not dependent on specific hosts. If engineers change within the team, security groups would require a change every time. Hence this is not a clean solution. Since audit logs requirement is not specified, this is not very relevant in this context.</p><p><strong>INCORRECT:</strong> \"Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer's devices. Enable AWS Config for EC2 security group resource changes. Enable AWS Firewall Manager and apply a security group policy that automatically remediates changes to rules\" is incorrect.</p><p>Like above option, this would create dependency on engineer’s device and hence pose a security risk. Firewall rules are same to all instances in this case and centralized firewall management is not the intended solution here.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html</a></p><p><a href=\"https://github.com/aws/session-manager-plugin\">https://github.com/aws/session-manager-plugin</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>",
        "answers": [
          "<p>Install and configure EC2 Instance Connect on the fleet of EC2 instances. Remove all security group rules attached to EC2 instances that allow inbound TCP on port 22. Advise the engineers to remotely access the instances by using the EC2 Instance Connect CLI.</p>",
          "<p>Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer's devices. Install the Amazon CloudWatch agent on all EC2 instances and send operating system audit logs to CloudWatch Logs.</p>",
          "<p>Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer's devices. Enable AWS Config for EC2 security group resource changes. Enable AWS Firewall Manager and apply a security group policy that automatically remediates changes to rules.</p>",
          "<p>Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances. Remove all security group rules attached to the EC2 instances that allow inbound TCP on port 22. Have the engineers install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Management & Governance",
      "question_plain": "A start-up company has been using bastion hosts to connect to EC2 instances which are based on the latest Amazon Linux 2 AMI. They use these bastion hosts to SSH into EC2 instances to view logs and other troubleshooting activities.So far, they have configured a VPC with private and public subnets, and a NAT gateway. Also, they have a Site-to-Site VPN for connectivity with the on-premises environment and EC2 security groups with direct SSH access from the on-premises environmentTo increase security control and comply with auditing requirements around access to instances, which strategy should a solutions architect use?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080571,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An online shopping portal is running in eu-west-1 region. One of the application components uses AWS Lambda functions and stores inventory data in an Amazon Aurora database. Deployment of the Lambda functions is performed using a deployment package. The company has configured automated backups for Aurora.</p><p>The company wants to move the application to another AWS account within the same AWS organization. The application processes critical data and downtime must be minimized or avoided if possible.</p><p>Which solution will meet the requirements for moving this application from the source account to the target account?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>By using AWS Resource Access Manager (AWS RAM) with Amazon Aurora, you can share Aurora DB clusters and clones that belong to your AWS account with another AWS account or organization. Such <em>cross-account cloning</em> is much faster than creating and restoring a database snapshot. You can create a clone of one of your Aurora DB clusters and share the clone. Or you can share your Aurora DB cluster with another AWS account and let the account holder create the clone. The approach that you choose depends on your use case.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-12-19_21-18-28-af4f2eb3f0d1ae54c4c9d4bdeb2ff083.jpg\"><p><strong>CORRECT: </strong>\"Download the Lambda function package from the source account. Use the deployment package and create new Lambda functions in the target account. Share the Aurora DB cluster with the target account by using AWS Resource Access Manager (AWS RAM). Grant the Target account permission to clone the Aurora DB cluster\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Download the Lambda function package from the source account and use it to replicate the Lambda functions into the target account. Share an automated Aurora DB cluster snapshot with the target account\" is incorrect.</p><p>This is a partially correct option since after sharing the account, permissions also need to be transferred to the new account to be able to provide access.</p><p><strong>INCORRECT:</strong> \"Share the Lambda functions and the Aurora DB cluster with the target account using AWS Resource Access Manager (AWS RAM). Grant the target account permission to clone the Aurora DB cluster\" is incorrect.</p><p>Even though Aurora can be shared, Lambda is not a sharable resource with AWS RAM, hence this option is incorrect.</p><p><strong>INCORRECT:</strong> \"Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with the Target account. Share the automated Aurora DB cluster snapshot with the target account\" is incorrect.</p><p>This option is reverse of the correct solution. Lambda is not a sharable resource in RAM and Aurora is a perfect candidate for RAM in a cross-account share, hence this option is incorrect as well.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html</a></p><p><a href=\"https://docs.aws.amazon.com/ram/latest/userguide/shareable.html#shareable-aur\">https://docs.aws.amazon.com/ram/latest/userguide/shareable.html#shareable-aur</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
        "answers": [
          "<p>Download the Lambda function package from the source account and use it to replicate the Lambda functions into the target account. Share an automated Aurora DB cluster snapshot with the target account.</p>",
          "<p>Download the Lambda function package from the source account. Use the deployment package and create new Lambda functions in the target account. Share the Aurora DB cluster with the target account by using AWS Resource Access Manager (AWS RAM). Grant the Target account permission to clone the Aurora DB cluster.</p>",
          "<p>Share the Lambda functions and the Aurora DB cluster with the target account using AWS Resource Access Manager (AWS RAM). Grant the target account permission to clone the Aurora DB cluster.</p>",
          "<p>Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with the Target account. Share the automated Aurora DB cluster snapshot with the target account.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Migration & Transfer",
      "question_plain": "An online shopping portal is running in eu-west-1 region. One of the application components uses AWS Lambda functions and stores inventory data in an Amazon Aurora database. Deployment of the Lambda functions is performed using a deployment package. The company has configured automated backups for Aurora.The company wants to move the application to another AWS account within the same AWS organization. The application processes critical data and downtime must be minimized or avoided if possible.Which solution will meet the requirements for moving this application from the source account to the target account?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080573,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A corporation is seeking to develop a disaster recovery (DR) plan for its web application, which is currently operational in a single AWS Region. This application utilizes a microservices architecture with services running on AWS Fargate within Amazon Elastic Container Service (ECS). The data layer is handled by an Amazon RDS for MySQL database, and DNS management is conducted through Amazon Route 53. An Amazon CloudWatch alarm is configured to trigger an Amazon EventBridge rule in the event of an application failure.</p><p>The task is to design a DR strategy that enables quick restoration of the application in a different AWS Region following a failure.</p><p>Which approach would best meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>This solution is efficient for minimizing recovery time in a disaster scenario. By having a standby ECS cluster and service in a different Region and a cross-Region RDS read replica, the application can quickly switch to the standby environment.</p><p>The AWS Lambda function plays a crucial role in automating the promotion of the read replica to a primary database and updating Route 53 to ensure traffic is directed to the standby cluster. This approach ensures a rapid transition with minimal downtime.</p><p><strong>CORRECT: </strong>\"Set up a standby ECS cluster and service on Fargate in a different Region. Create a cross-Region RDS read replica in this new Region. Design an AWS Lambda function to promote the read replica to a primary database and reconfigure Route 53 to reroute traffic to the standby ECS cluster. Adjust the EventBridge rule to include this Lambda function as a target\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a new ECS cluster and service on Fargate in an alternate Region. Create an AWS Lambda function to snapshot the RDS database, replicate the snapshot to the new Region, start a new RDS instance from this snapshot, and reconfigure Route 53 to point to the new ECS cluster. Update the EventBridge rule to trigger this Lambda function\" is incorrect.</p><p>While creating a new ECS cluster and using a Lambda function to handle snapshots and Route 53 updates is a viable strategy, it may not be as efficient as using a read replica, especially in terms of data synchronization and recovery time.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function to automate the deployment of a backup ECS cluster and service in a second Region. This function should: initiate a backup of the RDS database, transfer this backup to the alternate Region, establish a new RDS instance from the backup, and update Route 53 to redirect traffic to the backup ECS cluster. Modify the EventBridge rule to activate this Lambda function\" is incorrect.</p><p>Automating the deployment of a backup ECS cluster through a Lambda function is a good approach, but the process of initiating a backup and establishing a new RDS instance from it could be more time-consuming compared to using a read replica.</p><p><strong>INCORRECT:</strong> \"Create a redundant ECS cluster and service on Fargate in another Region. Regularly back up the RDS database and store these backups in the secondary Region. Create an AWS Lambda function to initiate a new RDS instance from the latest backup and update Route 53 to direct traffic to the redundant ECS cluster when needed. Revise the EventBridge rule to call this Lambda function\" is incorrect.</p><p>Establishing a redundant ECS cluster and storing regular RDS backups in another Region is a valid strategy, but it might lead to longer recovery times due to the need to initiate a new RDS instance from backups during a disaster event.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/disaster-recovery/\">https://aws.amazon.com/disaster-recovery/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
        "answers": [
          "<p>Create a new ECS cluster and service on Fargate in an alternate Region. Create an AWS Lambda function to snapshot the RDS database, replicate the snapshot to the new Region, start a new RDS instance from this snapshot, and reconfigure Route 53 to point to the new ECS cluster. Update the EventBridge rule to trigger this Lambda function.</p>",
          "<p>Create an AWS Lambda function to automate the deployment of a backup ECS cluster and service in a second Region. This function should: initiate a backup of the RDS database, transfer this backup to the alternate Region, establish a new RDS instance from the backup, and update Route 53 to redirect traffic to the backup ECS cluster. Modify the EventBridge rule to activate this Lambda function.</p>",
          "<p>Set up a standby ECS cluster and service on Fargate in a different Region. Create a cross-Region RDS read replica in this new Region. Design an AWS Lambda function to promote the read replica to a primary database and reconfigure Route 53 to reroute traffic to the standby ECS cluster. Adjust the EventBridge rule to include this Lambda function as a target.</p>",
          "<p>Create a redundant ECS cluster and service on Fargate in another Region. Regularly back up the RDS database and store these backups in the secondary Region. Create an AWS Lambda function to initiate a new RDS instance from the latest backup and update Route 53 to direct traffic to the redundant ECS cluster when needed. Revise the EventBridge rule to call this Lambda function.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Compute",
      "question_plain": "A corporation is seeking to develop a disaster recovery (DR) plan for its web application, which is currently operational in a single AWS Region. This application utilizes a microservices architecture with services running on AWS Fargate within Amazon Elastic Container Service (ECS). The data layer is handled by an Amazon RDS for MySQL database, and DNS management is conducted through Amazon Route 53. An Amazon CloudWatch alarm is configured to trigger an Amazon EventBridge rule in the event of an application failure.The task is to design a DR strategy that enables quick restoration of the application in a different AWS Region following a failure.Which approach would best meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080575,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A company's serverless application, comprising several AWS Lambda functions and Amazon DynamoDB tables, is undergoing an upgrade to include interaction with an Amazon Neptune DB cluster. This cluster is distributed across two subnets within a VPC.</p><p>Identify two solutions that would enable the Lambda functions to access both the Neptune DB cluster and the DynamoDB tables (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Placing Lambda functions in private subnets within the Neptune VPC and using a NAT gateway for managing internet traffic is a secure method to allow access to the Neptune DB cluster. This configuration ensures controlled internet access and efficient communication between the Lambda functions and the Neptune cluster.</p><p>Hosting Lambda functions in dedicated subnets within the Neptune VPC and creating a VPC endpoint for DynamoDB provides a secure and direct connection to DynamoDB. This setup allows the Lambda functions to efficiently access both the Neptune DB cluster and the DynamoDB tables within a controlled environment.</p><p><strong>CORRECT: </strong>\"Configure two private subnets in the Neptune VPC and route internet traffic via a NAT gateway. Deploy the Lambda functions in these private subnets\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create two new subnets in the Neptune VPC, specifically for hosting the Lambda functions. Implement a VPC endpoint for DynamoDB to facilitate direct access from these subnets\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the Lambda functions outside the VPC and establish a VPC endpoint for the Neptune database, enabling the Lambda functions to connect to Neptune through this endpoint\" is incorrect.</p><p>Hosting Lambda functions outside the VPC and using a VPC endpoint for Neptune can introduce latency and potential security concerns.</p><p><strong>INCORRECT:</strong> \"Create two public subnets in the Neptune VPC, with traffic managed through an internet gateway. Position the Lambda functions in these public subnets\" is incorrect.</p><p>Utilizing public subnets for Lambda functions to access the Neptune DB cluster is generally not recommended due to potential security risks, as it exposes the functions to the public internet.</p><p><strong>INCORRECT:</strong> \"Keep the Lambda functions outside the VPC but modify the security group of the Neptune DB cluster to allow access based on the Lambda functions' dynamic IP ranges\" is incorrect.</p><p>Modifying the Neptune DB cluster's security group to allow access from Lambda functions' dynamic IP ranges is not practical, as Lambda functions do not have static IP addresses. This approach would be complex and potentially insecure.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html</a></p><p><a href=\"https://docs.aws.amazon.com/neptune/latest/userguide/intro.html\">https://docs.aws.amazon.com/neptune/latest/userguide/intro.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
        "answers": [
          "<p>Configure two private subnets in the Neptune VPC and route internet traffic via a NAT gateway. Deploy the Lambda functions in these private subnets.</p>",
          "<p>Deploy the Lambda functions outside the VPC and establish a VPC endpoint for the Neptune database, enabling the Lambda functions to connect to Neptune through this endpoint.</p>",
          "<p>Create two new subnets in the Neptune VPC, specifically for hosting the Lambda functions. Implement a VPC endpoint for DynamoDB to facilitate direct access from these subnets.</p>",
          "<p>Create two public subnets in the Neptune VPC, with traffic managed through an internet gateway. Position the Lambda functions in these public subnets.</p>",
          "<p>Keep the Lambda functions outside the VPC but modify the security group of the Neptune DB cluster to allow access based on the Lambda functions' dynamic IP ranges.</p>"
        ]
      },
      "correct_response": ["a", "c"],
      "section": "AWS Database",
      "question_plain": "A company's serverless application, comprising several AWS Lambda functions and Amazon DynamoDB tables, is undergoing an upgrade to include interaction with an Amazon Neptune DB cluster. This cluster is distributed across two subnets within a VPC.Identify two solutions that would enable the Lambda functions to access both the Neptune DB cluster and the DynamoDB tables (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080577,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A car rental company operates a serverless REST API, which includes an Amazon API Gateway with a Regional endpoint, AWS Lambda functions, and an Amazon Aurora MySQL Serverless DB cluster. This API, initially serving a mobile app, has been extended to partner mobile apps, leading to a substantial increase in requests and occasional database memory errors. Analysis shows that clients frequently make repeated HTTP GET requests for the same queries in short intervals, especially during business hours and around holidays.</p><p>To enhance the system's capacity to handle this increased load without significantly raising costs, what approach should the company adopt?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Implementing Amazon ElastiCache for Redis provides an effective caching mechanism for frequently accessed data. By caching the results of common database queries, the Lambda functions can serve repeated requests from the cache rather than querying the Aurora Serverless DB each time.</p><p>This approach significantly reduces the load on the database, mitigating memory errors caused by high request volumes. Additionally, using a cache can improve response times for the API clients and is a cost-effective way to scale the application to handle increased traffic without needing to proportionally scale the database.</p><p><strong>CORRECT: </strong>\"Integrate an Amazon ElastiCache for Redis layer to cache database query results. Update the Lambda functions to retrieve data from this cache when available\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Convert the API Gateway Regional endpoint to an edge-optimized endpoint and enable response caching at the production stage of the API Gateway\" is incorrect/</p><p>While enabling caching in API Gateway can reduce the number of requests reaching the backend, converting to an edge-optimized endpoint primarily benefits geographically distributed clients and may not address the core issue of database load.</p><p><strong>INCORRECT:</strong> \"Adjust the configuration of the Aurora Serverless DB cluster to augment the maximum memory allocation\" is incorrect.</p><p>Increasing the maximum memory of the Aurora Serverless DB cluster can handle more requests but may not be the most cost-effective solution. It addresses the symptom (memory errors) rather than the cause (repeated queries).</p><p><strong>INCORRECT:</strong> \"Implement rate limiting and burst control in the API Gateway production stage to manage the influx of incoming API calls\" is incorrect.</p><p>Throttling can prevent the system from being overwhelmed by too many requests but doesn't optimize the handling of legitimate, repeated queries. It might also degrade the user experience by limiting access to the API during peak times.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
        "answers": [
          "<p>Convert the API Gateway Regional endpoint to an edge-optimized endpoint and enable response caching at the production stage of the API Gateway.</p>",
          "<p>Integrate an Amazon ElastiCache for Redis layer to cache database query results. Update the Lambda functions to retrieve data from this cache when available.</p>",
          "<p>Adjust the configuration of the Aurora Serverless DB cluster to augment the maximum memory allocation.</p>",
          "<p>Implement rate limiting and burst control in the API Gateway production stage to manage the influx of incoming API calls.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Storage",
      "question_plain": "A car rental company operates a serverless REST API, which includes an Amazon API Gateway with a Regional endpoint, AWS Lambda functions, and an Amazon Aurora MySQL Serverless DB cluster. This API, initially serving a mobile app, has been extended to partner mobile apps, leading to a substantial increase in requests and occasional database memory errors. Analysis shows that clients frequently make repeated HTTP GET requests for the same queries in short intervals, especially during business hours and around holidays.To enhance the system's capacity to handle this increased load without significantly raising costs, what approach should the company adopt?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080579,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A healthcare organization is planning to transition its on-premises data processing workloads to AWS. Before migration, the organization needs a thorough assessment of its current server infrastructure to determine appropriate sizing for Amazon EC2 instances. Key data to be collected includes CPU and memory usage, network I/O, and a list of active services on each server. Additionally, the organization wants to analyze network traffic patterns to understand dependencies between servers.</p><p>What is the most cost-effective method to gather this comprehensive data for migration planning?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Application Discovery Service is specifically designed to assist organizations in collecting detailed information about their on-premises infrastructure, which is crucial for cloud migration planning.</p><p>By deploying the data collection agent on each server, the organization can obtain in-depth data on server performance, including CPU and memory usage, network I/O, and running services.</p><p>This detailed level of data collection, including network traffic analysis, is essential for accurately determining the sizing and configuration of Amazon EC2 instances for the migration.</p><p>This approach ensures a comprehensive assessment with minimal cost implications, as it leverages AWS's native tools designed for migration planning.</p><p><strong>CORRECT: </strong>\"Implement AWS Application Discovery Service with the installation of its data collection agent on each server in the organization's data center to gather detailed server usage and network data\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure Amazon CloudWatch agents on all on-premises servers to collect the required metrics and send the data to Amazon CloudWatch for analysis and storage\" is incorrect.</p><p>While Amazon CloudWatch agents can collect a wide range of metrics, they are primarily designed for monitoring AWS resources. Using CloudWatch agents for on-premises servers would not provide the same level of detailed discovery and analysis for migration planning as AWS Application Discovery Service. Additionally, CloudWatch is more focused on monitoring rather than assessing and planning migrations.</p><p><strong>INCORRECT:</strong> \"Use AWS Application Discovery Service with agentless discovery configured in the organization's virtualized environment to collect the necessary server and network information\" is incorrect.</p><p>The agentless discovery option in AWS Application Discovery Service is generally used for gathering basic server information in virtualized environments. It might not provide as detailed or comprehensive data (especially regarding network dependencies and service inventory) as the agent-based discovery.</p><p><strong>INCORRECT:</strong> \"Activate AWS Application Discovery Service via the AWS Management Console and set up network traffic mirroring to capture and analyze inter-server communication and usage metrics\" is incorrect.</p><p>AWS Application Discovery Service does not natively support network traffic mirroring. This option would not be as effective in collecting the detailed server and application usage data required for thorough migration planning.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html\">https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
        "answers": [
          "<p>Configure Amazon CloudWatch agents on all on-premises servers to collect the required metrics and send the data to Amazon CloudWatch for analysis and storage.</p>",
          "<p>Use AWS Application Discovery Service with agentless discovery configured in the organization's virtualized environment to collect the necessary server and network information.</p>",
          "<p>Activate AWS Application Discovery Service via the AWS Management Console and set up network traffic mirroring to capture and analyze inter-server communication and usage metrics.</p>",
          "<p>Implement AWS Application Discovery Service with the installation of its data collection agent on each server in the organization's data center to gather detailed server usage and network data.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Migration & Transfer",
      "question_plain": "A healthcare organization is planning to transition its on-premises data processing workloads to AWS. Before migration, the organization needs a thorough assessment of its current server infrastructure to determine appropriate sizing for Amazon EC2 instances. Key data to be collected includes CPU and memory usage, network I/O, and a list of active services on each server. Additionally, the organization wants to analyze network traffic patterns to understand dependencies between servers.What is the most cost-effective method to gather this comprehensive data for migration planning?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 85080581,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A financial services company is developing a secure web application on AWS. This application will handle sensitive customer data and needs to be accessible only within the company's corporate network. The application is hosted on Amazon EC2 instances within a VPC. The company wants to ensure that this web application is not accessible from the public internet for enhanced security.</p><p>As AWS solutions architect must ensure that the web application is only accessible from the company's corporate network and not from the public internet. Which action should be taken?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Setting up a VPN connection between the company's corporate network and the AWS VPC, and configuring security groups for the EC2 instances to only allow traffic from this VPN connection, ensures that the application is accessible only from within the company’s network.</p><p>This setup maintains a secure, private connection and meets the requirement of not being accessible from the public internet.</p><p><strong>CORRECT: </strong>\"Create a VPN connection between the company’s corporate network and the VPC. Configure security groups for the EC2 instances to only allow traffic from the VPN connection\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Elastic Load Balancer (ELB) and configure it to only accept traffic from the company's corporate network IP range. Attach the ELB to the EC2 instances hosting the web application\" is incorrect.</p><p>While an ELB can restrict traffic to certain IP ranges, it is typically used for load balancing internet-facing traffic and does not inherently prevent public internet access.</p><p><strong>INCORRECT:</strong> \"Create a NAT Gateway and configure route tables to allow traffic only from the corporate network IP range to the EC2 instances\" is incorrect.</p><p>A NAT Gateway is used to enable instances in a private subnet to connect to the internet or other AWS services. It does not restrict inbound access to the application from the public internet.</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon CloudFront distribution and configure it with an origin access identity (OAI) to restrict access to the EC2 instances\" is incorrect.</p><p>Amazon CloudFront with an OAI is typically used for content delivery and caching; it does not inherently restrict application access to a corporate network. CloudFront distributions can still be accessed from the public internet unless additional configurations are implemented.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/security.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/security.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>Create an Elastic Load Balancer (ELB) and configure it to only accept traffic from the company's corporate network IP range. Attach the ELB to the EC2 instances hosting the web application.</p>",
          "<p>Create a NAT Gateway and configure route tables to allow traffic only from the corporate network IP range to the EC2 instances.</p>",
          "<p>Create a VPN connection between the company’s corporate network and the VPC. Configure security groups for the EC2 instances to only allow traffic from the VPN connection.</p>",
          "<p>Deploy an Amazon CloudFront distribution and configure it with an origin access identity (OAI) to restrict access to the EC2 instances.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A financial services company is developing a secure web application on AWS. This application will handle sensitive customer data and needs to be accessible only within the company's corporate network. The application is hosted on Amazon EC2 instances within a VPC. The company wants to ensure that this web application is not accessible from the public internet for enhanced security.As AWS solutions architect must ensure that the web application is only accessible from the company's corporate network and not from the public internet. Which action should be taken?",
      "related_lectures": []
    }
  ]
}
