{
  "count": 65,
  "next": null,
  "previous": null,
  "results": [
    {
      "_class": "assessment",
      "id": 80480370,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect needs to run a PowerShell script on a fleet of Amazon EC2 instances running Microsoft Windows. The instances have already been launched in an Amazon VPC. What tool can be run from the AWS Management Console that to execute the script on all target EC2 instances?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Run Command is designed to support a wide range of enterprise scenarios including installing software, running ad hoc scripts or Microsoft PowerShell commands, configuring Windows Update settings, and more.</p><p>Run Command can be used to implement configuration changes across Windows instances on a consistent yet ad hoc basis and is accessible from the AWS Management Console, the AWS Command Line Interface (CLI), the AWS Tools for Windows PowerShell, and the AWS SDKs.</p><p><strong>CORRECT: </strong>\"Run Command\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodeDeploy\" is incorrect. AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.</p><p><strong>INCORRECT:</strong> \"AWS Config\" is incorrect. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It is not used for ad-hoc script execution.</p><p><strong>INCORRECT:</strong> \"AWS OpsWorks\" is incorrect. AWS OpsWorks provides instances of managed Puppet and Chef.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-ec2-run-command-remote-instance-management-at-scale/\">https://aws.amazon.com/blogs/aws/new-ec2-run-command-remote-instance-management-at-scale/</a></p>",
        "answers": [
          "<p>AWS OpsWorks  </p>",
          "<p>Run Command  </p>",
          "<p>AWS Config  </p>",
          "<p>AWS CodeDeploy  </p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "A Solutions Architect needs to run a PowerShell script on a fleet of Amazon EC2 instances running Microsoft Windows. The instances have already been launched in an Amazon VPC. What tool can be run from the AWS Management Console that to execute the script on all target EC2 instances?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480372,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A security officer has requested that all data associated with a specific customer is encrypted. The data resides on Elastic Block Store (EBS) volumes. Which of the following statements about using EBS encryption are correct? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Amazon EBS encryption uses AWS KMS keys when creating encrypted volumes and snapshots. Encryption operations occur on the servers that host EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached EBS storage.</p><p>Encryption is supported by all EBS volume types. Amazon EBS encryption is available on all current generation and previous generation instance types.</p><p><strong>CORRECT: </strong>\"Data in transit between an instance and an encrypted volume is also encrypted\" is the correct answer.</p><p><strong>CORRECT: </strong>\"There is no direct way to change the encryption state of a volume\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Not all EBS types support encryption\" is incorrect as all EBS volume types support encryption.</p><p><strong>INCORRECT:</strong> \"All attached EBS volumes must share the same encryption state\" is incorrect. You can have encrypted and non-encrypted EBS volumes on a single instance.</p><p><strong>INCORRECT:</strong> \"Only current generation instance types are supported\" is incorrect. Amazon EBS encryption is available on all current generation and previous generation instance types.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
        "answers": [
          "<p>Not all EBS types support encryption</p>",
          "<p>All attached EBS volumes must share the same encryption state</p>",
          "<p>Only current generation instance types are supported.</p>",
          "<p>Data in transit between an instance and an encrypted volume is also encrypted</p>",
          "<p>There is no direct way to change the encryption state of a volume</p>"
        ]
      },
      "correct_response": ["d", "e"],
      "section": "AWS Storage",
      "question_plain": "A security officer has requested that all data associated with a specific customer is encrypted. The data resides on Elastic Block Store (EBS) volumes. Which of the following statements about using EBS encryption are correct? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480374,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>Several Amazon EC2 Spot instances are being used to process messages from an Amazon SQS queue and store results in an Amazon DynamoDB table. Shortly after picking up a message from the queue AWS terminated the Spot instance. The Spot instance had not finished processing the message. What will happen to the message?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The visibility timeout is the amount of time a message is invisible in the queue after a reader picks up the message. If a job is processed within the visibility timeout the message will be deleted. If a job is not processed within the visibility timeout the message will become visible again (could be delivered twice). The maximum visibility timeout for an Amazon SQS message is 12 hours.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-16-54-ed49b8985fe33f8c30a3f5c24800aca9.png\"></p><p><strong>CORRECT: </strong>\"The message will become available for processing again after the visibility timeout expires\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The message will be lost as it would have been deleted from the queue when processed\" is incorrect. The message will not be lost and will not be immediately picked up by another instance.</p><p><strong>INCORRECT:</strong> \"The message will remain in the queue and be immediately picked up by another instance\" is incorrect. As mentioned above it will be available for processing in the queue again after the timeout expires.</p><p><strong>INCORRECT:</strong> \"The results may be duplicated in DynamoDB as the message will likely be processed multiple times\" is incorrect. As the instance had not finished processing the message it should only be fully processed once. Depending on your application process however it is possible some data was written to DynamoDB.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
        "answers": [
          "<p>The message will be lost as it would have been deleted from the queue when processed  </p>",
          "<p>The message will remain in the queue and be immediately picked up by another instance  </p>",
          "<p>The message will become available for processing again after the visibility timeout expires  </p>",
          "<p>The results may be duplicated in DynamoDB as the message will likely be processed multiple times  </p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Application Integration",
      "question_plain": "Several Amazon EC2 Spot instances are being used to process messages from an Amazon SQS queue and store results in an Amazon DynamoDB table. Shortly after picking up a message from the queue AWS terminated the Spot instance. The Spot instance had not finished processing the message. What will happen to the message?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480376,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company needs to capture detailed information about all HTTP requests that are processed by their Internet facing Application Load Balancer (ALB). The company requires information on the requester, IP address, and request type for analyzing traffic patterns to better understand their customer base.</p><p>Which actions should a Solutions Architect recommend?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>You can enable access logs on the ALB and this will provide the information required including requester, IP, and request type. Access logs are not enabled by default. You can optionally store and retain the log files on S3.</p><p><strong>CORRECT: </strong>\"Enable Access Logs and store the data on S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure metrics in CloudWatch for the ALB\" is incorrect. CloudWatch is used for performance monitoring and CloudTrail is used for auditing API access..</p><p><strong>INCORRECT:</strong> \"Enable EC2 detailed monitoring\" is incorrect. Enabling EC2 detailed monitoring will not capture the information requested.</p><p><strong>INCORRECT:</strong> Use CloudTrail to capture all API calls made to the ALB\"\" is incorrect. CloudTrail captures API activity and would not include the requested information.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
        "answers": [
          "<p>Enable Access Logs and store the data on S3  </p>",
          "<p>Configure metrics in CloudWatch for the ALB  </p>",
          "<p>Use CloudTrail to capture all API calls made to the ALB  </p>",
          "<p>Enable EC2 detailed monitoring  </p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A company needs to capture detailed information about all HTTP requests that are processed by their Internet facing Application Load Balancer (ALB). The company requires information on the requester, IP address, and request type for analyzing traffic patterns to better understand their customer base.Which actions should a Solutions Architect recommend?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480378,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company is transitioning their web presence into the AWS cloud. As part of the migration the company will be running a web application both on-premises and in AWS for a period of time. During the period of co-existence the client would like 80% of the traffic to hit the AWS-based web servers and 20% to be directed to the on-premises web servers.</p><p>What method can a Solutions Architect use to distribute traffic as requested?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Route 53 weighted routing policy is similar to simple but you can specify a weight per IP address. You create records that have the same name and type and assign each record a relative weight which is a numerical value that favours one IP over another (values must total 100). To stop sending traffic to a resource you can change the weight of the record to 0.</p><p><strong>CORRECT: </strong>\"Use Route 53 with a weighted routing policy and configure the respective weights\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Route 53 with a simple routing policy\" is incorrect as this will not split traffic based on weights as required.</p><p><strong>INCORRECT:</strong> \"Use an Application Load Balancer to distribute traffic based on IP address\" is incorrect. Application Load Balancer can distribute traffic to AWS and on-premise resources using IP addresses but cannot be used to distribute traffic in a weighted manner.</p><p><strong>INCORRECT:</strong> \"Use a Network Load Balancer to distribute traffic based on Instance ID\" is incorrect. Network Load Balancer can distribute traffic to AWS and on-premise resources using IP addresses (not Instance IDs).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
        "answers": [
          "<p>Use a Network Load Balancer to distribute traffic based on Instance ID  </p>",
          "<p>Use an Application Load Balancer to distribute traffic based on IP address  </p>",
          "<p>Use Route 53 with a weighted routing policy and configure the respective weights  </p>",
          "<p>Use Route 53 with a simple routing policy  </p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A company is transitioning their web presence into the AWS cloud. As part of the migration the company will be running a web application both on-premises and in AWS for a period of time. During the period of co-existence the client would like 80% of the traffic to hit the AWS-based web servers and 20% to be directed to the on-premises web servers.What method can a Solutions Architect use to distribute traffic as requested?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480380,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect enabled Access Logs on an Application Load Balancer (ALB) and needs to process the log files using a hosted Hadoop service. What configuration changes and services can be leveraged to deliver this requirement?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Access Logs can be enabled on ALB and configured to store data in an S3 bucket. Amazon EMR is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data. EMR utilizes a hosted Hadoop framework running on Amazon EC2 and Amazon S3.</p><p><strong>CORRECT: </strong>\"Configure Access Logs to be delivered to S3 and use EMR for processing the log files\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure Access Logs to be delivered to EC2 and install Hadoop for processing the log files\" is incorrect. EC2 does not provide a hosted Hadoop service.</p><p><strong>INCORRECT:</strong> \"Configure Access Logs to be delivered to DynamoDB and use EMR for processing the log files\" is incorrect. You cannot configure access logs to be delivered to DynamoDB.</p><p><strong>INCORRECT:</strong> \"Configure Access Logs to be delivered to S3 and use Kinesis for processing the log files\" is incorrect. Kinesis does not provide a hosted Hadoop service.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-emr/\">https://digitalcloud.training/amazon-emr/</a></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
        "answers": [
          "<p>Configure Access Logs to be delivered to DynamoDB and use EMR for processing the log files  </p>",
          "<p>Configure Access Logs to be delivered to S3 and use Kinesis for processing the log files  </p>",
          "<p>Configure Access Logs to be delivered to S3 and use EMR for processing the log files  </p>",
          "<p>Configure Access Logs to be delivered to EC2 and install Hadoop for processing the log files  </p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Analytics",
      "question_plain": "A Solutions Architect enabled Access Logs on an Application Load Balancer (ALB) and needs to process the log files using a hosted Hadoop service. What configuration changes and services can be leveraged to deliver this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480382,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A Solutions Architect has created a new security group in an Amazon VPC. No rules have been created. Which of the statements below are correct regarding the default state of the security group? (choose 2)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Custom security groups do not have inbound allow rules (all inbound traffic is denied by default) whereas default security groups do have inbound allow rules (allowing traffic from within the group). All outbound traffic is allowed by default in both custom and default security groups.</p><p>Security groups act like a stateful firewall at the instance level. Specifically security groups operate at the network interface level of an EC2 instance. You can only assign permit rules in a security group, you cannot assign deny rules and there is an implicit deny rule at the end of the security group. All rules are evaluated until a permit is encountered or continues until the implicit deny. You can create ingress and egress rules.</p><p><strong>CORRECT: </strong>\"There is an outbound rule that allows all traffic to all IP addresses\" is the correct answer.</p><p><strong>CORRECT: </strong>\"There are no inbound rules and traffic will be implicitly denied\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"There is an inbound rule allowing traffic from the Internet to port 22 for management\" is incorrect. This is not true.</p><p><strong>INCORRECT:</strong> \"There are is an inbound rule that allows traffic from the Internet Gateway\" is incorrect. There are no inbound allow rules by default.</p><p><strong>INCORRECT:</strong> \"There is an outbound rule allowing traffic to the Internet Gateway\" is incorrect. There is an outbound allow rule but it allows traffic to anywhere, it does not specify the internet gateway.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>There is an outbound rule that allows all traffic to all IP addresses  </p>",
          "<p>There are no inbound rules and traffic will be implicitly denied  </p>",
          "<p>There is an inbound rule allowing traffic from the Internet to port 22 for management  </p>",
          "<p>There is an outbound rule allowing traffic to the Internet Gateway  </p>",
          "<p>There are is an inbound rule that allows traffic from the Internet Gateway  </p>"
        ]
      },
      "correct_response": ["a", "b"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A Solutions Architect has created a new security group in an Amazon VPC. No rules have been created. Which of the statements below are correct regarding the default state of the security group? (choose 2)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480384,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A web application runs on a series of Amazon EC2 instances behind an Application Load Balancer (ALB). A Solutions Architect is updating the configuration with a health check and needs to select the protocol to use. What options are available? (choose 2)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>An Application Load Balancer periodically sends requests to its registered targets to test their status. These tests are called <em>health checks</em>.</p><p>Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. After your target is registered, it must pass one health check to be considered healthy. After each health check is completed, the load balancer node closes the connection that was established for the health check.</p><p>If a target group contains only unhealthy registered targets, the load balancer nodes route requests across its unhealthy targets.</p><p>For an ALB the possible protocols are HTTP and HTTPS. The default is the HTTP protocol.</p><p><strong>CORRECT: </strong>\"HTTP\" is the correct answer.</p><p><strong>CORRECT: </strong>\"HTTPS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"SSL\" is incorrect as this is not supported by the ALB.</p><p><strong>INCORRECT:</strong> \"TCP\" is incorrect as this is not supported by the ALB.</p><p><strong>INCORRECT:</strong> \"ICMP\" is incorrect as this is not supported by the ALB.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
        "answers": [
          "<p>HTTP</p>",
          "<p>SSL</p>",
          "<p>HTTPS</p>",
          "<p>TCP</p>",
          "<p>ICMP</p>"
        ]
      },
      "correct_response": ["a", "c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A web application runs on a series of Amazon EC2 instances behind an Application Load Balancer (ALB). A Solutions Architect is updating the configuration with a health check and needs to select the protocol to use. What options are available? (choose 2)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480386,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A Solutions Architect has created a new Network ACL in an Amazon VPC. No rules have been created. Which of the statements below are correct regarding the default state of the Network ACL? (choose 2)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>A VPC automatically comes with a default network ACL which allows all inbound/outbound traffic. A custom NACL denies all traffic both inbound and outbound by default.</p><p>Network ACL’s function at the subnet level and you can have permit and deny rules. Network ACLs have separate inbound and outbound rules and each rule can allow or deny traffic.</p><p>Network ACLs are stateless so responses are subject to the rules for the direction of traffic. NACLs only apply to traffic that is ingress or egress to the subnet not to traffic within the subnet.</p><p><strong>CORRECT: </strong>\"There is a default inbound rule denying all traffic\" is a correct answer.</p><p><strong>CORRECT: </strong>\"There is a default outbound rule denying all traffic\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"There is a default inbound rule allowing traffic from the VPC CIDR block\" is incorrect as inbound traffic is not allowed from anywhere by default.</p><p><strong>INCORRECT:</strong> \"There is a default outbound rule allowing traffic to the Internet Gateway\" is incorrect as outbound traffic is not allowed to anywhere by default.</p><p><strong>INCORRECT:</strong> \"There is a default outbound rule allowing all traffic\" is incorrect as all traffic is denied.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>There is a default inbound rule denying all traffic  </p>",
          "<p>There is a default outbound rule allowing all traffic  </p>",
          "<p>There is a default inbound rule allowing traffic from the VPC CIDR block  </p>",
          "<p>There is a default outbound rule denying all traffic  </p>",
          "<p>There is a default outbound rule allowing traffic to the Internet Gateway  </p>"
        ]
      },
      "correct_response": ["a", "d"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A Solutions Architect has created a new Network ACL in an Amazon VPC. No rules have been created. Which of the statements below are correct regarding the default state of the Network ACL? (choose 2)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480388,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An Amazon EBS-backed EC2 instance has been launched. A requirement has come up for some high-performance ephemeral storage.</p><p>How can a Solutions Architect add a new instance store volume?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>You can specify the instance store volumes for your instance only when you launch an instance. You can’t attach instance store volumes to an instance after you’ve launched it.</p><p><strong>CORRECT: </strong>\"You can specify the instance store volumes for your instance only when you launch an instance\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"You must shutdown the instance in order to be able to add the instance store volume\" is incorrect. You can use a block device mapping to specify additional EBS volumes when you launch your instance, or you can attach additional EBS volumes after your instance is running.</p><p><strong>INCORRECT:</strong> \"You must use an Elastic Network Adapter (ENA) to add instance store volumes. First, attach an ENA, and then attach the instance store volume\" is incorrect. An Elastic Network Adapter has nothing to do with adding instance store volumes.</p><p><strong>INCORRECT:</strong> \"You can use a block device mapping to specify additional instance store volumes when you launch your instance, or you can attach additional instance store volumes after your instance is running\" is incorrect. You can’t attach instance store volumes to an instance after you’ve launched it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/add-instance-store-volumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/add-instance-store-volumes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
        "answers": [
          "<p>You can specify the instance store volumes for your instance only when you launch an instance  </p>",
          "<p>You can use a block device mapping to specify additional instance store volumes when you launch your instance, or you can attach additional instance store volumes after your instance is running  </p>",
          "<p>You must shutdown the instance in order to be able to add the instance store volume  </p>",
          "<p>You must use an Elastic Network Adapter (ENA) to add instance store volumes. First, attach an ENA, and then attach the instance store volume  </p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Storage",
      "question_plain": "An Amazon EBS-backed EC2 instance has been launched. A requirement has come up for some high-performance ephemeral storage.How can a Solutions Architect add a new instance store volume?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480390,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A large quantity of data that is rarely accessed is being archived onto Amazon Glacier. Your CIO wants to understand the resilience of the service. Which of the statements below is correct about Amazon Glacier storage? (choose 2) </p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Glacier is designed for durability of 99.999999999% of objects across multiple Availability Zones. Data is resilient in the event of one entire Availability Zone destruction. Glacier supports SSL for data in transit and encryption of data at rest. Glacier is extremely low cost and is ideal for long-term archival.</p><p><strong>CORRECT: </strong>\"Provides 99.999999999% durability of archives\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Data is resilient in the event of one entire Availability Zone destruction\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Data is replicated globally\" is incorrect. Data is not replicated globally.</p><p><strong>INCORRECT:</strong> \"Data is resilient in the event of one entire region destruction\" is incorrect. Data is not resilient to the failure of an entire region.</p><p><strong>INCORRECT:</strong> \"Provides 99.9% availability of archives\" is incorrect. Glacier is “designed for” availability of <strong>99.99%</strong></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
        "answers": [
          "<p>Provides 99.9% availability of archives  </p>",
          "<p>Data is resilient in the event of one entire region destruction  </p>",
          "<p>Data is resilient in the event of one entire Availability Zone destruction  </p>",
          "<p>Provides 99.999999999% durability of archives  </p>",
          "<p>Data is replicated globally  </p>"
        ]
      },
      "correct_response": ["c", "d"],
      "section": "AWS Storage",
      "question_plain": "A large quantity of data that is rarely accessed is being archived onto Amazon Glacier. Your CIO wants to understand the resilience of the service. Which of the statements below is correct about Amazon Glacier storage? (choose 2)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480392,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>The application development team in a company have developed a Java application and saved the source code in a .war file. They would like to run the application on AWS resources and are looking for a service that can handle the provisioning and management of the underlying resources it will run on.</p><p>Which AWS service should a Solutions Architect recommend the Developers use to upload the Java source code file?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Elastic Beanstalk can be used to quickly deploy and manage applications in the AWS Cloud. Developers upload applications and Elastic Beanstalk handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring</p><p>Elastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby, as well as different platform configurations for each language. To use Elastic Beanstalk, you create an application, upload an application version in the form of an application source bundle (for example, a Java .war file) to Elastic Beanstalk, and then provide some information about the application.</p><p><strong>CORRECT: </strong>\"AWS Elastic Beanstalk\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodeDeploy\" is incorrect. AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.</p><p><strong>INCORRECT:</strong> \"AWS CloudFormation\" is incorrect. AWS CloudFormation uses templates to deploy infrastructure as code. It is not a PaaS service like Elastic Beanstalk and is more focused on infrastructure than applications and management of applications.</p><p><strong>INCORRECT:</strong> \"AWS OpsWorks\" is incorrect. AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
        "answers": [
          "<p>AWS CodeDeploy  </p>",
          "<p>AWS Elastic Beanstalk  </p>",
          "<p>AWS CloudFormation  </p>",
          "<p>AWS OpsWorks  </p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "The application development team in a company have developed a Java application and saved the source code in a .war file. They would like to run the application on AWS resources and are looking for a service that can handle the provisioning and management of the underlying resources it will run on.Which AWS service should a Solutions Architect recommend the Developers use to upload the Java source code file?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480394,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>The load on a MySQL database running on Amazon EC2 is increasing and performance has been impacted. Which of the options below would help to increase storage performance? (choose 2)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>EBS optimized instances provide dedicated capacity for Amazon EBS I/O. EBS optimized instances are designed for use with all EBS volume types.</p><p>Provisioned IOPS EBS volumes allow you to specify the amount of IOPS you require up to 50 IOPS per GB. Within this limitation you can therefore choose to select the IOPS required to improve the performance of your volume.</p><p>RAID can be used to increase IOPS, however RAID 1 does not. For example:</p><p>– RAID 0 = 0 striping – data is written across multiple disks and increases performance but no redundancy.</p><p>– RAID 1 = 1 mirroring – creates 2 copies of the data but does not increase performance, only redundancy.</p><p>HDD, Cold – (SC1) provides the lowest cost storage and low performance</p><p><strong>CORRECT: </strong>\"Use Provisioned IOPS (I01) EBS volumes\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Use EBS optimized instances\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Use a larger instance size within the instance family\" is incorrect as this may not increase storage performance.</p><p><strong>INCORRECT:</strong> \"Use HDD, Cold (SC1) EBS volumes\" is incorrect. As this will likely decrease storage performance.</p><p><strong>INCORRECT:</strong> \"Create a RAID 1 array from multiple EBS volumes\" is incorrect. As explained above, mirroring does not increase performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
        "answers": [
          "<p>Use EBS optimized instances  </p>",
          "<p>Use a larger instance size within the instance family  </p>",
          "<p>Create a RAID 1 array from multiple EBS volumes  </p>",
          "<p>Use Provisioned IOPS (I01) EBS volumes  </p>",
          "<p>Use HDD, Cold (SC1) EBS volumes  </p>"
        ]
      },
      "correct_response": ["a", "d"],
      "section": "AWS Storage",
      "question_plain": "The load on a MySQL database running on Amazon EC2 is increasing and performance has been impacted. Which of the options below would help to increase storage performance? (choose 2)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480396,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A web application receives order processing information from customers and places the messages on an Amazon SQS queue. A fleet of Amazon EC2 instances are configured to pick up the messages, process them, and store the results in a DynamoDB table. The current configuration has been resulting in a large number of empty responses to <code>ReceiveMessage</code> API requests.</p><p>A Solutions Architect needs to eliminate empty responses to reduce operational overhead. How can this be done?&nbsp; </p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The correct answer is to use Long Polling which will eliminate empty responses by allowing Amazon SQS to wait until a message is available in a queue before sending a response.</p><p>The problem does not relate to the order in which the messages are processed in and there are no concerns over messages being delivered more than once so it doesn’t matter whether you use a FIFO or standard queue.</p><p><strong>Long Polling:</strong></p><p>– Uses fewer requests and reduces cost.</p><p>– Eliminates false empty responses by querying all servers.</p><p>– SQS waits until a message is available in the queue before sending a response.</p><p><strong>Short Polling:</strong></p><p>– Does not wait for messages to appear in the queue.</p><p>– It queries only a subset of the available servers for messages (based on weighted random execution).</p><p>– Short polling is the default.</p><p>– ReceiveMessageWaitTime is set to 0.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-20-24-648b8e1ff1276f787c24226b61455dd3.png\"></p><p><strong>CORRECT: </strong>\"Configure Long Polling to eliminate empty responses by allowing Amazon SQS to wait until a message is available in a queue before sending a response\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a Standard queue to provide at-least-once delivery, which means that each message is delivered at least once\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Use a FIFO (first-in-first-out) queue to preserve the exact order in which messages are sent and received\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Configure Short Polling to eliminate empty responses by reducing the length of time a connection request remains open\" is incorrect as explained above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
        "answers": [
          "<p>Configure Long Polling to eliminate empty responses by allowing Amazon SQS to wait until a message is available in a queue before sending a response  </p>",
          "<p>Configure Short Polling to eliminate empty responses by reducing the length of time a connection request remains open  </p>",
          "<p>Use a FIFO (first-in-first-out) queue to preserve the exact order in which messages are sent and received  </p>",
          "<p>Use a Standard queue to provide at-least-once delivery, which means that each message is delivered at least once  </p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Application Integration",
      "question_plain": "A web application receives order processing information from customers and places the messages on an Amazon SQS queue. A fleet of Amazon EC2 instances are configured to pick up the messages, process them, and store the results in a DynamoDB table. The current configuration has been resulting in a large number of empty responses to ReceiveMessage API requests.A Solutions Architect needs to eliminate empty responses to reduce operational overhead. How can this be done?&nbsp;",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480398,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A Solutions Architect is launching an Amazon EC2 instance with multiple attached volumes by modifying the block device mapping. Which block device can be specified in a block device mapping to be used with an EC2 instance? (choose 2)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Each instance that you launch has an associated root device volume, either an Amazon EBS volume or an instance store volume.</p><p>You can use block device mapping to specify additional EBS volumes or instance store volumes to attach to an instance when it’s launched. You can also attach additional EBS volumes to a running instance.</p><p>You cannot use a block device mapping to specify a snapshot, EFS volume or S3 bucket.</p><p><strong>CORRECT: </strong>\"EBS volume\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Instance store volume\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"EFS volume\" is incorrect as described above.</p><p><strong>INCORRECT:</strong> \"Snapshot\" is incorrect as described above.</p><p><strong>INCORRECT:</strong> \"S3 bucket\" is incorrect as described above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/block-device-mapping-concepts.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/block-device-mapping-concepts.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
        "answers": [
          "<p>Snapshot  </p>",
          "<p>Instance store volume  </p>",
          "<p>EBS volume  </p>",
          "<p>EFS volume  </p>",
          "<p>S3 bucket  </p>"
        ]
      },
      "correct_response": ["b", "c"],
      "section": "AWS Storage",
      "question_plain": "A Solutions Architect is launching an Amazon EC2 instance with multiple attached volumes by modifying the block device mapping. Which block device can be specified in a block device mapping to be used with an EC2 instance? (choose 2)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480400,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A Solutions Architect has created an AWS account and selected the Asia Pacific (Sydney) region. Within the default VPC there is a default security group. What settings are configured within this security group by default? (choose 2)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Default security groups have inbound allow rules (allowing traffic from within the group) whereas custom security groups do not have inbound allow rules (all inbound traffic is denied by default). All outbound traffic is allowed by default in custom and default security groups.</p><p><strong>CORRECT: </strong>\"There is an inbound rule that allows all traffic from the security group itself\" is a correct answer.</p><p><strong>CORRECT: </strong>\"There is an outbound rule that allows all traffic to all addresses\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"There is an inbound rule that allows all traffic from any address\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"There is an outbound rule that allows all traffic to the security group itself\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"There is an outbound rule that allows traffic to the VPC router\" is incorrect as explained above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>There is an inbound rule that allows all traffic from the security group itself  </p>",
          "<p>There is an inbound rule that allows all traffic from any address  </p>",
          "<p>There is an outbound rule that allows all traffic to all addresses  </p>",
          "<p>There is an outbound rule that allows all traffic to the security group itself  </p>",
          "<p>There is an outbound rule that allows traffic to the VPC router  </p>"
        ]
      },
      "correct_response": ["a", "c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A Solutions Architect has created an AWS account and selected the Asia Pacific (Sydney) region. Within the default VPC there is a default security group. What settings are configured within this security group by default? (choose 2)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480402,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An Amazon EC2 instance is generating very high packets-per-second and performance of the application stack is being impacted. A Solutions Architect needs to determine a resolution to the issue that results in improved performance.</p><p>Which action should the Architect take?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Enhanced networking provides higher bandwidth, higher packet-per-second (PPS) performance, and consistently lower inter-instance latencies. If your packets-per-second rate appears to have reached its ceiling, you should consider moving to enhanced networking because you have likely reached the upper thresholds of the VIF driver. It is only available for certain instance types and only supported in VPC. You must also launch an HVM AMI with the appropriate drivers.</p><p>AWS currently supports enhanced networking capabilities using SR-IOV. SR-IOV provides direct access to network adapters, provides higher performance (packets-per-second) and lower latency.</p><p><strong>CORRECT: </strong>\"Use enhanced networking\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure a RAID 1 array from multiple EBS volumes\" is incorrect. You do not need to create a RAID 1 array (which is more for redundancy than performance anyway).</p><p><strong>INCORRECT:</strong> \"Create a placement group and put the EC2 instance in it\" is incorrect. A placement group is used to increase network performance between instances. In this case there is only a single instance so it won’t help.</p><p><strong>INCORRECT:</strong> \"Add multiple Elastic IP addresses to the instance\" is incorrect. Adding multiple IP addresses is not a way to increase performance of the instance as the same amount of bandwidth is available to the Elastic Network Interface (ENI).</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/enable-configure-enhanced-networking/\">https://aws.amazon.com/premiumsupport/knowledge-center/enable-configure-enhanced-networking/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
        "answers": [
          "<p>Configure a RAID 1 array from multiple EBS volumes  </p>",
          "<p>Create a placement group and put the EC2 instance in it  </p>",
          "<p>Use enhanced networking  </p>",
          "<p>Add multiple Elastic IP addresses to the instance  </p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Compute",
      "question_plain": "An Amazon EC2 instance is generating very high packets-per-second and performance of the application stack is being impacted. A Solutions Architect needs to determine a resolution to the issue that results in improved performance.Which action should the Architect take?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480404,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An Amazon EC2 instance behind an Elastic Load Balancer (ELB) is in the process of being de-registered. Which ELB feature is used to allow existing connections to close cleanly?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Connection draining is enabled by default and provides a period of time for existing connections to close cleanly. When connection draining is in action an CLB will be in the status “InService: Instance deregistration currently in progress”.</p><p><strong>CORRECT: </strong>\"Connection Draining\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Sticky Sessions\" is incorrect. Session stickiness uses cookies and ensures a client is bound to an individual back-end instance for the duration of the cookie lifetime.</p><p><strong>INCORRECT:</strong> \"Proxy Protocol\" is incorrect. The Proxy Protocol header helps you identify the IP address of a client when you have a load balancer that uses TCP for back-end connections.</p><p><strong>INCORRECT:</strong> \"Deletion Protection\" is incorrect. Deletion protection is used to protect the ELB from deletion.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2014/03/20/elastic-load-balancing-supports-connection-draining/\">https://aws.amazon.com/about-aws/whats-new/2014/03/20/elastic-load-balancing-supports-connection-draining/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
        "answers": [
          "<p>Sticky Sessions  </p>",
          "<p>Deletion Protection  </p>",
          "<p>Connection Draining  </p>",
          "<p>Proxy Protocol  </p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "An Amazon EC2 instance behind an Elastic Load Balancer (ELB) is in the process of being de-registered. Which ELB feature is used to allow existing connections to close cleanly?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480406,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect needs to upload a large (2GB) file to an S3 bucket. What is the recommended way to upload a single large file to an S3 bucket?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.</p><p><strong>CORRECT: </strong>\"Use Multipart Upload\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS Import/Export\" is incorrect. AWS Import/Export is a service in which you send in HDDs with data on to AWS and they import your data into S3. It is not used for single files.</p><p><strong>INCORRECT:</strong> \"Use a single PUT request to upload the large file\" is incorrect. The largest object that can be uploaded in a single PUT is 5 gigabytes.</p><p><strong>INCORRECT:</strong> \"Use Amazon Snowball\" is incorrect. Snowball is used for migrating large quantities (TB/PB) of data into AWS, it is overkill for this requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
        "answers": [
          "<p>Use Amazon Snowball  </p>",
          "<p>Use a single PUT request to upload the large file  </p>",
          "<p>Use Multipart Upload  </p>",
          "<p>Use AWS Import/Export  </p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "A Solutions Architect needs to upload a large (2GB) file to an S3 bucket. What is the recommended way to upload a single large file to an S3 bucket?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480408,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>One of the departments in a company has been generating a large amount of data on Amazon S3 and costs are increasing. Data older than 90 days is rarely accessed but must be retained for several years. If this data does need to be accessed at least 24 hours notice is provided.</p><p>How can a Solutions Architect optimize the costs associated with storage of this data whilst ensuring it is accessible if required?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>To manage your objects so that they are stored cost effectively throughout their lifecycle, configure their lifecycle. A lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. Transition actions define when objects transition to another storage class.</p><p>For example, you might choose to transition objects to the STANDARD_IA storage class 30 days after you created them, or archive objects to the GLACIER storage class one year after creating them.</p><p>GLACIER retrieval times:</p><p>- Standard retrieval is 3-5 hours which is well within the requirements here.</p><p>- You can use Expedited retrievals to access data in 1 – 5 minutes.</p><p>- You can use Bulk retrievals to access up to petabytes of data in approximately 5 – 12 hours.</p><p><strong>CORRECT: </strong>\"Use S3 lifecycle policies to move data to GLACIER after 90 days\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement archival software that automatically moves the data to tape\" is incorrect as this solution can be fully automated using lifecycle policies.</p><p><strong>INCORRECT:</strong> \"Use S3 lifecycle policies to move data to the STANDARD_IA storage class\" is incorrect. STANDARD_IA is good for infrequently accessed data and provides faster access times than GLACIER but is more expensive so not the best option here.</p><p><strong>INCORRECT:</strong> \"Select the older data and manually migrate it to GLACIER\" is incorrect as a lifecycle policy can automate the process.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2016/11/access-your-amazon-glacier-data-in-minutes-with-new-retrieval-options/\">https://aws.amazon.com/about-aws/whats-new/2016/11/access-your-amazon-glacier-data-in-minutes-with-new-retrieval-options/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
        "answers": [
          "<p>Select the older data and manually migrate it to GLACIER  </p>",
          "<p>Use S3 lifecycle policies to move data to GLACIER after 90 days  </p>",
          "<p>Use S3 lifecycle policies to move data to the STANDARD_IA storage class  </p>",
          "<p>Implement archival software that automatically moves the data to tape  </p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Storage",
      "question_plain": "One of the departments in a company has been generating a large amount of data on Amazon S3 and costs are increasing. Data older than 90 days is rarely accessed but must be retained for several years. If this data does need to be accessed at least 24 hours notice is provided.How can a Solutions Architect optimize the costs associated with storage of this data whilst ensuring it is accessible if required?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480410,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application receives a high traffic load between 7:30am and 9:30am daily. The application uses an Auto Scaling group to maintain three instances most of the time but during the peak period it requires six instances.</p><p>How can a Solutions Architect configure Auto Scaling to perform a daily scale-out event at 7:30am and a scale-in event at 9:30am to account for the peak load?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The following scaling policy options are available:</p><p><strong>Simple</strong> – maintains a current number of instances, you can manually change the ASGs min/desired/max and attach/detach instances.</p><p><strong>Scheduled</strong> – Used for predictable load changes, can be a single event or a recurring schedule</p><p><strong>Dynamic </strong>(event based) – scale in response to an event/alarm.</p><p>Step – configure multiple scaling steps in response to multiple alarms.</p><p><strong>CORRECT: </strong>\"Use a Scheduled scaling policy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a Simple scaling policy\" is incorrect. Please refer to the description above.</p><p><strong>INCORRECT:</strong> \"Use a Dynamic scaling policy\" is incorrect. Please refer to the description above.</p><p><strong>INCORRECT:</strong> \"Use a Step scaling policy\" is incorrect. Please refer to the description above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
        "answers": [
          "<p>Use a Simple scaling policy  </p>",
          "<p>Use a Scheduled scaling policy  </p>",
          "<p>Use a Dynamic scaling policy  </p>",
          "<p>Use a Step scaling policy  </p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "An application receives a high traffic load between 7:30am and 9:30am daily. The application uses an Auto Scaling group to maintain three instances most of the time but during the peak period it requires six instances.How can a Solutions Architect configure Auto Scaling to perform a daily scale-out event at 7:30am and a scale-in event at 9:30am to account for the peak load?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480412,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company requires an Elastic Load Balancer (ELB) for an application they are planning to deploy on AWS. The application requires extremely high throughput and extremely low latencies. The connections will be made using the TCP protocol and the ELB must support load balancing to multiple ports on an instance. Which ELB would should the company use?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The Network Load Balancer operates at the connection level (Layer 4), routing connections to targets – Amazon EC2 instances, containers and IP addresses based on IP protocol data. It is architected to handle millions of requests/sec, sudden volatile traffic patterns and provides extremely low latencies.</p><p>The NLB provides high throughput and extremely low latencies and is designed to handle traffic as it grows and can load balance millions of requests/second. NLB also supports load balancing to multiple ports on an instance.</p><p><strong>CORRECT: </strong>\"Network Load Balancer\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Classic Load Balancer\" is incorrect. The CLB operates using the TCP, SSL, HTTP and HTTPS protocols. It is not the best choice for requirements of extremely high throughput and low latency and does not support load balancing to multiple ports on an instance.</p><p><strong>INCORRECT:</strong> \"Application Load Balancer\" is incorrect. The ALB operates at the HTTP and HTTPS level only (does not support TCP load balancing).</p><p><strong>INCORRECT:</strong> \"Route 53\" is incorrect. Route 53 is a DNS service, it is not a type of ELB (though you can do some types of load balancing with it).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
        "answers": [
          "<p>Classic Load Balancer  </p>",
          "<p>Application Load Balancer  </p>",
          "<p>Network Load Balancer  </p>",
          "<p>Route 53  </p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A company requires an Elastic Load Balancer (ELB) for an application they are planning to deploy on AWS. The application requires extremely high throughput and extremely low latencies. The connections will be made using the TCP protocol and the ELB must support load balancing to multiple ports on an instance. Which ELB would should the company use?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480414,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company runs a web-based application that uses Amazon EC2 instances for the web front-end and Amazon RDS for the database back-end. The web application writes transaction log files to an Amazon S3 bucket and the quantity of files is becoming quite large. It is acceptable to retain the most recent 60 days of log files and permanently delete the rest.</p><p>Which action can a Solutions Architect take to enable this to happen automatically?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>To manage your objects so that they are stored cost effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:</p><p>- Transition actions—Define when objects transition to another <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html\">storage class</a>. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them.</p><p>- Expiration actions—Define when objects expire. Amazon S3 deletes expired objects on your behalf.</p><p><strong>CORRECT: </strong>\"Use an S3 lifecycle policy with object expiration configured to automatically remove objects that are more than 60 days old\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Write a Ruby script that checks the age of objects and deletes any that are more than 60 days old\" is incorrect as the automated method is to use object expiration.</p><p><strong>INCORRECT:</strong> \"Use an S3 bucket policy that deletes objects that are more than 60 days old\" is incorrect as you cannot do this with bucket policies.</p><p><strong>INCORRECT:</strong> \"Use an S3 lifecycle policy to move the log files that are more than 60 days old to the GLACIER storage class\" is incorrect. Moving logs to Glacier may save cost but the question requests that the files are permanently deleted.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
        "answers": [
          "<p>Write a Ruby script that checks the age of objects and deletes any that are more than 60 days old  </p>",
          "<p>Use an S3 lifecycle policy to move the log files that are more than 60 days old to the GLACIER storage class  </p>",
          "<p>Use an S3 lifecycle policy with object expiration configured to automatically remove objects that are more than 60 days old  </p>",
          "<p>Use an S3 bucket policy that deletes objects that are more than 60 days old  </p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "A company runs a web-based application that uses Amazon EC2 instances for the web front-end and Amazon RDS for the database back-end. The web application writes transaction log files to an Amazon S3 bucket and the quantity of files is becoming quite large. It is acceptable to retain the most recent 60 days of log files and permanently delete the rest.Which action can a Solutions Architect take to enable this to happen automatically?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480416,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An on-premise data center will be connected to an Amazon VPC by a hardware VPN that has public and VPN-only subnets. The security team has requested that traffic hitting public subnets on AWS that’s destined to on-premise applications must be directed over the VPN to the corporate firewall.</p><p>How can this be achieved?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Route tables determine where network traffic is directed. In your route table, you must add a route for your remote network and specify the virtual private gateway as the target. This enables traffic from your VPC that’s destined for your remote network to route via the virtual private gateway and over one of the VPN tunnels. You can enable route propagation for your route table to automatically propagate your network routes to the table for you.</p><p><strong>CORRECT: </strong>\"In the public subnet route table, add a route for your remote network and specify the virtual private gateway as the target\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"In the VPN-only subnet route table, add a route that directs all Internet traffic to the virtual private gateway\" is incorrect. You must create the route table rule in the route table attached to the public subnet, not the VPN-only subnet.</p><p><strong>INCORRECT:</strong> \"In the public subnet route table, add a route for your remote network and specify the customer gateway as the target\" is incorrect. You must select the virtual private gateway (AWS side of the VPN) not the customer gateway (customer side of the VPN) in the target in the route table.</p><p><strong>INCORRECT:</strong> \"Configure a NAT Gateway and configure all traffic to be directed via the virtual private gateway\" is incorrect. NAT Gateways are used to enable Internet access for EC2 instances in private subnets, they cannot be used to direct traffic to VPG.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_VPN.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_VPN.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario3.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario3.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>In the public subnet route table, add a route for your remote network and specify the customer gateway as the target</p>",
          "<p>Configure a NAT Gateway and configure all traffic to be directed via the virtual private gateway  </p>",
          "<p>In the public subnet route table, add a route for your remote network and specify the virtual private gateway as the target  </p>",
          "<p>In the VPN-only subnet route table, add a route that directs all Internet traffic to the virtual private gateway  </p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "An on-premise data center will be connected to an Amazon VPC by a hardware VPN that has public and VPN-only subnets. The security team has requested that traffic hitting public subnets on AWS that’s destined to on-premise applications must be directed over the VPN to the corporate firewall.How can this be achieved?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480418,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect is designing the disk configuration for an Amazon EC2 instance. The instance needs to support a MapReduce process that requires high throughput for a large dataset with large I/O sizes.</p><p>Which Amazon EBS volume is the MOST cost-effective solution for these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>EBS Throughput Optimized HDD is good for the following use cases (and is the most cost-effective option:</p><p>- Frequently accessed, throughput intensive workloads with large datasets and large I/O sizes, such as MapReduce, Kafka, log processing, data warehouse, and ETL workloads.</p><p>Throughput is measured in MB/s, and includes the ability to burst up to 250 MB/s per TB, with a baseline throughput of 40 MB/s per TB and a maximum throughput of 500 MB/s per volume.</p><p><strong>CORRECT: </strong>\"EBS Throughput Optimized HDD\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"EBS General Purpose SSD in a RAID 1 configuration\" is incorrect. This is not the best solution for the requirements or the most cost-effective.</p><p><strong>INCORRECT:</strong> \"EBS Provisioned IOPS SSD\" is incorrect. SSD disks are more expensive.</p><p><strong>INCORRECT:</strong> \"EBS General Purpose SSD\" is incorrect. SSD disks are more expensive.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
        "answers": [
          "<p>EBS General Purpose SSD  </p>",
          "<p>EBS Provisioned IOPS SSD  </p>",
          "<p>EBS Throughput Optimized HDD  </p>",
          "<p>EBS General Purpose SSD in a RAID 1 configuration  </p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "A Solutions Architect is designing the disk configuration for an Amazon EC2 instance. The instance needs to support a MapReduce process that requires high throughput for a large dataset with large I/O sizes.Which Amazon EBS volume is the MOST cost-effective solution for these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480420,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect has logged into an Amazon EC2 Linux instance using SSH and needs to determine a few pieces of information including what IAM role is assigned, the instance ID and the names of the security groups that are assigned to the instance.</p><p>From the options below, what would be the best source of this information?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p><em>Instance metadata</em> is data about your instance that you can use to configure or manage the running instance. Instance metadata is divided into categories, for example, host name, events, and security groups.</p><p>Instance metadata is available at http://169.254.169.254/latest/meta-data.</p><p><strong>CORRECT: </strong>\"Metadata\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Tags\" is incorrect. Tags are used to categorize and label resources.</p><p><strong>INCORRECT:</strong> \"User data\" is incorrect. User data is used to configure the system at launch time and specify scripts.</p><p><strong>INCORRECT:</strong> \"Parameters\" is incorrect. Parameters are used in databases.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
        "answers": [
          "<p>Tags  </p>",
          "<p>Parameters  </p>",
          "<p>User data  </p>",
          "<p>Metadata  </p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Compute",
      "question_plain": "A Solutions Architect has logged into an Amazon EC2 Linux instance using SSH and needs to determine a few pieces of information including what IAM role is assigned, the instance ID and the names of the security groups that are assigned to the instance.From the options below, what would be the best source of this information?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480422,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A development team needs to run up a few lab servers on a weekend for a new project. The servers will need to run uninterrupted for a few hours. Which EC2 pricing option would be most suitable?  </p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>On-Demand pricing ensures that instances will not be terminated and is the most economical option. Use on-demand for ad-hoc requirements where you cannot tolerate interruption.</p><p><strong>CORRECT: </strong>\"On-Demand\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Spot\" is incorrect. Spot pricing may be the most economical option for a short duration over a weekend but you may have the instances terminated by AWS and there is a requirement that the servers run uninterrupted.</p><p><strong>INCORRECT:</strong> \"Reserved\" is incorrect. Reserved pricing provides a reduced cost for a contracted period (1 or 3 years), and is not suitable for ad hoc requirements.</p><p><strong>INCORRECT:</strong> \"Dedicated instances\" is incorrect. Dedicated instances run on hardware that’s dedicated to a single customer and are more expensive than regular On-Demand instances.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
        "answers": [
          "<p>Spot  </p>",
          "<p>Reserved</p>",
          "<p>On-Demand</p>",
          "<p>Dedicated instances</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Compute",
      "question_plain": "A development team needs to run up a few lab servers on a weekend for a new project. The servers will need to run uninterrupted for a few hours. Which EC2 pricing option would be most suitable?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480424,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An organization in the agriculture sector is deploying sensors and smart devices around factory plants and fields. The devices will collect information and send it to cloud applications running on AWS. </p><p>Which AWS service will securely connect the devices to the cloud applications?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS IoT Core is a managed cloud service that lets connected devices easily and securely interact with cloud applications and other devices. AWS IoT Core can support billions of devices and trillions of messages, and can process and route those messages to AWS endpoints and to other devices reliably and securely.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-24-14-48d1a8665d7d70aa01818799e7168c7e.png\"><p><strong>CORRECT: </strong>\"AWS IoT Core\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Glue\" is incorrect. AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics.</p><p><strong>INCORRECT:</strong> \"AWS DMS\" is incorrect. AWS Database Migration Service helps you migrate databases to AWS quickly and securely.</p><p><strong>INCORRECT:</strong> \"AWS Lambda\" is incorrect. AWS Lambda lets you run code without provisioning or managing servers.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/iot-core/\">https://aws.amazon.com/iot-core/</a></p>",
        "answers": [
          "<p>AWS Lambda  </p>",
          "<p>AWS IoT Core  </p>",
          "<p>AWS Glue  </p>",
          "<p>AWS DMS  </p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Internet of Things",
      "question_plain": "An organization in the agriculture sector is deploying sensors and smart devices around factory plants and fields. The devices will collect information and send it to cloud applications running on AWS. Which AWS service will securely connect the devices to the cloud applications?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480426,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect has created a VPC and is in the process of formulating the subnet design. The VPC will be used to host a two-tier application that will include Internet facing web servers, and internal-only DB servers. Zonal redundancy is required. </p><p>How many subnets are required to support this requirement?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Zonal redundancy indicates that the architecture should be split across multiple Availability Zones. Subnets are mapped 1:1 to AZs.</p><p>A public subnet should be used for the Internet-facing web servers and a separate private subnet should be used for the internal-only DB servers. Therefore you need 4 subnets – 2 (for redundancy) per public/private subnet.</p><p><strong>CORRECT: </strong>\"4 subnets\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"2 subnets\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"6 subnets\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"2 subnet\" is incorrect as explained above.</p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>1 subnet</p>",
          "<p>2 subnets</p>",
          "<p>4 subnets</p>",
          "<p>6 subnets</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A Solutions Architect has created a VPC and is in the process of formulating the subnet design. The VPC will be used to host a two-tier application that will include Internet facing web servers, and internal-only DB servers. Zonal redundancy is required. How many subnets are required to support this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480428,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A company is deploying a new two-tier web application that uses EC2 web servers and a DynamoDB database backend. An Internet facing ELB distributes connections between the web servers.</p><p>The Solutions Architect has created a security group for the web servers and needs to create a security group for the ELB. What rules should be added? (choose 2)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>An inbound rule should be created for the relevant protocols (HTTP/HTTPS) and the source should be set to any address (0.0.0.0/0).</p><p>The outbound rule should forward the relevant protocols (HTTP/HTTPS) and the destination should be set to the web server security group.</p><p>Note that on the web server security group you’d want to add an Inbound rule allowing HTTP/HTTPS from the ELB security group.</p><p><strong>CORRECT: </strong>\"Add an Outbound rule that allows HTTP/HTTPS, and specify the destination as the web server security group\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Add an Inbound rule that allows HTTP/HTTPS, and specify the source as 0.0.0.0/0\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Add an Outbound rule that allows ALL TCP, and specify the destination as the Internet Gateway\" is incorrect as the relevant protocol should be specified and the destination should be the web server security group.</p><p><strong>INCORRECT:</strong> \"Add an Outbound rule that allows HTTP/HTTPS, and specify the destination as VPC CIDR\" is incorrect. Using the VPC CIDR would not be secure and you cannot specify an Internet Gateway in a security group (not that you’d want to anyway).</p><p><strong>INCORRECT:</strong> \"Add an Inbound rule that allows HTTP/HTTPS, and specify the source as 0.0.0.0/32\" is incorrect. The address 0.0.0.0/32 is incorrect as the 32 mask means an exact match is required (0.0.0.0).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
        "answers": [
          "<p>Add an Inbound rule that allows HTTP/HTTPS, and specify the source as 0.0.0.0/32  </p>",
          "<p>Add an Inbound rule that allows HTTP/HTTPS, and specify the source as 0.0.0.0/0  </p>",
          "<p>Add an Outbound rule that allows HTTP/HTTPS, and specify the destination as the web server security group  </p>",
          "<p>Add an Outbound rule that allows HTTP/HTTPS, and specify the destination as VPC CIDR  </p>",
          "<p>Add an Outbound rule that allows ALL TCP, and specify the destination as the Internet Gateway  </p>"
        ]
      },
      "correct_response": ["b", "c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A company is deploying a new two-tier web application that uses EC2 web servers and a DynamoDB database backend. An Internet facing ELB distributes connections between the web servers.The Solutions Architect has created a security group for the web servers and needs to create a security group for the ELB. What rules should be added? (choose 2)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480430,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An Amazon DynamoDB table has a variable load, ranging from sustained heavy usage some days, to only having small spikes on others. The load is 80% read and 20% write. The provisioned throughput capacity has been configured to account for the heavy load to ensure throttling does not occur.</p><p>What would be the most efficient solution to optimize cost?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p><em>Amazon DynamoDB auto scaling </em>uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This is the most efficient and cost-effective solution to optimizing for cost.</p><p><strong>CORRECT: </strong>\"Create a DynamoDB Auto Scaling scaling policy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that triggers an AWS Lambda function that adjusts the provisioned throughput\" is incorrect. Using AWS Lambda to modify the provisioned throughput is possible but it would be more cost-effective to use DynamoDB Auto Scaling as there is no cost to using it.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that notifies you of increased/decreased load, and manually adjust the provisioned throughput\" is incorrect. Manually adjusting the provisioned throughput is not efficient.</p><p><strong>INCORRECT:</strong> \"Use DynamoDB DAX to increase the performance of the database\" is incorrect. DynamoDB DAX is an in-memory cache that increases the performance of DynamoDB. However, it costs money and there is no requirement to increase performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
        "answers": [
          "<p>Create a DynamoDB Auto Scaling scaling policy  </p>",
          "<p>Create a CloudWatch alarm that notifies you of increased/decreased load, and manually adjust the provisioned throughput  </p>",
          "<p>Create a CloudWatch alarm that triggers an AWS Lambda function that adjusts the provisioned throughput  </p>",
          "<p>Use DynamoDB DAX to increase the performance of the database  </p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Database",
      "question_plain": "An Amazon DynamoDB table has a variable load, ranging from sustained heavy usage some days, to only having small spikes on others. The load is 80% read and 20% write. The provisioned throughput capacity has been configured to account for the heavy load to ensure throttling does not occur.What would be the most efficient solution to optimize cost?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480432,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company runs a streaming media service and the content is stored on Amazon S3. The media catalog server pulls updated content from S3 and can issue over 1 million read operations per second for short periods. Latency must be kept under 5ms for these updates. Which solution will provide the BEST performance for the media catalog updates?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Some applications, such as media catalog updates require high frequency reads, and consistent throughput. For such applications, customers often complement S3 with an in-memory cache, such as Amazon ElastiCache for Redis, to reduce the S3 retrieval cost and to improve performance.</p><p>ElastiCache for Redis is a fully managed, in-memory data store that provides sub-millisecond latency performance with high throughput. ElastiCache for Redis complements S3 in the following ways:</p><p>- Redis stores data in-memory, so it provides sub-millisecond latency and supports incredibly high requests per second.</p><p>- It supports key/value based operations that map well to S3 operations (for example, GET/SET =&gt; GET/PUT), making it easy to write code for both S3 and ElastiCache.</p><p>- It can be implemented as an application side cache. This allows you to use S3 as your persistent store and benefit from its durability, availability, and low cost. Your applications decide what objects to cache, when to cache them, and how to cache them.</p><p>In this example the media catalog is pulling updates from S3 so the performance between these components is what needs to be improved. Therefore, using ElastiCache to cache the content will dramatically increase the performance.</p><p><strong>CORRECT: </strong>\"Update the application code to use an Amazon ElastiCache for Redis cluster\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement Amazon CloudFront and cache the content at Edge Locations\" is incorrect. CloudFront is good for getting media closer to users but in this case we’re trying to improve performance within the data center moving data from S3 to the media catalog server.</p><p><strong>INCORRECT:</strong> \"Update the application code to use an Amazon DynamoDB Accelerator cluster\" is incorrect. DynamoDB Accelerator (DAX) is used with DynamoDB but is unsuitable for use with Amazon S3.</p><p><strong>INCORRECT:</strong> \"Implement an Instance store volume on the media catalog server\" is incorrect. This will improve local disk performance but will not improve reads from Amazon S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/storage/turbocharge-amazon-s3-with-amazon-elasticache-for-redis/\">https://aws.amazon.com/blogs/storage/turbocharge-amazon-s3-with-amazon-elasticache-for-redis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
        "answers": [
          "<p>Update the application code to use an Amazon ElastiCache for Redis cluster</p>",
          "<p>Implement Amazon CloudFront and cache the content at Edge Locations</p>",
          "<p>Update the application code to use an Amazon DynamoDB Accelerator cluster</p>",
          "<p>Implement an Instance store volume on the media catalog server</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Database",
      "question_plain": "A company runs a streaming media service and the content is stored on Amazon S3. The media catalog server pulls updated content from S3 and can issue over 1 million read operations per second for short periods. Latency must be kept under 5ms for these updates. Which solution will provide the BEST performance for the media catalog updates?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480434,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>Three AWS accounts are owned by the same company but in different regions. Account Z has two AWS Direct Connect connections to two separate company offices. Accounts A and B require the ability to route across account Z’s Direct Connect connections to each company office. A Solutions Architect has created an AWS Direct Connect gateway in account Z. </p><p>How can the required connectivity be configured?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>You can associate an <em>AWS Direct Connect gateway</em> with either of the following gateways:</p><p>- A transit gateway when you have multiple VPCs in the same Region.</p><p>- A virtual private gateway.</p><p>In this case account Z owns the Direct Connect gateway so a VPG in accounts A and B must be associated with it to enable this configuration to work. After Account Z accepts the proposals, Account A and Account B can route traffic from their virtual private gateway to the Direct Connect gateway.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-27-50-f7c361a3527b03f1a25861bfd2e7664b.png\"></p><p><strong>CORRECT: </strong>\"Associate the Direct Connect gateway to a virtual private gateway in account A and B\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Associate the Direct Connect gateway to a transit gateway in each region\" is incorrect. This would be a good solution if the accounts were in VPCs within a region rather than across regions.</p><p><strong>INCORRECT:</strong> \"Create a VPC Endpoint to the Direct Connect gateway in account A and B\" is incorrect. You cannot create a VPC endpoint for Direct Connect gateways.</p><p><strong>INCORRECT:</strong> \"Create a PrivateLink connection in Account Z and ENIs in accounts A and B\" is incorrect. You cannot use PrivateLink connections to publish a Direct Connect gateway.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-direct-connect/\">https://digitalcloud.training/aws-direct-connect/</a></p>",
        "answers": [
          "<p>Associate the Direct Connect gateway to a transit gateway in each region</p>",
          "<p>Associate the Direct Connect gateway to a virtual private gateway in account A and B</p>",
          "<p>Create a VPC Endpoint to the Direct Connect gateway in account A and B</p>",
          "<p>Create a PrivateLink connection in Account Z and ENIs in accounts A and B</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "Three AWS accounts are owned by the same company but in different regions. Account Z has two AWS Direct Connect connections to two separate company offices. Accounts A and B require the ability to route across account Z’s Direct Connect connections to each company office. A Solutions Architect has created an AWS Direct Connect gateway in account Z. How can the required connectivity be configured?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480436,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A tool needs to analyze data stored in an Amazon S3 bucket. Processing the data takes a few seconds and results are then written to another S3 bucket. Less than 256 MB of memory is needed to run the process. What would be the MOST cost-effective compute solutions for this use case?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Lambda has a maximum execution time of 900 seconds and memory can be allocated up to 3008 MB. Therefore, the most cost-effective solution will be AWS Lambda.</p><p><strong>CORRECT: </strong>\"AWS Lambda functions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Fargate tasks\" is incorrect. Fargate runs Docker containers and is serverless. However, you do pay for the running time of the tasks so it will not be as cost-effective.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 spot instances\" is incorrect. EC2 instances must run continually waiting for jobs to process so even with spot this would be less cost-effective (and subject to termination).</p><p><strong>INCORRECT:</strong> \"Amazon Elastic Beanstalk\" is incorrect. This services also relies on Amazon EC2 instances so would not be as cost-effective.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
        "answers": [
          "<p>AWS Fargate tasks</p>",
          "<p>AWS Lambda functions</p>",
          "<p>Amazon EC2 spot instances</p>",
          "<p>Amazon Elastic Beanstalk</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "A tool needs to analyze data stored in an Amazon S3 bucket. Processing the data takes a few seconds and results are then written to another S3 bucket. Less than 256 MB of memory is needed to run the process. What would be the MOST cost-effective compute solutions for this use case?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480438,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application makes calls to a REST API running on Amazon EC2 instances behind an Application Load Balancer (ALB). Most API calls complete quickly. However, a single endpoint is making API calls that require much longer to complete and this is introducing overall latency into the system. What steps can a Solutions Architect take to minimize the effects of the long-running API calls?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>An Amazon Simple Queue Service (SQS) can be used to offload and decouple the long-running requests. They can then be processed asynchronously by separate EC2 instances. This is the best way to reduce the overall latency introduced by the long-running API call.</p><p><strong>CORRECT: </strong>\"Create an Amazon SQS queue and decouple the long-running API calls\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Change the EC2 instance to one with enhanced networking to reduce latency\" is incorrect. This will not reduce the latency of the API call as network latency is not the issue here, it is the latency of how long the API call takes to complete.</p><p><strong>INCORRECT:</strong> \"Increase the ALB idle timeout to allow the long-running requests to complete\" is incorrect. The issue is not the connection being interrupted, it is that the API call takes a long time to complete.</p><p><strong>INCORRECT:</strong> \"Change the ALB to a Network Load Balancer (NLB) and use SSL/TLS termination\" is incorrect. SSL/TLS termination is not of benefit here as the problem is not encryption or processing of encryption. The issue is API call latency.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
        "answers": [
          "<p>Change the EC2 instance to one with enhanced networking to reduce latency</p>",
          "<p>Create an Amazon SQS queue and decouple the long-running API calls</p>",
          "<p>Increase the ALB idle timeout to allow the long-running requests to complete</p>",
          "<p>Change the ALB to a Network Load Balancer (NLB) and use SSL/TLS termination</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Application Integration",
      "question_plain": "An application makes calls to a REST API running on Amazon EC2 instances behind an Application Load Balancer (ALB). Most API calls complete quickly. However, a single endpoint is making API calls that require much longer to complete and this is introducing overall latency into the system. What steps can a Solutions Architect take to minimize the effects of the long-running API calls?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480440,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application runs on EC2 instances in a private subnet behind an Application Load Balancer in a public subnet. The application is highly available and distributed across multiple AZs. The EC2 instances must make API calls to an internet-based service. How can the Solutions Architect enable highly available internet connectivity?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The only solution presented that actually works is to create a NAT gateway in the public subnet of each AZ. They must be created in the public subnet as they gain public IP addresses and use an internet gateway for internet access.</p><p>The route tables in the private subnets must then be configured with a route to the NAT gateway and then the EC2 instances will be able to access the internet (subject to security group configuration).</p><p><strong>CORRECT: </strong>\"Create a NAT gateway in the public subnet of each AZ. Update the route tables for each private subnet to direct internet-bound traffic to the NAT gateway\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a NAT gateway and attach it to the VPC. Add a route to the gateway to each private subnet route table\" is incorrect. You do not attach NAT gateways to VPCs, you add them to public subnets.</p><p><strong>INCORRECT:</strong> \"Configure an internet gateway. Add a route to the gateway to each private subnet route table\" is incorrect. You cannot add a route to an internet gateway to a private subnet route table (private EC2 instances don’t even have public IP addresses).</p><p><strong>INCORRECT:</strong> \"Create a NAT instance in the private subnet of each AZ. Update the route tables for each private subnet to direct internet-bound traffic to the NAT instance\" is incorrect. You do not create NAT instances in private subnets, they must be created in public subnets.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>Create a NAT gateway and attach it to the VPC. Add a route to the gateway to each private subnet route table</p>",
          "<p>Configure an internet gateway. Add a route to the gateway to each private subnet route table</p>",
          "<p>Create a NAT instance in the private subnet of each AZ. Update the route tables for each private subnet to direct internet-bound traffic to the NAT instance</p>",
          "<p>Create a NAT gateway in the public subnet of each AZ. Update the route tables for each private subnet to direct internet-bound traffic to the NAT gateway</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Compute",
      "question_plain": "An application runs on EC2 instances in a private subnet behind an Application Load Balancer in a public subnet. The application is highly available and distributed across multiple AZs. The EC2 instances must make API calls to an internet-based service. How can the Solutions Architect enable highly available internet connectivity?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480442,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A legacy application is being migrated into AWS. The application has a large amount of data that is rarely accessed. When files are accessed they are retrieved sequentially. The application will be migrated onto an Amazon EC2 instance.</p><p>What is the LEAST expensive EBS volume type for this use case?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The cold HDD (sc1) EBS volume type is the lowest cost option that is suitable for this use case. The sc1 volume type is suitable for infrequently accessed data and use cases that are oriented towards throughput like sequential data access.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-29-08-7da5cb7bfcd2de70f1d5a02ddab9fb12.png\"></p><p><strong>CORRECT: </strong>\"Cold HDD (sc1)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Provisioned IOPS SSD (io1)\" is incorrect. This is the most expensive option and used for use cases that demand high IOPS.</p><p><strong>INCORRECT:</strong> \"General Purpose SSD (gp2)\" is incorrect. This is a more expensive SSD volume type that is used for general use cases.</p><p><strong>INCORRECT:</strong> \"Throughput Optimized HDD (st1)\" is incorrect. This is also used for throughput-oriented use cases however it is higher cost than sc1 and better for frequently accessed data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
        "answers": [
          "<p>Cold HDD (sc1)</p>",
          "<p>Provisioned IOPS SSD (io1)</p>",
          "<p>General Purpose SSD (gp2)</p>",
          "<p>Throughput Optimized HDD (st1)</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Compute",
      "question_plain": "A legacy application is being migrated into AWS. The application has a large amount of data that is rarely accessed. When files are accessed they are retrieved sequentially. The application will be migrated onto an Amazon EC2 instance.What is the LEAST expensive EBS volume type for this use case?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480444,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application uses an Amazon RDS database and Amazon EC2 instances in a web tier. The web tier instances must not be directly accessible from the internet to improve security.</p><p>How can a Solutions Architect meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>To prevent direct connectivity to the EC2 instances from the internet you can deploy your EC2 instances in a private subnet and have the ELB in a public subnet. To configure this you must enable a public subnet in the ELB that is in the same AZ as the private subnet.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-29-50-1c44f7977941f91bda9d48cd258911c6.png\"></p><p><strong>CORRECT: </strong>\"Launch the EC2 instances in a private subnet and create an Application Load Balancer in a public subnet\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instances in a private subnet with a NAT gateway and update the route table\" is incorrect. This configuration will not allow the application to be accessible from the internet, the aim is to only prevent direct access to the EC2 instances.</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instances in a public subnet and use AWS WAF to protect the instances from internet-based attacks\" is incorrect. With the EC2 instances in a public subnet, direct access from the internet is possible. It only takes a security group misconfiguration or software exploit and the instance becomes vulnerable to attack.</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instances in a public subnet and create an Application Load Balancer in a public subnet\" is incorrect. The EC2 instances should be launched in a private subnet.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/\">https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>Launch the EC2 instances in a private subnet and create an Application Load Balancer in a public subnet</p>",
          "<p>Launch the EC2 instances in a private subnet with a NAT gateway and update the route table</p>",
          "<p>Launch the EC2 instances in a public subnet and use AWS WAF to protect the instances from internet-based attacks</p>",
          "<p>Launch the EC2 instances in a public subnet and create an Application Load Balancer in a public subnet</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "An application uses an Amazon RDS database and Amazon EC2 instances in a web tier. The web tier instances must not be directly accessible from the internet to improve security.How can a Solutions Architect meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480446,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company runs an application on premises that stores a large quantity of semi-structured data using key-value pairs. The application code will be migrated to AWS Lambda and a highly scalable solution is required for storing the data.</p><p>Which datastore will be the best fit for these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon DynamoDB is a no-SQL database that stores data using key-value pairs. It is ideal for storing large amounts of semi-structured data and is also highly scalable. This is the best solution for storing this data based on the requirements in the scenario.</p><p><strong>CORRECT: </strong>\"Amazon DynamoDB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EFS\" is incorrect. The Amazon Elastic File System (EFS) is not suitable for storing key-value pairs.</p><p><strong>INCORRECT:</strong> \"Amazon RDS MySQL\" is incorrect. Amazon Relational Database Service (RDS) is used for structured data as it is an SQL type of database.</p><p><strong>INCORRECT:</strong> \"Amazon EBS\" is incorrect. Amazon Elastic Block Store (EBS) is a block-based storage system. You attach volumes to EC2 instances. It is not used for key-value pairs or to be used by Lambda functions.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/features/\">https://aws.amazon.com/dynamodb/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
        "answers": [
          "<p>Amazon EFS</p>",
          "<p>Amazon RDS MySQL</p>",
          "<p>Amazon EBS</p>",
          "<p>Amazon DynamoDB</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Database",
      "question_plain": "A company runs an application on premises that stores a large quantity of semi-structured data using key-value pairs. The application code will be migrated to AWS Lambda and a highly scalable solution is required for storing the data.Which datastore will be the best fit for these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480448,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application uses a MySQL database running on an Amazon EC2 instance. The application generates high I/O and constant writes to a single table on the database. Which Amazon EBS volume type will provide the MOST consistent performance and low latency?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The Provisioned IOPS SSD (io1) volume type will offer the most consistent performance and can be configured with the amount of IOPS required by the application. It will also provide the lowest latency of the options presented.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-30-39-f006b5d6b4ffa97e2db382aff552857e.png\"></p><p><strong>CORRECT: </strong>\"Provisioned IOPS SSD (io1)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"General Purpose SSD (gp2)\" is incorrect. This is not the best solution for when you require high I/O, consistent performance and low latency.</p><p><strong>INCORRECT:</strong> \"Throughput Optimized HDD (st1)\" is incorrect. This is a HDD type of disk and not suitable for low latency workloads that require consistent performance.</p><p><strong>INCORRECT:</strong> \"Cold HDD (sc1)\" is incorrect. This is the lowest cost option and not suitable for frequently accessed workloads.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
        "answers": [
          "<p>General Purpose SSD (gp2)</p>",
          "<p>Provisioned IOPS SSD (io1)</p>",
          "<p>Throughput Optimized HDD (st1)</p>",
          "<p>Cold HDD (sc1)</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "An application uses a MySQL database running on an Amazon EC2 instance. The application generates high I/O and constant writes to a single table on the database. Which Amazon EBS volume type will provide the MOST consistent performance and low latency?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480450,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect needs to capture information about the traffic that reaches an Amazon Elastic Load Balancer. The information should include the source, destination, and protocol.</p><p>What is the most secure and reliable method for gathering this data?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>You can use VPC Flow Logs to capture detailed information about the traffic going to and from your Elastic Load Balancer. Create a flow log for each network interface for your load balancer. There is one network interface per load balancer subnet.</p><p><strong>CORRECT: </strong>\"Create a VPC flow log for each network interface associated with the ELB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable Amazon CloudTrail logging and configure packet capturing\" is incorrect. CloudTrail performs auditing of API actions, it does not do packet capturing.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Logs to review detailed logging information\" is incorrect as this service does not record this information in CloudWatch logs.</p><p><strong>INCORRECT:</strong> \"Create a VPC flow log for the subnets in which the ELB is running\" is incorrect as the more secure option is to use the ELB network interfaces.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html</a><br><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-monitoring.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-monitoring.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
        "answers": [
          "<p>Create a VPC flow log for each network interface associated with the ELB</p>",
          "<p>Enable Amazon CloudTrail logging and configure packet capturing</p>",
          "<p>Use Amazon CloudWatch Logs to review detailed logging information</p>",
          "<p>Create a VPC flow log for the subnets in which the ELB is running</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A Solutions Architect needs to capture information about the traffic that reaches an Amazon Elastic Load Balancer. The information should include the source, destination, and protocol.What is the most secure and reliable method for gathering this data?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480452,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>The Solutions Architect in charge of a critical application must ensure the Amazon EC2 instances are able to be launched in another AWS Region in the event of a disaster.</p><p>What steps should the Solutions Architect take? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>You can create AMIs of the EC2 instances and then copy them across Regions. This provides a point-in-time copy of the state of the EC2 instance in the remote Region.</p><p>Once you’ve created AMIs of EC2 instances and copied them to the second Region, you can then launch the EC2 instances from the AMIs in that Region.</p><p>This is a good DR strategy as you have moved stateful EC2 instances to another Region.</p><p><strong>CORRECT: </strong>\"Create AMIs of the instances and copy them to another Region\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Launch instances in the second Region from the AMIs\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Launch instances in the second Region using the S3 API\" is incorrect. Though snapshots (and EBS-backed AMIs) are stored on Amazon S3, you cannot actually access them using the S3 API. You must use the EC2 API.</p><p><strong>INCORRECT:</strong> \"Enable cross-region snapshots for the Amazon EC2 instances\" is incorrect. You cannot enable “cross-region snapshots” as this is not a feature that currently exists.</p><p><strong>INCORRECT:</strong> \"Copy the snapshots using Amazon S3 cross-region replication\" is incorrect. You cannot work with snapshots using Amazon S3 at all including leveraging the cross-region replication feature.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/ebs-snapshot-copy/\">https://aws.amazon.com/blogs/aws/ebs-snapshot-copy/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
        "answers": [
          "<p>Launch instances in the second Region using the S3 API</p>",
          "<p>Create AMIs of the instances and copy them to another Region</p>",
          "<p>Enable cross-region snapshots for the Amazon EC2 instances</p>",
          "<p>Launch instances in the second Region from the AMIs</p>",
          "<p>Copy the snapshots using Amazon S3 cross-region replication</p>"
        ]
      },
      "correct_response": ["b", "d"],
      "section": "AWS Compute",
      "question_plain": "The Solutions Architect in charge of a critical application must ensure the Amazon EC2 instances are able to be launched in another AWS Region in the event of a disaster.What steps should the Solutions Architect take? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480454,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company needs to ensure that they can failover between AWS Regions in the event of a disaster seamlessly with minimal downtime and data loss. The applications will run in an active-active configuration.</p><p>Which DR strategy should a Solutions Architect recommend?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>A multi-site solution runs on AWS as well as on your existing on-site infrastructure in an active- active configuration. The data replication method that you employ will be determined by the recovery point that you choose. This is either Recovery Time Objective (the maximum allowable downtime before degraded operations are restored) or Recovery Point Objective (the maximum allowable time window whereby you will accept the loss of transactions during the DR process).</p><p><strong>CORRECT: </strong>\"Multi-site\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Backup and restore\" is incorrect. This is the lowest cost DR approach that simply entails creating online backups of all data and applications.</p><p><strong>INCORRECT:</strong> \"Pilot light\" is incorrect. With a pilot light strategy a core minimum of services are running and the remainder are only brought online during a disaster recovery situation.</p><p><strong>INCORRECT:</strong> \"Warm standby\" is incorrect. The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/\">https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/</a></p>",
        "answers": [
          "<p>Backup and restore</p>",
          "<p>Pilot light</p>",
          "<p>Warm standby</p>",
          "<p>Multi-site</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Cloud Architecture & Design",
      "question_plain": "A company needs to ensure that they can failover between AWS Regions in the event of a disaster seamlessly with minimal downtime and data loss. The applications will run in an active-active configuration.Which DR strategy should a Solutions Architect recommend?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480456,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A company has launched a multi-tier application architecture. The web tier and database tier run on Amazon EC2 instances in private subnets within the same Availability Zone.</p><p>Which combination of steps should a Solutions Architect take to add high availability to this architecture? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>The Solutions Architect can use Auto Scaling group across multiple AZs with an ALB in front to create an elastic and highly available architecture. Then, migrate the database to an Amazon RDS multi-AZ deployment to create HA for the database tier. This results in a fully redundant architecture that can withstand the failure of an availability zone.</p><p><strong>CORRECT: </strong>\"Create an Amazon EC2 Auto Scaling group and Application Load Balancer (ALB) spanning multiple AZs\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Create new private subnets in the same VPC but in a different AZ. Migrate the database to an Amazon RDS multi-AZ deployment\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create new public subnets in the same AZ for high availability and move the web tier to the public subnets\" is incorrect. If subnets share the same AZ they are not suitable for splitting your tier across them for HA as the failure of a an AZ will take out both subnets.</p><p><strong>INCORRECT:</strong> \"Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer (ALB)\" is incorrect. The instances are in a single AZ so the Solutions Architect should create a new auto scaling group and launch instances across multiple AZs.</p><p><strong>INCORRECT:</strong> \"Create new private subnets in the same VPC but in a different AZ. Create a database using Amazon EC2 in one AZ\" is incorrect. A database in a single AZ will not be highly available.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-increase-availability.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-increase-availability.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
        "answers": [
          "<p>Create new public subnets in the same AZ for high availability and move the web tier to the public subnets</p>",
          "<p>Create an Amazon EC2 Auto Scaling group and Application Load Balancer (ALB) spanning multiple AZs</p>",
          "<p>Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer (ALB)</p>",
          "<p>Create new private subnets in the same VPC but in a different AZ. Create a database using Amazon EC2 in one AZ</p>",
          "<p>Create new private subnets in the same VPC but in a different AZ. Migrate the database to an Amazon RDS multi-AZ deployment</p>"
        ]
      },
      "correct_response": ["b", "e"],
      "section": "AWS Compute",
      "question_plain": "A company has launched a multi-tier application architecture. The web tier and database tier run on Amazon EC2 instances in private subnets within the same Availability Zone.Which combination of steps should a Solutions Architect take to add high availability to this architecture? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480458,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An on-premises server runs a MySQL database and will be migrated to the AWS Cloud. The company require a managed solution that supports high availability and automatic failover in the event of the outage of an Availability Zone (AZ).</p><p>Which solution is the BEST fit for these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The AWS DMS service can be used to directly migrate the MySQL database to an Amazon RDS Multi-AZ deployment. The entire process can be online and is managed for you. There is no need to perform schema translation between MySQL and RDS (assuming you choose the MySQL RDS engine).</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-31-42-b6a87bc2209574bb6eabad34a39f6577.png\"></p><p><strong>CORRECT: </strong>\"Use the AWS Database Migration Service (DMS) to directly migrate the database to an Amazon RDS MySQL Multi-AZ deployment\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use the AWS Database Migration Service (DMS) to directly migrate the database to an Amazon EC2 MySQL Multi-AZ deployment\" is incorrect as there is no such thing as “multi-AZ” on Amazon EC2 with MySQL, you must use RDS.</p><p><strong>INCORRECT:</strong> \"Create a snapshot of the MySQL database server and use AWS DataSync to migrate the data Amazon S3. Launch a new Amazon RDS MySQL Multi-AZ deployment from the snapshot\" is incorrect. You cannot create a snapshot of a MySQL database server running on-premises.</p><p><strong>INCORRECT:</strong> \"Use the AWS Database Migration Service (DMS) to directly migrate the database to Amazon RDS MySQL. Use the Schema Conversion Tool (SCT) to enable conversion from MySQL to Amazon RDS\" is incorrect. There is no need to convert the schema when migrating from MySQL to Amazon RDS (MySQL engine).</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
        "answers": [
          "<p>Use the AWS Database Migration Service (DMS) to directly migrate the database to an Amazon RDS MySQL Multi-AZ deployment</p>",
          "<p>Use the AWS Database Migration Service (DMS) to directly migrate the database to an Amazon EC2 MySQL Multi-AZ deployment</p>",
          "<p>Create a snapshot of the MySQL database server and use AWS DataSync to migrate the data Amazon S3. Launch a new Amazon RDS MySQL Multi-AZ deployment from the snapshot</p>",
          "<p>Use the AWS Database Migration Service (DMS) to directly migrate the database to Amazon RDS MySQL. Use the Schema Conversion Tool (SCT) to enable conversion from MySQL to Amazon RDS</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Migration & Transfer",
      "question_plain": "An on-premises server runs a MySQL database and will be migrated to the AWS Cloud. The company require a managed solution that supports high availability and automatic failover in the event of the outage of an Availability Zone (AZ).Which solution is the BEST fit for these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480460,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>The database layer of an on-premises web application is being migrated to AWS. The database currently uses an in-memory cache. A Solutions Architect must deliver a solution that supports high availability and replication for the caching layer.</p><p>Which service should the Solutions Architect recommend?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon ElastiCache Redis is an in-memory database cache and supports high availability through replicas and multi-AZ. The table below compares ElastiCache Redis with Memcached:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-32-43-a842e78974d591a0b9d23fb5d088de78.png\"></p><p><strong>CORRECT: </strong>\"Amazon ElastiCache Redis\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon ElastiCache Memcached\" is incorrect as it does not support high availability or multi-AZ.</p><p><strong>INCORRECT:</strong> \"Amazon RDS Multi-AZ\" is incorrect. This is not an in-memory database and it not suitable for use as a caching layer.</p><p><strong>INCORRECT:</strong> \"Amazon DynamoDB\" is incorrect. DynamoDB is a non-relational database. You would not use it for a caching layer. Also, the in-memory, low-latency caching for DynamoDB is implemented using DynamoDB Accelerator (DAX).</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
        "answers": [
          "<p>Amazon ElastiCache Redis</p>",
          "<p>Amazon RDS Multi-AZ</p>",
          "<p>Amazon ElastiCache Memcached</p>",
          "<p>Amazon DynamoDB</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Database",
      "question_plain": "The database layer of an on-premises web application is being migrated to AWS. The database currently uses an in-memory cache. A Solutions Architect must deliver a solution that supports high availability and replication for the caching layer.Which service should the Solutions Architect recommend?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480462,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect has created an AWS Organization with several AWS accounts. Security policy requires that use of specific API actions are limited across all accounts. The Solutions Architect requires a method of centrally controlling these actions.</p><p>What is the SIMPLEST method of achieving the requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Service control policies (SCPs) offer central control over the maximum available permissions for all accounts in your organization allowing you to ensure your accounts stay within your organization’s access control guidelines.</p><p>In the example below, a policy in OU1 restricts all users from launching EC2 instance types other than a t2.micro:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-33-24-3942465af0c21392a215faf713e45486.png\"></p><p><strong>CORRECT: </strong>\"Create a service control policy in the root organizational unit to deny access to the services or actions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a Network ACL that limits access to the services or actions and attach it to all relevant subnets\" is incorrect. Network ACLs control network traffic - not API actions.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy in the root account and attach it to users and groups in each account\" is incorrect. This is not an efficient or centrally managed method of applying the security restrictions.</p><p><strong>INCORRECT:</strong> \"Create cross-account roles in each account to limit access to the services and actions that are allowed\" is incorrect. This is another example of a complex and inefficient method of providing access across accounts and does not restrict API actions within the account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-accounts/\">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-accounts/</a></p>",
        "answers": [
          "<p>Create a Network ACL that limits access to the services or actions and attach it to all relevant subnets</p>",
          "<p>Create an IAM policy in the root account and attach it to users and groups in each account</p>",
          "<p>Create cross-account roles in each account to limit access to the services and actions that are allowed</p>",
          "<p>Create a service control policy in the root organizational unit to deny access to the services or actions</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Management & Governance",
      "question_plain": "A Solutions Architect has created an AWS Organization with several AWS accounts. Security policy requires that use of specific API actions are limited across all accounts. The Solutions Architect requires a method of centrally controlling these actions.What is the SIMPLEST method of achieving the requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480464,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A data analytics company is building a high-performance application that requires concurrent writes to a shared block storage volume from multiple Amazon EC2 instances.</p><p>The EC2 instances are Nitro-based and reside within the same Availability Zone. The company needs a storage solution that supports simultaneous connections to facilitate data resilience and high availability.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>io2 volumes are designed for I/O-intensive workloads, particularly database workloads, that require high performance and low latency. io1 and io2 volumes support Multi-Attach, which enables you to attach a single volume to multiple EC2 instances in the same Availability Zone.</p><p><strong>CORRECT: </strong>\"Use Provisioned IOPS SSD (io2) EBS volumes with Amazon EBS Multi-Attach\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon EFS with NFSv4.1 protocol across multiple EC2 instances\" is incorrect.</p><p>Amazon Elastic File System (EFS) is a scalable file storage for use with Amazon EC2. You can use an Amazon EFS file system as a common data source for workloads and applications running on multiple instances, but it does not provide the block-level storage required for high IOPS operations.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 with S3 Transfer Acceleration to enhance speed\" is incorrect.</p><p>Amazon S3 is an object storage service. While S3 Transfer Acceleration does enhance the speed of in-transit file transfers, it is not a block storage solution, it is an object storage solution and is not suitable for this use case.</p><p><strong>INCORRECT:</strong> \"Use General Purpose SSD (gp2) EBS volumes with Amazon EBS Multi-Attach\" is incorrect.</p><p>Amazon EBS Multi-Attach only supports io1 and io2 volumes, and it is not supported on gp2 volumes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
        "answers": [
          "<p>Use Amazon EFS with NFSv4.1 protocol across multiple EC2 instances.</p>",
          "<p>Use Amazon S3 with S3 Transfer Acceleration to enhance speed.</p>",
          "<p>Use Provisioned IOPS SSD (io2) EBS volumes with Amazon EBS Multi-Attach.</p>",
          "<p>Use General Purpose SSD (gp2) EBS volumes with Amazon EBS Multi-Attach.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Compute",
      "question_plain": "A data analytics company is building a high-performance application that requires concurrent writes to a shared block storage volume from multiple Amazon EC2 instances.The EC2 instances are Nitro-based and reside within the same Availability Zone. The company needs a storage solution that supports simultaneous connections to facilitate data resilience and high availability.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480466,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company operates a critical Python-based application that analyzes incoming real-time data. The application runs every 15 minutes and takes approximately 2 minutes to complete a run. It requires 1.5 GB of memory and uses the CPU intensively during its operation. The company wants to minimize the costs associated with running this application.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>This is the most cost-effective solution. AWS Lambda is designed for running code in response to events or on a schedule, and you only pay for the compute time that you consume.</p><p>Configuring the function with 1.5GB memory would ensure the function has enough resources, and using Amazon EventBridge for scheduling would enable running the function every 15 minutes.</p><p><strong>CORRECT: </strong>\"Implement the application as an AWS Lambda function configured with 1.5 GB of memory. Use Amazon EventBridge to schedule the function to run every 15 minutes\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS App2Container (A2C) to containerize the application. Run the application as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 1 virtual CPU (vCPU) and 1.5 GB of memory\" is incorrect.</p><p>This is not the most cost-effective solution. Even though AWS App2Container (A2C) would help in containerizing the application and AWS Fargate would abstract the need to manage underlying EC2 instances, it is still an overkill for an application that runs for short durations intermittently. It would still result in paying for unused compute resources.</p><p><strong>INCORRECT:</strong> \"Use AWS App2Container (A2C) to containerize the application. Deploy the container on an Amazon EC2 instance, configure an Amazon CloudWatch alarm to stop the instance when the application is not running\" is incorrect.</p><p>AWS App2Container (A2C) is used to help containerize applications, but this does not optimize for cost because it requires running an EC2 instance continuously and stopping the instance when not in use can be complex and might not be timely, resulting in potential unnecessary costs.</p><p><strong>INCORRECT:</strong> \"Deploy the application on an Amazon EC2 instance and manually start and stop the instance in alignment with the schedule of the application run\" is incorrect.</p><p>This solution involves significant manual intervention and managing EC2 instances. While it can work, it's not an optimized way, especially in terms of cost and operation overhead. It does not take advantage of the pay-per-use model and automatic scaling provided by AWS Lambda.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
        "answers": [
          "<p>Use AWS App2Container (A2C) to containerize the application. Run the application as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 1 virtual CPU (vCPU) and 1.5 GB of memory.</p>",
          "<p>Implement the application as an AWS Lambda function configured with 1.5 GB of memory. Use Amazon EventBridge to schedule the function to run every 15 minutes.</p>",
          "<p>Use AWS App2Container (A2C) to containerize the application. Deploy the container on an Amazon EC2 instance, configure an Amazon CloudWatch alarm to stop the instance when the application is not running.</p>",
          "<p>Deploy the application on an Amazon EC2 instance and manually start and stop the instance in alignment with the schedule of the application run.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "A company operates a critical Python-based application that analyzes incoming real-time data. The application runs every 15 minutes and takes approximately 2 minutes to complete a run. It requires 1.5 GB of memory and uses the CPU intensively during its operation. The company wants to minimize the costs associated with running this application.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480468,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A company operates multiple AWS accounts under AWS Organizations. To better manage the costs, the company wants to allocate different budgets for each of these accounts. The company also wants to prevent additional resource provisioning in an AWS account if it reaches its allocated budget before the end of the budget period.</p><p>Which combination of solutions will meet these requirements? (Select THREE.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", "", ""],
        "explanation": "<p>AWS Budgets is a tool that enables you to set custom cost and usage budgets. You can set your budget amount, and AWS provides you with estimated charges and forecasted costs for your AWS usage. Configuring the budgets in the Billing and Cost Management console is a recommended step.</p><p>AWS Budgets can execute budget actions (like preventing additional resource provisioning) using an IAM role with the necessary permissions.</p><p>Configuring alerts in AWS Budgets and linking a budget action to an IAM role for automatic prevention of additional resource provisioning is a correct and efficient way to manage costs.</p><p><strong>CORRECT: </strong>\"Use AWS Budgets to establish different budgets for each AWS account. Configure the budgets in the Billing and Cost Management console\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Set up an IAM role with the necessary permissions that allow AWS Budgets to execute budget actions\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure alerts in AWS Budgets to notify the company when an account is about to reach its budget threshold. Then use a budget action that links to the IAM role to prevent additional resource provisioning\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Budgets in the AWS Management Console to set up budgets and specify the cost threshold for each AWS account\" is incorrect.</p><p>While AWS Budgets can indeed be set up in the AWS Management Console, the budgets aren't set in the context of cost thresholds for each AWS account. This option is not fully accurate.</p><p><strong>INCORRECT:</strong> \"Create an IAM user with adequate permissions to allow AWS Budgets to enforce budget actions\" is incorrect.</p><p>Although you can create an IAM user with necessary permissions, using an IAM role is generally a better practice. An IAM user is an entity that you create in AWS to represent the person or service that uses it to interact with AWS, while an IAM role is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. A role does not have long-term credentials associated with it like an IAM user does.</p><p><strong>INCORRECT:</strong> \"Set up an alert in AWS Budgets to notify the company when a particular account meets its budget threshold. Enable real-time monitoring for immediate notification\" is incorrect.</p><p>AWS Budgets doesn't allow for real-time monitoring; the data can be delayed up to 24 hours. The frequency of budget alert notifications is not customizable to the minute or hour; they are typically sent out daily, weekly, or when a certain threshold is crossed.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-controls.html\">https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-controls.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cost-management/\">https://digitalcloud.training/aws-cost-management/</a></p>",
        "answers": [
          "<p>Use AWS Budgets to establish different budgets for each AWS account. Configure the budgets in the Billing and Cost Management console.</p>",
          "<p>Use AWS Budgets in the AWS Management Console to set up budgets and specify the cost threshold for each AWS account.</p>",
          "<p>Set up an IAM role with the necessary permissions that allow AWS Budgets to execute budget actions.</p>",
          "<p>Create an IAM user with adequate permissions to allow AWS Budgets to enforce budget actions.</p>",
          "<p>Configure alerts in AWS Budgets to notify the company when an account is about to reach its budget threshold. Then use a budget action that links to the IAM role to prevent additional resource provisioning.</p>",
          "<p>Set up an alert in AWS Budgets to notify the company when a particular account meets its budget threshold. Enable real-time monitoring for immediate notification.</p>"
        ]
      },
      "correct_response": ["a", "c", "e"],
      "section": "AWS Management & Governance",
      "question_plain": "A company operates multiple AWS accounts under AWS Organizations. To better manage the costs, the company wants to allocate different budgets for each of these accounts. The company also wants to prevent additional resource provisioning in an AWS account if it reaches its allocated budget before the end of the budget period.Which combination of solutions will meet these requirements? (Select THREE.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480470,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company is developing a web-based application that will be used for real-time chat functionality. The application should use WebSocket APIs to maintain a persistent connection with the client. The backend services of the application, hosted in containers within private subnets of a VPC, need to be accessed securely.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The requirement is for a real-time chat application, which makes the use of WebSocket APIs more suitable. Hosting the application in Amazon EKS within a private subnet allows secure and scalable management of the application. Creating a VPC link provides secure, private connectivity between API Gateway and the Amazon EKS service hosted inside the VPC.</p><p><strong>CORRECT: </strong>\"Develop a WebSocket API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Establish a private VPC link for the API Gateway to securely access the Amazon EKS cluster\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Establish a private VPC link for the API Gateway to securely access the Amazon EKS cluster\" is incorrect.</p><p>This solution does provide the secure hosting environment and private connectivity between API Gateway and the Amazon EKS cluster, but REST APIs are not suitable for real-time applications like a chat service. This is because REST APIs use request-response model which doesn't provide the continuous connection needed for real-time communication.</p><p><strong>INCORRECT:</strong> \"Develop a WebSocket API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster\" is incorrect.</p><p>This option, while correctly suggesting the use of WebSocket APIs and Amazon EKS, proposes the use of a security group for connectivity. However, security groups act as a firewall for associated Amazon EC2 instances, controlling both inbound and outbound traffic at the instance level, while access to services within VPCs is more securely managed through VPC links.</p><p><strong>INCORRECT:</strong> \"Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster\" is incorrect.</p><p>REST APIs are not suitable for a real-time chat application. Also, managing access via a security group is not the most secure method for accessing services hosted within private subnets in a VPC.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/best-practices-api-gateway-private-apis-integration/websocket-api.html\">https://docs.aws.amazon.com/whitepapers/latest/best-practices-api-gateway-private-apis-integration/websocket-api.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
        "answers": [
          "<p>Develop a WebSocket API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Establish a private VPC link for the API Gateway to securely access the Amazon EKS cluster.</p>",
          "<p>Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Establish a private VPC link for the API Gateway to securely access the Amazon EKS cluster.</p>",
          "<p>Develop a WebSocket API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster.</p>",
          "<p>Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A company is developing a web-based application that will be used for real-time chat functionality. The application should use WebSocket APIs to maintain a persistent connection with the client. The backend services of the application, hosted in containers within private subnets of a VPC, need to be accessed securely.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480472,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A media company hosts several terabytes of multimedia content across multiple AWS accounts. The company uses AWS Lake Formation to manage its data lake. The company's marketing team needs to securely access and analyze selective data from various accounts for targeted advertisement campaigns.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>With Lake Formation tag-based access control, you can manage permissions using tags and grant cross-account permissions, which would meet the requirements with the least operational overhead.</p><p><strong>CORRECT: </strong>\"Utilize Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the marketing team accounts\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Replicate the required data to a shared account. Create an IAM access role in that account. Grant access by defining a permission policy that includes users from the marketing team accounts as trusted entities\" is incorrect.</p><p>This solution involves the unnecessary replication of data, leading to increased storage costs and operational overhead.</p><p><strong>INCORRECT:</strong> \"Use the Lake Formation permissions Grant command in each account where the data is stored to permit the required marketing team users to access the data\" is incorrect.</p><p>The Grant command would need to be manually executed in each account where data is stored, which could lead to increased operational overhead, particularly if the data is spread across many accounts.</p><p><strong>INCORRECT:</strong> \"Use AWS DataSync to synchronize the necessary data to the marketing team accounts\" is incorrect.</p><p>AWS DataSync is designed for online data transfer, not for granting access permissions to data already stored in AWS, so this would not meet the requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/tag-based-access-control.html\">https://docs.aws.amazon.com/lake-formation/latest/dg/tag-based-access-control.html</a></p>",
        "answers": [
          "<p>Replicate the required data to a shared account. Create an IAM access role in that account. Grant access by defining a permission policy that includes users from the marketing team accounts as trusted entities.</p>",
          "<p>Use the Lake Formation permissions Grant command in each account where the data is stored to permit the required marketing team users to access the data.</p>",
          "<p>Use AWS DataSync to synchronize the necessary data to the marketing team accounts.</p>",
          "<p>Utilize Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the marketing team accounts.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Storage",
      "question_plain": "A media company hosts several terabytes of multimedia content across multiple AWS accounts. The company uses AWS Lake Formation to manage its data lake. The company's marketing team needs to securely access and analyze selective data from various accounts for targeted advertisement campaigns.Which solution will meet these requirements with the LEAST operational overhead?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480474,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A financial institution wants to use machine learning (ML) algorithms to detect potential fraudulent transactions. They need to create ML models based on their vast financial transaction data and integrate these models into their business intelligence system for real-time decision-making. The solution should require minimal operational overhead.</p><p>Which solution will best meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. It can directly connect with data sources and has built-in algorithms to ease the ML process.</p><p>Amazon QuickSight is a business intelligence tool that can be used to create dashboards for data visualization. This combination perfectly suits the requirement.</p><p><strong>CORRECT: </strong>\"Use Amazon SageMaker to build, train, and deploy ML models, and use Amazon QuickSight for data visualization\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to perform ETL jobs on the transaction data and use Amazon Forecast for predictive analytics\" is incorrect.</p><p>AWS Glue is primarily used for ETL jobs - cleaning, preparing, and moving data. Amazon Forecast is a fully managed service for time-series forecasting, which might not be a complete solution for detecting fraudulent transactions.</p><p><strong>INCORRECT:</strong> \"Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models and use AWS Athena for data visualization\" is incorrect.</p><p>AWS Marketplace ML AMIs can be used to create and train models, but this will require manual operational effort in terms of setting up and managing the instances. Athena is a query service and does not provide data visualization capabilities that a business intelligence tool like QuickSight provides.</p><p><strong>INCORRECT:</strong> \"Use Amazon Comprehend for analyzing the transaction data and Amazon Elasticsearch for visualization\" is incorrect.</p><p>Amazon Comprehend is primarily used for natural language processing (NLP), which isn't suited for detecting fraudulent transactions. Elasticsearch is a search and analytics engine and might not be the best tool for the use case described here.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/sagemaker/\">https://aws.amazon.com/sagemaker/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-machine-learning-services/\">https://digitalcloud.training/aws-machine-learning-services/</a></p>",
        "answers": [
          "<p>Use Amazon SageMaker to build, train, and deploy ML models, and use Amazon QuickSight for data visualization.</p>",
          "<p>1. Use AWS Glue to perform ETL jobs on the transaction data and use Amazon Forecast for predictive analytics.</p>",
          "<p>Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models and use AWS Athena for data visualization.</p>",
          "<p>Use Amazon Comprehend for analyzing the transaction data and Amazon Elasticsearch for visualization.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Machine Learning",
      "question_plain": "A financial institution wants to use machine learning (ML) algorithms to detect potential fraudulent transactions. They need to create ML models based on their vast financial transaction data and integrate these models into their business intelligence system for real-time decision-making. The solution should require minimal operational overhead.Which solution will best meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480476,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A corporation has a web-based multiplayer gaming service that operates using both TCP and UDP protocols. Amazon Route 53 is currently employed to direct application traffic to a set of Network Load Balancers (NLBs) in various AWS Regions. To prepare for an increase in user activity, the company must enhance application performance and reduce latency.</p><p>Which approach will best meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Global Accelerator is designed to improve the availability and performance of your applications for local and global users. It directs traffic to optimal endpoints over the AWS global network, thus enhancing the performance of your TCP and UDP traffic by routing packets through the AWS global network infrastructure, reducing jitter, and improving overall game performance.</p><p><strong>CORRECT: </strong>\"Implement AWS Global Accelerator ahead of the NLBs and align the Global Accelerator endpoint to use the appropriate listener ports\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Incorporate Amazon CloudFront in front of the NLBs and extend the duration of the Cache-Control max-age directive\" is incorrect.</p><p>Amazon CloudFront is a content delivery network (CDN) that speeds up the delivery of your static and dynamic web content. While it could potentially help with application performance, it doesn't directly improve TCP/UDP performance, which is the specific requirement in this case.</p><p><strong>INCORRECT:</strong> \"Substitute the NLBs with Application Load Balancers (ALBs) and set Route 53 to utilize latency-based routing\" is incorrect.</p><p>Application Load Balancers (ALBs) are layer 7 load balancers and they do not support the handling of raw TCP and UDP traffic, which is a requirement for the gaming application in the question. NLBs, on the other hand, are suitable for extreme performance needs and for TCP/UDP traffic.</p><p><strong>INCORRECT:</strong> \"Insert an Amazon API Gateway endpoint behind the NLBs, enable API caching, and customize method caching across different stages\" is incorrect.</p><p>While the API Gateway would add more control and security to the application, the caching feature is not necessarily beneficial for this real-time gaming scenario where the content is likely to change frequently and unpredictably.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-global-accelerator/\">https://digitalcloud.training/aws-global-accelerator/</a></p>",
        "answers": [
          "<p>Incorporate Amazon CloudFront in front of the NLBs and extend the duration of the Cache-Control max-age directive.</p>",
          "<p>Substitute the NLBs with Application Load Balancers (ALBs) and set Route 53 to utilize latency-based routing.</p>",
          "<p>Implement AWS Global Accelerator ahead of the NLBs and align the Global Accelerator endpoint to use the appropriate listener ports.</p>",
          "<p>Insert an Amazon API Gateway endpoint behind the NLBs, enable API caching, and customize method caching across different stages.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A corporation has a web-based multiplayer gaming service that operates using both TCP and UDP protocols. Amazon Route 53 is currently employed to direct application traffic to a set of Network Load Balancers (NLBs) in various AWS Regions. To prepare for an increase in user activity, the company must enhance application performance and reduce latency.Which approach will best meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480478,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A financial services company is migrating its sensitive customer data and applications to AWS. They want to ensure that the data is securely stored and managed while reducing the overall maintenance and operational overhead associated with managing databases.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon RDS makes it easy to go from project conception to deployment by managing time-consuming database administration tasks including backups, software patching, monitoring, scaling, and replication.</p><p>Amazon RDS supports encryption at rest, which ensures the security of sensitive data and meets regulatory compliance requirements. AWS Key Management Service (AWS KMS) is integrated with Amazon RDS to make it easier to create, control, and manage keys for encryption.</p><p><strong>CORRECT: </strong>\"Migrate the data and applications to Amazon RDS instances. Enable encryption at rest using AWS Key Management Service (AWS KMS)\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the applications and data to Amazon EC2 instances. Utilize the AWS Key Management Service (AWS KMS) customer managed keys for encryption\" is incorrect.</p><p>While this solution offers data encryption, it does not meet the requirement to reduce operational overhead. Managing databases on EC2 instances requires additional administrative tasks, such as managing backups and applying software patches, which Amazon RDS handles automatically.</p><p><strong>INCORRECT:</strong> \"Store the data in Amazon S3. Utilize Amazon Macie for ongoing data security and threat detection\" is incorrect.</p><p>Amazon S3 and Macie are suitable for data storage and security analysis, respectively. However, Amazon S3 is not designed to serve as a transactional database for applications, which is a key requirement in this scenario.</p><p><strong>INCORRECT:</strong> \"Migrate the data to Amazon RDS instances. Enable Amazon GuardDuty for data protection and threat detection\" is incorrect.</p><p>While Amazon RDS is a correct choice for database management and Amazon GuardDuty offers threat detection, GuardDuty is not specifically designed for data protection within databases. It's a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
        "answers": [
          "<p>Migrate the applications and data to Amazon EC2 instances. Utilize the AWS Key Management Service (AWS KMS) customer managed keys for encryption.</p>",
          "<p>Migrate the data and applications to Amazon RDS instances. Enable encryption at rest using AWS Key Management Service (AWS KMS).</p>",
          "<p>Store the data in Amazon S3. Utilize Amazon Macie for ongoing data security and threat detection.</p>",
          "<p>Migrate the data to Amazon RDS instances. Enable Amazon GuardDuty for data protection and threat detection.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Database",
      "question_plain": "A financial services company is migrating its sensitive customer data and applications to AWS. They want to ensure that the data is securely stored and managed while reducing the overall maintenance and operational overhead associated with managing databases.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480480,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A healthcare company is migrating its patient record system to AWS. The company receives thousands of encrypted patient data files every day through FTP. An on-premises server processes the data files twice a day. However, the processing job takes hours to finish.</p><p>The company wants the AWS solution to process incoming data files as soon as they arrive with minimal changes to the FTP clients that send the files. The solution must delete the incoming data files after the files have been processed successfully. Processing for each file needs to take around 10 minutes.</p><p>Which solution will meet these requirements in the MOST operationally efficient way?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Transfer Family provides fully managed support for file transfers directly into and out of Amazon S3 using SFTP. Storing incoming files in S3 Standard offers high durability, availability, and performance object storage for frequently accessed data.</p><p>AWS Lambda can respond immediately to S3 events, which allows processing of files as soon as they arrive. Lambda can also delete the files after processing. This meets all requirements and is operationally efficient, as it requires minimal management and has low costs.</p><p><strong>CORRECT: </strong>\"Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Standard. Create an AWS Lambda function to process the files and to delete the files after they are processed. Use an S3 event notification to invoke the Lambda function when the files arrive\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Glacier. Configure an Amazon EC2 instance to process the files. Use Amazon EventBridge rules to invoke the EC2 instance to process the files twice a day from S3 Glacier. Delete the objects after the job has processed the objects\" is incorrect.</p><p>This option involves using Amazon S3 Glacier, which is primarily used for long-term archival storage. Accessing data for processing could take longer and be more expensive than using S3 Standard. In addition, EC2 instances need to be managed and are less efficient for this scenario compared to AWS Lambda.</p><p><strong>INCORRECT:</strong> \"Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Standard. Use Amazon EC2 instances managed by an Auto Scaling group to process the files. Set an S3 event notification to trigger an AWS Lambda function that launches the EC2 instances when the files arrive. Delete the files after they are processed\" is incorrect.</p><p>While this solution will work, it is less efficient operationally because managing EC2 instances and an Auto Scaling group is more complex and likely more expensive than simply using AWS Lambda for processing.</p><p><strong>INCORRECT:</strong> \"Use an Amazon EC2 instance that runs an SFTP server to store incoming files in Amazon S3 Standard. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to invoke the job to process the files twice a day. Delete the files after the job has processed the files\" is incorrect.</p><p>This option does not meet the requirement of processing incoming data files as soon as they arrive, as EventBridge rules would invoke the job only twice a day. It also involves managing an EC2 instance, which is less operationally efficient than the AWS Transfer Family and AWS Lambda option.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/aws-transfer-family/\">https://aws.amazon.com/aws-transfer-family/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
        "answers": [
          "<p>Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Glacier. Configure an Amazon EC2 instance to process the files. Use Amazon EventBridge rules to invoke the EC2 instance to process the files twice a day from S3 Glacier. Delete the objects after the job has processed the objects.</p>",
          "<p>Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Standard. Use Amazon EC2 instances managed by an Auto Scaling group to process the files. Set an S3 event notification to trigger an AWS Lambda function that launches the EC2 instances when the files arrive. Delete the files after they are processed.</p>",
          "<p>Use an Amazon EC2 instance that runs an SFTP server to store incoming files in Amazon S3 Standard. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to invoke the job to process the files twice a day. Delete the files after the job has processed the files.</p>",
          "<p>Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Standard. Create an AWS Lambda function to process the files and to delete the files after they are processed. Use an S3 event notification to invoke the Lambda function when the files arrive.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Migration & Transfer",
      "question_plain": "A healthcare company is migrating its patient record system to AWS. The company receives thousands of encrypted patient data files every day through FTP. An on-premises server processes the data files twice a day. However, the processing job takes hours to finish.The company wants the AWS solution to process incoming data files as soon as they arrive with minimal changes to the FTP clients that send the files. The solution must delete the incoming data files after the files have been processed successfully. Processing for each file needs to take around 10 minutes.Which solution will meet these requirements in the MOST operationally efficient way?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480482,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A multinational organization has a distributed application that runs on Amazon EC2 instances, which are behind an Application Load Balancer in an Auto Scaling group. The application utilizes a MySQL database hosted on Amazon Aurora. The database cluster spans across multiple Availability Zones in a single region.</p><p>The organization plans to launch its services in a new geographical area and wants to ensure maximum availability with minimal service interruption.</p><p>Which strategy should the organization adopt?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>This solution involves creating an application layer in the new region and using Amazon Aurora Global Database, which supports replicating your databases across multiple regions with minimal impact on performance.</p><p>This configuration can enhance disaster recovery capabilities and can reduce the impact of planned maintenance. Amazon Route 53 health checks with a failover routing policy can automatically route traffic to the new region in the event of a failure in the primary region, thereby ensuring high availability.</p><p>With an Aurora global database, there are two different approaches to failover depending on the scenario. You can use manual unplanned failover (detach and promote) or managed planned failover.</p><p><strong>CORRECT: </strong>\"Establish the application layer in the new region. Use Amazon Aurora Global Database for deploying the database in the primary and new regions. Apply Amazon Route 53 health checks with a failover routing policy to the new region. Perform a manual failover as required\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Replicate the application layer in the new region. Implement an Aurora MySQL Read Replica in the new region using Route 53 health checks and a failover routing policy. In case of primary failure, promote the Read Replica to primary\" is incorrect.</p><p>This solution involves creating a Read Replica in the new region, which would indeed allow for the promotion of the Read Replica to a primary instance if necessary. However, this process isn't instantaneous and could lead to service interruption, which is not what the question asked for. Aurora Global Database provides a lower RTO/RPO.</p><p><strong>INCORRECT:</strong> \"Create a similar application layer in the new region. Establish a new Aurora MySQL database in this region. Use AWS Database Migration Service (AWS DMS) for ongoing replication from the primary database to the new region. Implement Amazon Route 53 health checks with a failover routing policy to the new region\" is incorrect.</p><p>AWS Database Migration Service (AWS DMS) is primarily used for migrating databases to AWS from on-premises environments or for replicating databases for data warehousing and other use cases. It isn't as suitable for ongoing high-availability or failover scenarios as Amazon Aurora Global Database, which is specifically designed for these situations.</p><p><strong>INCORRECT:</strong> \"Expand the existing Auto Scaling group into the new Region. Utilize Amazon Aurora Global Database to extend the database across the primary and new regions. Implement Amazon Route 53 health checks with a failover routing policy directed towards the new region\" is incorrect.</p><p>It is not possible to expand an Auto Scaling group across multiple Regions. ASGs operate within a Region only.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html#aurora-global-database-failover\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html#aurora-global-database-failover</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
        "answers": [
          "<p>Expand the existing Auto Scaling group into the new Region. Utilize Amazon Aurora Global Database to extend the database across the primary and new regions. Implement Amazon Route 53 health checks with a failover routing policy directed towards the new region.</p>",
          "<p>Replicate the application layer in the new region. Implement an Aurora MySQL Read Replica in the new region using Route 53 health checks and a failover routing policy. In case of primary failure, promote the Read Replica to primary.</p>",
          "<p>Create a similar application layer in the new region. Establish a new Aurora MySQL database in this region. Use AWS Database Migration Service (AWS DMS) for ongoing replication from the primary database to the new region. Implement Amazon Route 53 health checks with a failover routing policy to the new region.</p>",
          "<p>Establish the application layer in the new region. Use Amazon Aurora Global Database for deploying the database in the primary and new regions. Apply Amazon Route 53 health checks with a failover routing policy to the new region. Promote the secondary to primary as needed.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Compute",
      "question_plain": "A multinational organization has a distributed application that runs on Amazon EC2 instances, which are behind an Application Load Balancer in an Auto Scaling group. The application utilizes a MySQL database hosted on Amazon Aurora. The database cluster spans across multiple Availability Zones in a single region.The organization plans to launch its services in a new geographical area and wants to ensure maximum availability with minimal service interruption.Which strategy should the organization adopt?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480484,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A cloud architect is assessing the resilience of a web application deployed on AWS. It was observed that the application experienced a downtime of about 3 minutes when a scheduled failover was performed on the application's Amazon RDS MySQL database as part of a scaling operation.</p><p>The organization wants to mitigate such downtime in future scaling exercises while minimizing operational overhead.</p><p>Which solution will be the MOST effective in achieving this?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon RDS Proxy is a fully managed, highly available database proxy for Amazon RDS that makes applications more scalable, more resilient to database failures, and more secure.</p><p>During a failover, RDS Proxy automatically connects to a standby database instance while preserving connections from your application and reducing failover times for RDS and Aurora multi-AZ databases. So, there is minimal downtime for the application.</p><p><strong>CORRECT: </strong>\"Configure an Amazon RDS Proxy for the database and modify the application to connect to the proxy endpoint\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement more RDS MySQL read replicas in the cluster to manage the load during the failover\" is incorrect.</p><p>Adding more read replicas to the cluster does not decrease the downtime during a failover. It only improves the database's ability to handle read-heavy workloads. Read replicas do not contribute to a faster failover process.</p><p><strong>INCORRECT:</strong> \"Establish a secondary RDS MySQL cluster within the same AWS Region. During any future failover, modify the application to connect to the secondary cluster's writer endpoint\" is incorrect.</p><p>This approach is operationally heavy as it involves managing two separate RDS clusters and manually updating the application's database endpoint during a failover. Moreover, it does not necessarily reduce the downtime during a failover as there might be data inconsistency issues between the primary and secondary clusters, depending on the replication latency.</p><p><strong>INCORRECT:</strong> \"Implement an Amazon ElastiCache for Redis cluster to manage the load during the failover\" is incorrect.</p><p>ElastiCache is an in-memory cache and not a relational database service. It is typically used to cache frequently accessed data to reduce latency and improve application performance, not for managing failovers.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
        "answers": [
          "<p>Implement more RDS MySQL read replicas in the cluster to manage the load during the failover.</p>",
          "<p>Establish a secondary RDS MySQL cluster within the same AWS Region. During any future failover, modify the application to connect to the secondary cluster's writer endpoint.</p>",
          "<p>Implement an Amazon ElastiCache for Redis cluster to manage the load during the failover.</p>",
          "<p>Configure an Amazon RDS Proxy for the database and modify the application to connect to the proxy endpoint.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Compute",
      "question_plain": "A cloud architect is assessing the resilience of a web application deployed on AWS. It was observed that the application experienced a downtime of about 3 minutes when a scheduled failover was performed on the application's Amazon RDS MySQL database as part of a scaling operation.The organization wants to mitigate such downtime in future scaling exercises while minimizing operational overhead.Which solution will be the MOST effective in achieving this?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480486,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company is looking for ways to incorporate its current AWS usage expenditure into its operational expense tracking dashboard. A solutions architect has been tasked with proposing a method that enables the company to fetch its current year's cost data and project the costs for the forthcoming 12 months programmatically.</p><p>Which approach would fulfill these needs with the MINIMUM operational burden?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Cost Explorer API provides programmatic access to AWS cost and usage information. The user can query for aggregated data such as total monthly costs or total daily usage with this API.</p><p>Also, the Cost Explorer API supports pagination for managing larger data sets, making it efficient for larger queries.</p><p><strong>CORRECT: </strong>\"Leverage the AWS Cost Explorer API to retrieve usage cost-related data, using pagination for larger data sets\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Make use of downloadable AWS Cost Explorer report files in the .csv format to access usage cost-related data\" is incorrect.</p><p>While AWS Cost Explorer does allow you to download .csv reports of your cost data, this method would not be programmatically accessible and would involve manual steps to download and process the data.</p><p><strong>INCORRECT:</strong> \"Set up AWS Budgets actions to transmit usage cost data to the corporation via FTP\" is incorrect.</p><p>AWS Budgets actions allow you to set custom cost and usage budgets that trigger actions (such as turning off EC2 instances) when the budget thresholds you set are breached. However, AWS Budgets does not support transmitting data via FTP.</p><p><strong>INCORRECT:</strong> \"Generate AWS Budgets reports on usage cost data and dispatch the data to the corporation through SMTP\" is incorrect.</p><p>AWS Budgets does not support the dispatching of data through SMTP. AWS Budgets is primarily a tool for setting up alerts on your AWS costs or usage to control your costs, rather than a tool for exporting or transmitting cost data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/aws-cost-management/latest/APIReference/API_Operations_AWS_Cost_Explorer_Service.html\">https://docs.aws.amazon.com/aws-cost-management/latest/APIReference/API_Operations_AWS_Cost_Explorer_Service.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cost-management/\">https://digitalcloud.training/aws-cost-management/</a></p>",
        "answers": [
          "<p>Leverage the AWS Cost Explorer API to retrieve usage cost-related data, using pagination for larger data sets.</p>",
          "<p>Make use of downloadable AWS Cost Explorer report files in the .csv format to access usage cost-related data.</p>",
          "<p>Set up AWS Budgets actions to transmit usage cost data to the corporation via FTP.</p>",
          "<p>Generate AWS Budgets reports on usage cost data and dispatch the data to the corporation through SMTP.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Management & Governance",
      "question_plain": "A company is looking for ways to incorporate its current AWS usage expenditure into its operational expense tracking dashboard. A solutions architect has been tasked with proposing a method that enables the company to fetch its current year's cost data and project the costs for the forthcoming 12 months programmatically.Which approach would fulfill these needs with the MINIMUM operational burden?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480488,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company is in the process of improving its security posture and wants to analyze and rectify a high volume of failed login attempts and unauthorized activities being logged in AWS CloudTrail.</p><p>What is the most efficient solution to help the company identify these security events with the LEAST amount of operational effort?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon Athena can directly query data from S3 (where CloudTrail logs are stored) using standard SQL, making it a powerful and efficient tool for analyzing these logs. You don't need to manage any infrastructure or write custom scripts, and you can quickly write and run queries to identify the required security events.</p><p><strong>CORRECT: </strong>\"Use Amazon Athena to directly query CloudTrail logs for failed logins and unauthorized activities\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Leverage AWS Lambda to trigger on CloudTrail log updates and use a custom script to scan for failed logins and unauthorized actions\" is incorrect.</p><p>While Lambda functions can be triggered based on CloudTrail log updates and could theoretically be used to scan for security events, this would require substantial setup and ongoing maintenance of the script. It's not the most efficient choice.</p><p><strong>INCORRECT:</strong> \"Utilize AWS Data Pipeline to regularly extract CloudTrail logs and use a custom script to identify the required security events\" is incorrect.</p><p>This solution could work, but the operational overhead of managing the extraction process and maintaining a custom script for analysis is not minimal.</p><p><strong>INCORRECT:</strong> \"Implement Amazon Elasticsearch Service with Kibana to visualize the CloudTrail logs and manually search for these events\" is incorrect.</p><p>While Elasticsearch and Kibana provide powerful search and visualization capabilities, respectively, they require a fair amount of setup and management. This option would provide more in-depth analysis and real-time monitoring, but it wouldn't be the most efficient way to simply identify the security events mentioned.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html\">https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
        "answers": [
          "<p>Leverage AWS Lambda to trigger on CloudTrail log updates and use a custom script to scan for failed logins and unauthorized actions.</p>",
          "<p>Utilize AWS Data Pipeline to regularly extract CloudTrail logs and use a custom script to identify the required security events.</p>",
          "<p>Use Amazon Athena to directly query CloudTrail logs for failed logins and unauthorized activities.</p>",
          "<p>Implement Amazon Elasticsearch Service with Kibana to visualize the CloudTrail logs and manually search for these events.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Analytics",
      "question_plain": "A company is in the process of improving its security posture and wants to analyze and rectify a high volume of failed login attempts and unauthorized activities being logged in AWS CloudTrail.What is the most efficient solution to help the company identify these security events with the LEAST amount of operational effort?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480490,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An e-commerce company operates a serverless web application that must interact with numerous Amazon DynamoDB tables to fulfill user requests. It is critical that the application's performance remains consistent and unaffected while interacting with these tables.</p><p>Which method provides the MOST operationally efficient way to fulfill these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS AppSync simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one or more data sources. AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need, including from multiple DynamoDB tables.</p><p>AWS AppSync is designed for real-time and offline data access which makes it an ideal solution for this scenario.</p><p><strong>CORRECT: </strong>\"AWS AppSync with multiple data sources and resolvers\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS Lambda with Step Functions\" is incorrect.</p><p>AWS Step Functions make it easy to coordinate the components of distributed applications and microservices using visual workflows. However, while you could theoretically build a flow to retrieve data from multiple tables, it's not the most efficient solution as it introduces additional complexity and potential latency.</p><p><strong>INCORRECT:</strong> \"Amazon S3 with Lambda triggers\" is incorrect.</p><p>While you can use AWS Lambda to execute code in response to triggers like changes to data in an Amazon S3 bucket, this doesn't directly allow the application to retrieve data from multiple DynamoDB tables. This approach would also involve unnecessary data transfers and added latency.</p><p><strong>INCORRECT:</strong> \"AWS Glue with a DynamoDB connector\" is incorrect.</p><p>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load your data for analytics. However, AWS Glue isn't meant for real-time data retrieval in an application. Using it for real-time data retrieval would likely be overcomplicated and inefficient.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/appsync/product-details/\">https://aws.amazon.com/appsync/product-details/</a></p>",
        "answers": [
          "<p>AWS Lambda with Step Functions.</p>",
          "<p>Amazon S3 with Lambda triggers.</p>",
          "<p>AWS Glue with a DynamoDB connector.</p>",
          "<p>AWS AppSync with multiple data sources and resolvers.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Compute",
      "question_plain": "An e-commerce company operates a serverless web application that must interact with numerous Amazon DynamoDB tables to fulfill user requests. It is critical that the application's performance remains consistent and unaffected while interacting with these tables.Which method provides the MOST operationally efficient way to fulfill these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480492,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A software development company is deploying a microservices-based application on Amazon Elastic Kubernetes Service (Amazon EKS). The application's traffic fluctuates significantly throughout the day and the company wants to ensure that the EKS cluster scales up and down according to these traffic patterns.</p><p>Which combination of steps would satisfy these requirements with MINIMAL operational overhead? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>The Metrics Server collects resource metrics like CPU and memory usage from each node and its pods and provides these metrics to the Kubernetes API server for use by the Horizontal Pod Autoscaler, which automatically scales the number of pods in a deployment, replication controller, replica set, or stateful set based on observed CPU utilization.</p><p>The Kubernetes Cluster Autoscaler automatically adjusts the size of the Kubernetes cluster when there are pods that failed to run in the cluster due to insufficient resources or when there are nodes in the cluster that have been underutilized for an extended period and their pods can be placed on other existing nodes.</p><p><strong>CORRECT: </strong>\"Utilize the Kubernetes Metrics Server to enable horizontal pod autoscaling based on resource utilization\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Employ the Kubernetes Cluster Autoscaler for dynamically managing the quantity of nodes in the EKS cluster\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement the Kubernetes Vertical Pod Autoscaler to adjust the CPU and memory allocation for the pods\" is incorrect.</p><p>The Vertical Pod Autoscaler adjusts the resources of the pods and not the number of pods or nodes, which won't directly help with scaling according to traffic patterns.</p><p><strong>INCORRECT:</strong> \"Integrate Amazon SQS and connect it to Amazon EKS for workload management\" is incorrect.</p><p>Amazon SQS is a message queuing service, and while it can be used to manage workloads by decoupling microservices, it doesn't directly help with autoscaling an EKS cluster based on traffic patterns.</p><p><strong>INCORRECT:</strong> \"Leverage AWS X-Ray to track and analyze the application's network activity\" is incorrect.</p><p>AWS X-Ray provides insights into the behavior of your applications, but it doesn't directly help with autoscaling an EKS cluster.</p><p><strong>References:</strong></p><p><a href=\"https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-server\">https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-server</a></p><p><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html\">https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
        "answers": [
          "<p>Implement the Kubernetes Vertical Pod Autoscaler to adjust the CPU and memory allocation for the pods.</p>",
          "<p>Utilize the Kubernetes Metrics Server to enable horizontal pod autoscaling based on resource utilization.</p>",
          "<p>Employ the Kubernetes Cluster Autoscaler for dynamically managing the quantity of nodes in the EKS cluster.</p>",
          "<p>Integrate Amazon SQS and connect it to Amazon EKS for workload management.</p>",
          "<p>Leverage AWS X-Ray to track and analyze the application's network activity.</p>"
        ]
      },
      "correct_response": ["b", "c"],
      "section": "AWS Compute",
      "question_plain": "A software development company is deploying a microservices-based application on Amazon Elastic Kubernetes Service (Amazon EKS). The application's traffic fluctuates significantly throughout the day and the company wants to ensure that the EKS cluster scales up and down according to these traffic patterns.Which combination of steps would satisfy these requirements with MINIMAL operational overhead? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480494,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An international software firm provides its clients with custom solutions and tools designed for efficient data collection and analysis on AWS. The firm intends to centrally manage and distribute a standard set of solutions and tools for its clients' self-service needs.</p><p>Which solution would best satisfy these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Service Catalog enables organizations to create and manage catalogs of IT services that are approved for use on AWS. It allows centrally managed service portfolios, which clients can use on a self-service basis.</p><p>AWS Service Catalog provides a single location where organizations can centrally manage catalogs of IT services, which simplifies the organizational process and helps ensure compliance.</p><p><strong>CORRECT: </strong>\"Create AWS Service Catalog portfolios for the clients\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create AWS CloudFormation stacks for the clients\" is incorrect.</p><p>While AWS CloudFormation is a powerful service for infrastructure as code (IaC), it doesn't provide a straightforward way for clients to discover and use shared tools or solutions for self-service needs. It lacks the management features and access control mechanisms necessary for this scenario.</p><p><strong>INCORRECT:</strong> \"Create AWS Systems Manager documents for the clients\" is incorrect.</p><p>AWS Systems Manager documents define the actions that Systems Manager performs on your managed instances. Although Systems Manager allows the central management of resources and applications, it doesn't provide an effective means for clients to self-discover and use shared tools or solutions.</p><p><strong>INCORRECT:</strong> \"Create AWS Config rules for the clients\" is incorrect.</p><p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It isn't designed to centrally manage and distribute software tools or solutions.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/servicecatalog/\">https://aws.amazon.com/servicecatalog/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-service-catalog/\">https://digitalcloud.training/aws-service-catalog/</a></p>",
        "answers": [
          "<p>Create AWS CloudFormation stacks for the clients.</p>",
          "<p>Create AWS Service Catalog portfolios for the clients.</p>",
          "<p>Create AWS Systems Manager documents for the clients.</p>",
          "<p>Create AWS Config rules for the clients.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Management & Governance",
      "question_plain": "An international software firm provides its clients with custom solutions and tools designed for efficient data collection and analysis on AWS. The firm intends to centrally manage and distribute a standard set of solutions and tools for its clients' self-service needs.Which solution would best satisfy these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480496,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A digital media company uses an Amazon RDS MySQL instance for its content management system. Recently, the company has observed that their RDS instance is nearing its storage capacity due to the constant influx of new data. The company wants to ensure there's always sufficient storage without any operational interruption or manual intervention.</p><p>Which solution should the company use to address this situation with the LEAST operational overhead?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon RDS's automatic storage scaling allows the database to automatically increase its storage capacity when the available storage is low. This feature helps to prevent out-of-storage situations and requires no operational overhead.</p><p><strong>CORRECT: </strong>\"Enable automatic storage scaling for the MySQL instance\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the database to a larger Amazon RDS MySQL instance\" is incorrect.</p><p>While this would provide more storage, it does not address the issue of potential future storage shortages and requires significant operational effort for the migration.</p><p><strong>INCORRECT:</strong> \"Implement a lifecycle policy to delete older data from the MySQL instance\" is incorrect.</p><p>While this might help free up some storage, it might not be suitable if all data is essential for business operations. Also, this does not provide a long-term solution if data growth continues.</p><p><strong>INCORRECT:</strong> \"Utilize Amazon ElastiCache to offload some read traffic and reduce database load\" is incorrect.</p><p>While ElastiCache can help to improve the database's read efficiency, it doesn't directly address the disk space concern for the RDS instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
        "answers": [
          "<p>Enable automatic storage scaling for the MySQL instance.</p>",
          "<p>Migrate the database to a larger Amazon RDS MySQL instance.</p>",
          "<p>Implement a lifecycle policy to delete older data from the MySQL instance.</p>",
          "<p>Utilize Amazon ElastiCache to offload some read traffic and reduce database load.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Database",
      "question_plain": "A digital media company uses an Amazon RDS MySQL instance for its content management system. Recently, the company has observed that their RDS instance is nearing its storage capacity due to the constant influx of new data. The company wants to ensure there's always sufficient storage without any operational interruption or manual intervention.Which solution should the company use to address this situation with the LEAST operational overhead?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80480498,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A telecommunication company has an API that allows users to manage their mobile plans and services. The API experiences significant traffic spikes during specific times such as end of the month and special offer periods. The company needs to ensure low latency response time consistently to ensure a good user experience. The solution should also minimize operational overhead.</p><p>Which solution would meet these requirements MOST efficiently?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon API Gateway and AWS Lambda together make a highly scalable solution for APIs. Provisioned concurrency in Lambda ensures that there is always a warm pool of functions ready to quickly respond to API requests, thereby guaranteeing low latency even during peak traffic times.</p><p><strong>CORRECT: </strong>\"Use Amazon API Gateway along with AWS Lambda functions with provisioned concurrency\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement the API using AWS Elastic Beanstalk with auto-scaling groups\" is incorrect.</p><p>Elastic Beanstalk is a viable option for deploying applications and auto-scaling and can help handle increased traffic, but it doesn't guarantee the low latency requirement during peak traffic times.</p><p><strong>INCORRECT:</strong> \"Use Amazon API Gateway with AWS Fargate tasks to handle the API requests\" is incorrect.</p><p>API Gateway with Fargate can provide scalable compute, but this approach can result in higher operational overhead because of the need to manage the container lifecycle.</p><p><strong>INCORRECT:</strong> \"Implement the API on an Amazon EC2 instance behind an Application Load Balancer with manual scaling\" is incorrect.</p><p>This solution does not scale automatically and would require manual intervention to ensure optimal performance during traffic spikes. Therefore, it doesn't satisfy the requirement of minimizing operational overhead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
        "answers": [
          "<p>Implement the API using AWS Elastic Beanstalk with auto-scaling groups.</p>",
          "<p>Use Amazon API Gateway with AWS Fargate tasks to handle the API requests.</p>",
          "<p>Use Amazon API Gateway along with AWS Lambda functions with provisioned concurrency.</p>",
          "<p>Implement the API on an Amazon EC2 instance behind an Application Load Balancer with manual scaling.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A telecommunication company has an API that allows users to manage their mobile plans and services. The API experiences significant traffic spikes during specific times such as end of the month and special offer periods. The company needs to ensure low latency response time consistently to ensure a good user experience. The solution should also minimize operational overhead.Which solution would meet these requirements MOST efficiently?",
      "related_lectures": []
    }
  ]
}
