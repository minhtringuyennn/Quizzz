{
  "count": 65,
  "next": null,
  "previous": null,
  "results": [
    {
      "_class": "assessment",
      "id": 80399182,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect is attempting to clean up unused EBS volumes and snapshots to save some space and cost. How many of the most recent snapshots of an EBS volume need to be maintained to guarantee that you can recreate the full EBS volume from the snapshot?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Snapshots capture a point-in-time state of an instance. If you make periodic snapshots of a volume, the snapshots are incremental, which means that only the blocks on the device that have changed after your last snapshot are saved in the new snapshot.</p><p>Even though snapshots are saved incrementally, the snapshot deletion process is designed so that you need to retain only the most recent snapshot in order to restore the volume.</p><p><strong>CORRECT: </strong>\"Only the most recent snapshot. Snapshots are incremental, but the deletion process will ensure that no data is lost\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"You must retain all snapshots as the process is incremental and therefore data is required from each snapshot\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Two snapshots, the oldest and most recent snapshots\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"The oldest snapshot, as this references data in all other snapshots\" is incorrect as explained above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-deleting-snapshot.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-deleting-snapshot.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
        "answers": [
          "<p>Only the most recent snapshot. Snapshots are incremental, but the deletion process will ensure that no data is lost  </p>",
          "<p>The oldest snapshot, as this references data in all other snapshots  </p>",
          "<p>Two snapshots, the oldest and most recent snapshots  </p>",
          "<p>You must retain all snapshots as the process is incremental and therefore data is required from each snapshot  </p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Storage",
      "question_plain": "A Solutions Architect is attempting to clean up unused EBS volumes and snapshots to save some space and cost. How many of the most recent snapshots of an EBS volume need to be maintained to guarantee that you can recreate the full EBS volume from the snapshot?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399184,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect manages multiple Amazon RDS MySQL databases. To improve security, the Solutions Architect wants to enable secure user access with short-lived credentials. How can these requirements be met?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>With MySQL, authentication is handled by AWSAuthenticationPlugin—an AWS-provided plugin that works seamlessly with IAM to authenticate your IAM users. Connect to the DB instance and issue the CREATE USER statement, as shown in the following example.</p><pre class=\"prettyprint linenums\">CREATE USER jane_doe IDENTIFIED WITH AWSAuthenticationPlugin AS 'RDS';</pre><p>The IDENTIFIED WITH clause allows MySQL to use the AWSAuthenticationPlugin to authenticate the database account (jane_doe). The AS 'RDS' clause refers to the authentication method, and the specified database account should have the same name as the IAM user or role. In this example, both the database account and the IAM user or role are named jane_doe.</p><p><strong>CORRECT: </strong>\"Create the MySQL user accounts to use the AWSAuthenticationPlugin with IAM\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure the MySQL databases to use the AWS Security Token Service (STS)\" is incorrect. You cannot configure MySQL to directly use the AWS STS.</p><p><strong>INCORRECT:</strong> \"Configure the application to use the AUTH command to send a unique password\" is incorrect. This is used with Redis databases, not with RDS databases.</p><p><strong>INCORRECT:</strong> \"Configure the MySQL databases to use AWS KMS data encryption keys\" is incorrect. Data encryption keys are used for data encryption not management of connections strings.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.DBAccounts.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.DBAccounts.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
        "answers": [
          "<p>Configure the MySQL databases to use the AWS Security Token Service (STS)</p>",
          "<p>Configure the application to use the AUTH command to send a unique password</p>",
          "<p>Create the MySQL user accounts to use the AWSAuthenticationPlugin with IAM</p>",
          "<p>Configure the MySQL databases to use AWS KMS data encryption keys</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Database",
      "question_plain": "A Solutions Architect manages multiple Amazon RDS MySQL databases. To improve security, the Solutions Architect wants to enable secure user access with short-lived credentials. How can these requirements be met?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399186,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application is running in a private subnet of an Amazon VPC and must have outbound internet access for downloading updates. The Solutions Architect does not want the application exposed to inbound connection attempts. Which steps should be taken?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>To enable outbound connectivity for instances in private subnets a NAT gateway can be created. The NAT gateway is created in a public subnet and a route must be created in the private subnet pointing to the NAT gateway for internet-bound traffic. An internet gateway must be attached to the VPC to facilitate outbound connections.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-07-43-70afcd404db57b167e180b7ee7fee9d7.png\"></p><p>You cannot directly connect to an instance in a private subnet from the internet. You would need to use a bastion/jump host. Therefore, the application will not be exposed to inbound connection attempts.</p><p><strong>CORRECT: </strong>\"Create a NAT gateway and attach an internet gateway to the VPC\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a NAT gateway but do not create attach an internet gateway to the VPC\" is incorrect. An internet gateway must be attached to the VPC for any outbound connections to work.</p><p><strong>INCORRECT:</strong> \"Attach an internet gateway to the private subnet and create a NAT gateway\" is incorrect. You do not attach internet gateways to subnets, you attach them to VPCs.</p><p><strong>INCORRECT:</strong> \"Attach an internet gateway to the VPC but do not create a NAT gateway\" is incorrect. Without a NAT gateway the instances in the private subnet will not be able to download updates from the internet.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>Create a NAT gateway but do not attach an internet gateway to the VPC</p>",
          "<p>Attach an internet gateway to the private subnet and create a NAT gateway</p>",
          "<p>Attach an internet gateway to the VPC but do not create a NAT gateway</p>",
          "<p>Create a NAT gateway and attach an internet gateway to the VPC</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "An application is running in a private subnet of an Amazon VPC and must have outbound internet access for downloading updates. The Solutions Architect does not want the application exposed to inbound connection attempts. Which steps should be taken?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399188,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application has been migrated from on-premises to an Amazon EC2 instance. The migration has failed due to an unknown dependency that the application must communicate with an on-premises server using private IP addresses.</p><p>Which action should a solutions architect take to quickly provision the necessary connectivity?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>A virtual private gateway is a logical, fully redundant distributed edge routing function that sits at the edge of your VPC. You must create a VPG in your VPC before you can establish an AWS Managed site-to-site VPN connection. The other end of the connection is the customer gateway which must be established on the customer side of the connection.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-08-32-b6eebdb5a8f23f26677a1be7da633443.png\"></p><p><strong>CORRECT: </strong>\"Configure a Virtual Private Gateway\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Setup an AWS Direct Connect connection\" is incorrect as this would take too long to provision.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution\" is incorrect. This is not a solution for enabling connectivity using private addresses to an on-premises site. CloudFront is a content delivery network (CDN).</p><p><strong>INCORRECT:</strong> \"Create an AWS Transit Gateway\" is incorrect. AWS Transit Gateway connects VPCs and on-premises networks through a central hub which is not a requirement of this solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>Setup an AWS Direct Connect connection</p>",
          "<p>Configure a Virtual Private Gateway</p>",
          "<p>Create an Amazon CloudFront distribution</p>",
          "<p>Create an AWS Transit Gateway</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "An application has been migrated from on-premises to an Amazon EC2 instance. The migration has failed due to an unknown dependency that the application must communicate with an on-premises server using private IP addresses.Which action should a solutions architect take to quickly provision the necessary connectivity?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399190,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company runs an API on a Linux server in their on-premises data center. The company are planning to migrate the API to the AWS cloud. The company require a highly available, scalable and cost-effective solution. What should a Solutions Architect recommend?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The best option is to use a fully serverless solution. This will provide high availability, scalability and be cost-effective. The components for this would be Amazon API Gateway for hosting the API and AWS Lambda for running the backend.</p><p>As you can see in the image below, API Gateway can be the frontend for multiple backend services:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-09-23-7dad959e9a620f37463b0048341769bc.png\"></p><p><strong>CORRECT: </strong>\"Migrate the API to Amazon API Gateway and use AWS Lambda as the backend\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Migrate the API to Amazon API Gateway and migrate the backend to Amazon EC2\" is incorrect. This is a less available and cost-effective solution for the backend compared to AWS Lambda.</p><p><strong>INCORRECT:</strong> \"Migrate the API server to Amazon EC2 instances in an Auto Scaling group and attach an Application Load Balancer\" is incorrect. Firstly, it may be difficult to load balance to an API. Additionally, this is a less cost-effective solution.</p><p><strong>INCORRECT:</strong> \"Migrate the API to Amazon CloudFront and use AWS Lambda as the origin\" is incorrect. You cannot migrate an API to CloudFront. You can use CloudFront in front of API Gateway but that is not what this answer specifies.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
        "answers": [
          "<p>Migrate the API to Amazon API Gateway and migrate the backend to Amazon EC2</p>",
          "<p>Migrate the API server to Amazon EC2 instances in an Auto Scaling group and attach an Application Load Balancer</p>",
          "<p>Migrate the API to Amazon API Gateway and use AWS Lambda as the backend</p>",
          "<p>Migrate the API to Amazon CloudFront and use AWS Lambda as the origin</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A company runs an API on a Linux server in their on-premises data center. The company are planning to migrate the API to the AWS cloud. The company require a highly available, scalable and cost-effective solution. What should a Solutions Architect recommend?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399192,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application that is being installed on an Amazon EC2 instance requires a persistent block storage volume. The data must be encrypted at rest and regular volume-level backups must be automated.</p><p>Which solution options should be used?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>For block storage the Solutions Architect should use either Amazon EBS or EC2 instance store. However, the instance store is non-persistent so EBS must be used. With EBS you can encrypt your volume and automate volume-level backups using snapshots that are run by Data Lifecycle Manager.</p><p><strong>CORRECT: </strong>\"Use an encrypted Amazon EBS volume and use Data Lifecycle Manager to automate snapshots\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an encrypted Amazon EFS filesystem and use an Amazon CloudWatch Events rule to start a backup copy of data using AWS Lambda\" is incorrect. EFS is not block storage, it is a file-level storage service.</p><p><strong>INCORRECT:</strong> \"Use server-side encryption on an Amazon S3 bucket and use Cross-Region-Replication to backup on a schedule\" is incorrect. Amazon S3 is an object-based storage system not a block-based storage system.</p><p><strong>INCORRECT:</strong> \"Use an encrypted Amazon EC2 instance store and copy the data to another EC2 instance using a cron job and a batch script \" is incorrect as the EC2 instance store is a non-persistent volume.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
        "answers": [
          "<p>Use an encrypted Amazon EBS volume and use Data Lifecycle Manager to automate snapshots</p>",
          "<p>Use an encrypted Amazon EFS filesystem and use an Amazon CloudWatch Events rule to start a backup copy of data using AWS Lambda</p>",
          "<p>Use server-side encryption on an Amazon S3 bucket and use Cross-Region-Replication to backup on a schedule</p>",
          "<p>Use an encrypted Amazon EC2 instance store and copy the data to another EC2 instance using a cron job and a batch script</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Compute",
      "question_plain": "An application that is being installed on an Amazon EC2 instance requires a persistent block storage volume. The data must be encrypted at rest and regular volume-level backups must be automated.Which solution options should be used?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399194,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company has several AWS accounts each with multiple Amazon VPCs. The company must establish routing between all private subnets. The architecture should be simple and allow transitive routing to occur.</p><p>How should the network connectivity be configured?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>You can build a hub-and-spoke topology with AWS Transit Gateway that supports transitive routing. This simplifies the network topology and adds additional features over VPC peering. AWS Resource Access Manager can be used to share the connection with the other AWS accounts.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-10-15-06b3dc1b26269bff9b93a25c89727620.png\"></p><p><strong>CORRECT: </strong>\"Create an AWS Transit Gateway and share it with each account using AWS Resource Access Manager\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a transitive VPC peering connection between each Amazon VPC and configure route tables\" is incorrect. You cannot create transitive connections with VPC peering.</p><p><strong>INCORRECT:</strong> \"Create an AWS Managed VPN between each Amazon VPC and configure route tables\" is incorrect. This is a much more complex solution compared to AWS Transit Gateway so is not the best option.</p><p><strong>INCORRECT:</strong> \"Create a hub-and-spoke topology with AWS App Mesh and use AWS Resource Access Manager to share route tables\" is incorrect. AWS App Mesh is used for application-level networking for microservices applications.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-use-an-aws-transit-gateway-to-simplify-your-network-architecture/\">https://aws.amazon.com/blogs/aws/new-use-an-aws-transit-gateway-to-simplify-your-network-architecture/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>Create a transitive VPC peering connection between each Amazon VPC and configure route tables</p>",
          "<p>Create an AWS Transit Gateway and share it with each account using AWS Resource Access Manager</p>",
          "<p>Create an AWS Managed VPN between each Amazon VPC and configure route tables</p>",
          "<p>Create a hub-and-spoke topology with AWS App Mesh and use AWS Resource Access Manager to share route tables</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A company has several AWS accounts each with multiple Amazon VPCs. The company must establish routing between all private subnets. The architecture should be simple and allow transitive routing to occur.How should the network connectivity be configured?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399196,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An organization is planning their disaster recovery solution. They plan to run a scaled down version of a fully functional environment. In a DR situation the recovery time must be minimized.</p><p>Which DR strategy should a Solutions Architect recommend?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud. A warm standby solution extends the pilot light elements and preparation.</p><p>It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems on AWS and have them always on.</p><p><strong>CORRECT: </strong>\"Warm standby\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Backup and restore\" is incorrect. This is the lowest cost DR approach that simply entails creating online backups of all data and applications.</p><p><strong>INCORRECT:</strong> Pilot light\"\" is incorrect. With a pilot light strategy a core minimum of services are running and the remainder are only brought online during a disaster recovery situation.</p><p><strong>INCORRECT:</strong> \"Multi-site\" is incorrect. A multi-site solution runs on AWS as well as on your existing on-site infrastructure in an active- active configuration.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/\">https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/</a></p>",
        "answers": [
          "<p>Backup and restore</p>",
          "<p>Pilot light</p>",
          "<p>Warm standby</p>",
          "<p>Multi-site</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Cloud Architecture & Design",
      "question_plain": "An organization is planning their disaster recovery solution. They plan to run a scaled down version of a fully functional environment. In a DR situation the recovery time must be minimized.Which DR strategy should a Solutions Architect recommend?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399198,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application analyzes images of people that are uploaded to an Amazon S3 bucket. The application determines demographic data which is then saved to a .CSV file in another S3 bucket. The data must be encrypted at rest and then queried using SQL. The solution should be fully serverless.</p><p>Which actions should a Solutions Architect take to encrypt and query the data?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Amazon Athena supports encrypted data for both the source data and query results, for example, using Amazon S3 with AWS KMS.</p><p><strong>CORRECT: </strong>\"Use AWS KMS encryption keys for the S3 bucket and use Amazon Athena to query the data\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 server-side encryption and use Amazon RedShift Spectrum to query the data\" is incorrect. RedShift Spectrum is not serverless as it requires a RedShift cluster which is based on EC2 instances.</p><p><strong>INCORRECT:</strong> \"Use AWS KMS encryption keys for the S3 bucket and use Amazon Kinesis Data Analytics to query the data\" is incorrect. Kinesis Data Analytics is used for analyzing real-time streaming data in Kinesis streams.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 server-side encryption and Amazon QuickSight to query the data\" is incorrect. Amazon QuickSight is an interactive dashboard, it is not a service for running queries on data.</p><p><strong>References:</strong></p><p><a href=\"https://d1.awsstatic.com/whitepapers/architecture/wellarchitected-Machine-Learning-Lens.pdf\">https://d1.awsstatic.com/whitepapers/architecture/wellarchitected-Machine-Learning-Lens.pdf</a></p><p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
        "answers": [
          "<p>Use Amazon S3 server-side encryption and use Amazon RedShift Spectrum to query the data</p>",
          "<p>Use AWS KMS encryption keys for the S3 bucket and use Amazon Athena to query the data</p>",
          "<p>Use AWS KMS encryption keys for the S3 bucket and use Amazon Kinesis Data Analytics to query the data</p>",
          "<p>Use Amazon S3 server-side encryption and Amazon QuickSight to query the data</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "An application analyzes images of people that are uploaded to an Amazon S3 bucket. The application determines demographic data which is then saved to a .CSV file in another S3 bucket. The data must be encrypted at rest and then queried using SQL. The solution should be fully serverless.Which actions should a Solutions Architect take to encrypt and query the data?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399200,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A large quantity of data is stored on a NAS device on-premises and accessed using the SMB protocol. The company require a managed service for hosting the filesystem and a tool to automate the migration.</p><p>Which actions should a Solutions Architect take?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. This is the most suitable destination for this use case.</p><p>AWS DataSync can be used to move large amounts of data online between on-premises storage and Amazon S3, Amazon EFS, or Amazon FSx for Windows File Server. The source datastore can be Server Message Block (SMB) file servers.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-11-07-d909de298264e2e376a88159790c0be1.png\"></p><p><strong>CORRECT: </strong>\"Migrate the data to Amazon FSx for Windows File Server using AWS DataSync\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Migrate the data to Amazon EFS using the AWS Server Migration Service (SMS)\" is incorrect. EFS is used for hosting filesystems accessed over NFS from Linux (not Windows). The SMS service is used for migrating virtual machines, not data.</p><p><strong>INCORRECT:</strong> \"Migrate the data to Amazon FSx for Lustre using AWS DataSync\" is incorrect. Amazon FSx for Windows File Server should be used for hosting SMB shares.</p><p><strong>INCORRECT:</strong> \"Migrate the data to Amazon S3 using and AWS Snowball Edge device\" is incorrect. Amazon S3 is an object store and unsuitable for hosting an SMB filesystem. Snowball is not required in this case as the data is not going to S3 and there are no time or bandwidth limitations mentioned in the scenario.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p><p><a href=\"https://aws.amazon.com/datasync/features/\">https://aws.amazon.com/datasync/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
        "answers": [
          "<p>Migrate the data to Amazon EFS using the AWS Server Migration Service (SMS)</p>",
          "<p>Migrate the data to Amazon FSx for Lustre using AWS DataSync</p>",
          "<p>Migrate the data to Amazon FSx for Windows File Server using AWS DataSync</p>",
          "<p>Migrate the data to Amazon S3 using and AWS Snowball Edge device</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "A large quantity of data is stored on a NAS device on-premises and accessed using the SMB protocol. The company require a managed service for hosting the filesystem and a tool to automate the migration.Which actions should a Solutions Architect take?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399202,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>The database layer of an on-premises web application is being migrated to AWS. The database uses a multi-threaded, in-memory caching layer to improve performance for repeated queries. Which service would be the most suitable replacement for the database cache?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon ElastiCache with the Memcached engine is an in-memory database that can be used as a database caching layer. The memached engine supports multiple cores and threads and large nodes.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-11-45-0d3bdca18464fbf41521677922bfcec8.png\"></p><p><strong>CORRECT: </strong>\"Amazon ElastiCache Memcached\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon ElastiCache Redis\" is incorrect. The Redis engine does not support multiple CPU cores or threads.</p><p><strong>INCORRECT:</strong> \"Amazon DynamoDB DAX\" is incorrect. Amazon DynamoDB Accelerator (DAX) is a database cache that should be used with DynamoDB only.</p><p><strong>INCORRECT:</strong> \"Amazon RDS MySQL\" is incorrect as this is not an example of an in-memory database that can be used as a database caching layer.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
        "answers": [
          "<p>Amazon ElastiCache Redis</p>",
          "<p>Amazon DynamoDB DAX</p>",
          "<p>Amazon ElastiCache Memcached</p>",
          "<p>Amazon RDS MySQL</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Database",
      "question_plain": "The database layer of an on-premises web application is being migrated to AWS. The database uses a multi-threaded, in-memory caching layer to improve performance for repeated queries. Which service would be the most suitable replacement for the database cache?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399204,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect is designing an application for processing and extracting data from log files. The log files are generated by an application and the number and frequency of updates varies. The files are up to 1 GB in size and processing will take around 40 seconds for each file.</p><p>Which solution is the most cost-effective?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The question asks for the most cost-effective solution and therefor a serverless and automated solution will be the best choice.</p><p>AWS Lambda can run custom code in response to Amazon S3 bucket events. You upload your custom code to AWS Lambda and create a function. When Amazon S3 detects an event of a specific type (for example, an object created event), it can publish the event to AWS Lambda and invoke your function in Lambda. In response, AWS Lambda executes your function.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-06-28_20-12-32-5693ddd5be6f40334bb8d981a4099fbd.png\"></p><p><strong>CORRECT: </strong>\"Write the log files to an Amazon S3 bucket. Create an event notification to invoke an AWS Lambda function that will process the files\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Write the log files to an Amazon EC2 instance with an attached EBS volume. After processing, save the files to an Amazon S3 bucket\" is incorrect. This is not cost effective as it is not serverless.</p><p><strong>INCORRECT:</strong> \"Write the log files to an Amazon SQS queue. Use AWS Lambda to process the files from the queue and save to an Amazon S3 bucket\" is incorrect. SQS has a maximum message size of 256 KB so the message body would need to be saved in S3 anyway. Using an event source mapping from S3 would be less complex and preferable.</p><p><strong>INCORRECT:</strong> \"Write the log files to an Amazon S3 bucket. Create an event notification to invoke an Amazon ECS task to process the files and save to an Amazon S3 bucket\" is incorrect. You cannot use event notifications to process Amazon ECS tasks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
        "answers": [
          "<p>Write the log files to an Amazon EC2 instance with an attached EBS volume. After processing, save the files to an Amazon S3 bucket</p>",
          "<p>Write the log files to an Amazon SQS queue. Use AWS Lambda to process the files from the queue and save to an Amazon S3 bucket</p>",
          "<p>Write the log files to an Amazon S3 bucket. Create an event notification to invoke an Amazon ECS task to process the files and save to an Amazon S3 bucket</p>",
          "<p>Write the log files to an Amazon S3 bucket. Create an event notification to invoke an AWS Lambda function that will process the files</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Compute",
      "question_plain": "A Solutions Architect is designing an application for processing and extracting data from log files. The log files are generated by an application and the number and frequency of updates varies. The files are up to 1 GB in size and processing will take around 40 seconds for each file.Which solution is the most cost-effective?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399206,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect is designing a migration strategy for a company moving to the AWS Cloud. The company use a shared Microsoft filesystem that uses Distributed File System Namespaces (DFSN). What will be the MOST suitable migration strategy for the filesystem?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The destination filesystem should be Amazon FSx for Windows File Server. This supports DFSN and is the most suitable storage solution for Microsoft filesystems. AWS DataSync supports migrating to the Amazon FSx and automates the process.</p><p><strong>CORRECT: </strong>\"Use AWS DataSync to migrate to Amazon FSx for Windows File Server\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use the AWS Server Migration Service to migrate to Amazon FSx for Lustre\" is incorrect. The server migration service is used to migrate virtual machines and FSx for Lustre does not support Windows filesystems.</p><p><strong>INCORRECT:</strong> \"Use AWS DataSync to migrate to an Amazon EFS filesystem\" is incorrect. You can migrate data to EFS using DataSync but it is the wrong destination for a Microsoft filesystem (Linux only).</p><p><strong>INCORRECT:</strong> \"Use the AWS Server Migration Service to migrate to an Amazon S3 bucket\" is incorrect. The server migration service is used to migrate virtual machines and Amazon S3 is an object-based storage system and unsuitable for hosting a Microsoft filesystem.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/storage/migrate-to-amazon-fsx-for-windows-file-server-using-aws-datasync/\">https://aws.amazon.com/blogs/storage/migrate-to-amazon-fsx-for-windows-file-server-using-aws-datasync/</a></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-files-fsx.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-files-fsx.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
        "answers": [
          "<p>Use the AWS Server Migration Service to migrate to an Amazon S3 bucket</p>",
          "<p>Use the AWS Server Migration Service to migrate to Amazon FSx for Lustre</p>",
          "<p>Use AWS DataSync to migrate to an Amazon EFS filesystem</p>",
          "<p>Use AWS DataSync to migrate to Amazon FSx for Windows File Server</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Migration & Transfer",
      "question_plain": "A Solutions Architect is designing a migration strategy for a company moving to the AWS Cloud. The company use a shared Microsoft filesystem that uses Distributed File System Namespaces (DFSN). What will be the MOST suitable migration strategy for the filesystem?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399208,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application is used by a large bank to ingest incoming messages. The messages are then quickly consumed by dozens of other applications and microservices. The number of messages can increase suddenly from a few messages per second up to 120,000 messages per second. In response to several recent outages and failures, the company wants to decouple this applications architecture and solution to ensure scalability.</p><p>Which solution meets these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon SNS can be used in this situation in a fanout architecture where messages sent to the SNS topic and then forwarded to multiple SQS queues that are subscribed to the topic. The messages can then be processed by different consumer applications from these queues.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-09-21_03-18-09-96e319b141d13bca0d11b7e412c0d801.jpg\"><p><strong>CORRECT: </strong>\"Post the messages to an Amazon Simple Notification Service topic with multiple Amazon Simple Queue Service subscriptions. Process messages from queues using the Consumer Applications” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Persist the messages in Amazon Kinesis Data Analytics. Make sure the consumer applications are configured to read and process the messages” is incorrect. Amazon Kinesis Data Analytics is used for analyzing data using SQL, it is not used for ingesting messages.</p><p><strong>INCORRECT:</strong> \"Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group and scale up and down based on CPU Utilization” is incorrect. This is not a scalable enough architecture for the application’s needs as scaling based on auto scaling groups can take many minutes, not seconds as is required for the application.</p><p><strong>INCORRECT:</strong> \"Write the messages to Amazon Kinesis Data Streams using one shard. Use an AWS Lambda function to process messages and place them in a DynamoDB table. The Applications can then be read from the DynamoDB table” is incorrect. A single shard is limited to 1 MB or 1000 messages/sec, therefore multiple shards would be required.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
        "answers": [
          "<p>Persist the messages in Amazon Kinesis Data Analytics. Make sure the consumer applications are configured to read and process the messages.</p>",
          "<p>Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group and scale up and down based on CPU Utilization.</p>",
          "<p>Write the messages to Amazon Kinesis Data Streams using one shard. Use an AWS Lambda function to process messages and place them in a DynamoDB table. The Applications can then be read from the DynamoDB table.</p>",
          "<p>Post the messages to an Amazon Simple Notification Service topic with multiple Amazon Simple Queue Service subscriptions. Process messages from queues using the Consumer Applications.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Application Integration",
      "question_plain": "An application is used by a large bank to ingest incoming messages. The messages are then quickly consumed by dozens of other applications and microservices. The number of messages can increase suddenly from a few messages per second up to 120,000 messages per second. In response to several recent outages and failures, the company wants to decouple this applications architecture and solution to ensure scalability.Which solution meets these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399210,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A large manufacturing company is migrating many of its on-premises applications to AWS. The applications are staged in many different AWS accounts under a payer account, using AWS Organizations. The company's security team needs to enable a single sign-on (SSO) solution across all the company's accounts, and this must be integrated with the company's existing Active Directory setup.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS IAM Identity Center (successor to AWS Single Sign-On) requires a two-way trust so that it has permissions to read user and group information from your domain to synchronize user and group metadata. IAM Identity Center uses this metadata when assigning access to permission sets or applications.</p><p>User and group metadata is also used by applications for collaboration, like when you share a dashboard with another user or group. The trust from AWS Directory Service for Microsoft Active Directory to your domain permits IAM Identity Center to trust your domain for authentication. The trust in the opposite direction grants AWS permissions to read user and group metadata.</p><p><strong>CORRECT: </strong>\"Enable AWS IAM Identity Center (successor to AWS SSO). Create a two-way forest trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable AWS IAM Identity Center (successor to AWS SSO). Create a one-way domain trust to connect the company's self-managed Microsoft Active Directory by using AWS Directory Service for Microsoft Active Directory” is incorrect. A two-way trust is required by IAM Identity Center.</p><p><strong>INCORRECT:</strong> \"Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed Microsoft Active Directory” is incorrect. This solution does not enable SSO across the accounts as it does not involve IAM Identity Center.</p><p><strong>INCORRECT:</strong> \"Deploy an identity provider (IdP) on premises. Enable AWS IAM Identity Center (successor to AWS SSO) from the AWS Identity Center console” is incorrect. The IdP is already deployed as the company has Microsoft AD. This does not provide a solution for integrating and enabling SSO.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
        "answers": [
          "<p>Enable AWS IAM Identity Center (successor to AWS SSO). Create a one-way domain trust to connect the company's self-managed Microsoft Active Directory by using AWS Directory Service for Microsoft Active Directory.</p>",
          "<p>Enable AWS IAM Identity Center (successor to AWS SSO). Create a two-way forest trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.</p>",
          "<p>Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed Microsoft Active Directory.</p>",
          "<p>Deploy an identity provider (IDP) on-premises. Enable AWS IAM Identity Center (successor to AWS SSO) from the AWS Identity Center console.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A large manufacturing company is migrating many of its on-premises applications to AWS. The applications are staged in many different AWS accounts under a payer account, using AWS Organizations. The company's security team needs to enable a single sign-on (SSO) solution across all the company's accounts, and this must be integrated with the company's existing Active Directory setup.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399212,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A retail company is running an important event. The company require guaranteed capacity in two specific Availability Zones in a specific AWS Region for running Amazon EC2 instances for 5 consecutive days.</p><p>What is the best way to ensure guaranteed EC2 capacity?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>On-Demand Capacity Reservations enable you to reserve compute capacity for your Amazon EC2 instances in a specific Availability Zone for any duration.</p><p>When creating Capacity Reservations, you ensure that you always have access to EC2 capacity when you need it, for as long as you need it, in this case for 5 days. You can create Capacity Reservations at any time, without entering a one-year or three-year term commitment.</p><p>Also, when you create a Capacity Reservation, you specify:</p><p>- The Availability Zone in which to reserve the capacity.</p><p>- The number of instances for which to reserve capacity.</p><p>- The instance attributes, including the instance type, tenancy, and platform/OS.</p><p><strong>CORRECT: </strong>\"Create an On-Demand Capacity Reservation that specifies the Region and two Availability Zones needed\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Purchase Reserved Instances that specify the Region\" is incorrect. Reserved Instances do not provide guaranteed capacity and are solely a billing discount.</p><p><strong>INCORRECT:</strong> \"Create an On-Demand Capacity Reservation that specifies the Region” is incorrect as you must specify the Availability zones required when reserving capacity.</p><p><strong>INCORRECT:</strong> \"Purchase Reserved Instances that specify the Region and two Availability Zones needed” is incorrect. Reserved Instances do not provide guaranteed capacity and are solely a billing discount.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-billing-and-pricing/\">https://digitalcloud.training/aws-billing-and-pricing/</a></p>",
        "answers": [
          "<p>Purchase Reserved Instances that specify the Region.</p>",
          "<p>Create an On-Demand Capacity Reservation that specifies the Region.</p>",
          "<p>Purchase Reserved Instances that specify the Region and two Availability Zones needed.</p>",
          "<p>Create an On-Demand Capacity Reservation that specifies the Region and two Availability Zones needed.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Compute",
      "question_plain": "A retail company is running an important event. The company require guaranteed capacity in two specific Availability Zones in a specific AWS Region for running Amazon EC2 instances for 5 consecutive days.What is the best way to ensure guaranteed EC2 capacity?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399214,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company provides a Voice over Internet Protocol (VoIP) service that uses UDP as the protocol. The service utilizes Amazon EC2 instances that are scaled automatically using an Auto Scaling group. The company currently uses multiple AWS Regions for its AWS deployments.</p><p>The company needs to route users to the appropriate Region based on the lowest latency. The company also needs automated failover between Regions.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>For UDP traffic the solution must use a Network Load Balancer as ALBs do not support UDP. The solution also requires both latency-based routing and automated failover. AWS Global Accelerator can be used to achieve both these requirements. It will direct users to the lowest latency endpoint and if an endpoint becomes unhealthy it automatically reroutes to the next best endpoint.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-09-21_03-21-33-01ff999266aba8530a30ecca0b2c32d9.jpg\"><p><strong>CORRECT: </strong>\"Set up a Network Load Balancer (NLB) and an associated target group. Assign the target group with the Auto Scaling group. In each region, use the NLB as an AWS Global Accelerator endpoint\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer (NLB) and an associated target group. Assign the target group to the Auto Scaling group and create an Amazon Route 53 latency record that points to aliases for each NLB\" is incorrect. An NLB must be used but Route 53 latency-based routing will not automatically failover the application to another endpoint unless health checks are enabled, and this is not described.</p><p><strong>INCORRECT:</strong> \"Set up an Application Load Balancer (ALB) and a target group. Associate the target group with the Auto Scaling group and use the ALB as an AWS Global Accelerator endpoint in each Region” is incorrect as Application Load Balancers balance HTTP and HTTPS traffic at Layer 7, not UDP traffic.</p><p><strong>INCORRECT:</strong> \"Deploy an Application Load Balancer (ALB) and its associated target group. Assign the target group to the Auto Scaling group and create an Amazon Route 53 weighted record that points to aliases for each ALB\" is incorrect as ALBs do not support UDP listeners and weighted routing is not used for latency or failover.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
        "answers": [
          "<p>Set up a Network Load Balancer (NLB) and an associated target group. Assign the target group with the Auto Scaling group. In each region, use the NLB as an AWS Global Accelerator endpoint.</p>",
          "<p>Set up an Application Load Balancer (ALB) and a target group. Associate the target group with the Auto Scaling group and use the ALB as an AWS Global Accelerator endpoint in each Region.</p>",
          "<p>Create a Network Load Balancer (NLB) and an associated target group. Assign the target group to the Auto Scaling group and create an Amazon Route 53 latency record that points to aliases for each NLB.</p>",
          "<p>Deploy an Application Load Balancer (ALB) and its associated target group. Assign the target group to the Auto Scaling group and create an Amazon Route 53 weighted record that points to aliases for each ALB.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A company provides a Voice over Internet Protocol (VoIP) service that uses UDP as the protocol. The service utilizes Amazon EC2 instances that are scaled automatically using an Auto Scaling group. The company currently uses multiple AWS Regions for its AWS deployments.The company needs to route users to the appropriate Region based on the lowest latency. The company also needs automated failover between Regions.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399216,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A retail organization is building an ecommerce application on AWS. The application sends information about new orders to a REST API hosted on Amazon API Gateway to process. The company needs the orders to be processed in the order that they are received.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Based on the application requirements of having the orders to be processed in the order that they are received, you could use a FIFO queue, which offers high throughput, exactly-once-processing, and first-in-first-out-delivery.</p><p><strong>CORRECT: </strong>\"When an order is received, use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. For processing, configure the SQS FIFO queue to invoke an AWS Lambda function” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Integrate the Amazon Simple Notification Service (Amazon SNS) with API Gateway. The Amazon SNS topic will send a message to AWS Lambda where the message will be processed. \" is incorrect. Amazon SNS is not suitable for this application as Amazon SNS is a one-to-many messaging service designed to deliver messages to subscribers using SMS, Emails etc.</p><p><strong>INCORRECT:</strong> \"While the application processes an order, API Gateway authorizers will block any requests” is incorrect. A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API, which does not change how the traffic is delivered in which order.</p><p><strong>INCORRECT:</strong> \"When an order is received, use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue. For processing, configure the SQS standard queue to invoke an AWS Lambda function” is incorrect as an SQS standard queue offers best-effort-ordering, which is not suitable for this use case.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
        "answers": [
          "<p>Integrate the Amazon Simple Notification Service (Amazon SNS) with API Gateway. The Amazon SNS topic will send a message to AWS Lambda where the message will be processed.</p>",
          "<p>When an order is received, use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. For processing, configure the SQS FIFO queue to invoke an AWS Lambda function.</p>",
          "<p>While the application processes an order, API Gateway authorizers will block any requests.</p>",
          "<p>When an order is received, use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue. For processing, configure the SQS standard queue to invoke an AWS Lambda function.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Application Integration",
      "question_plain": "A retail organization is building an ecommerce application on AWS. The application sends information about new orders to a REST API hosted on Amazon API Gateway to process. The company needs the orders to be processed in the order that they are received.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399218,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An e-commerce website uses Amazon EC2 instance stores for storing session data. The company want to make sure that this data is highly available, and that the information is stored durably.</p><p>What should a solutions architect do to meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon DynamoDB is a NoSQL database and is ideal for storing session data. The data will be both highly available and durable and can be stored persistently. DynamoDB also offers time to live (TTL) attributes that can be used to automatically expire items from the table after specified time periods.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-09-21_03-25-17-85c40bff067bf144ef733a6f48acfe24.jpg\"><p><strong>CORRECT: </strong>\"Store the session data in an Amazon DynamoDB table\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Move the session data to Amazon ElastiCache for Memcached” is incorrect. ElastiCache Memcached does not store data durably or persistently. ElastiCache can be used for storing session data, but the Redis engine should be used instead.</p><p><strong>INCORRECT:</strong> \"Deploy a larger EC2 instance with a larger instance store” is incorrect. Instance stores use ephemeral storage which means it is non-persistent. The size of the instance store does not change anything here.</p><p><strong>INCORRECT:</strong> \" Move the session data to Amazon S3 Glacier Deep Archive” is incorrect. Glacier is an archiving solution and cannot be used for data that requires immediate access. It is unsuitable for storing session data.</p><p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
        "answers": [
          "<p>Move the session data to Amazon ElastiCache for Memcached.</p>",
          "<p>Deploy a larger EC2 instance with a larger instance store.</p>",
          "<p>Move the session data to Amazon S3 Glacier Deep Archive.</p>",
          "<p>Store the session data in an Amazon DynamoDB table.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Database",
      "question_plain": "An e-commerce website uses Amazon EC2 instance stores for storing session data. The company want to make sure that this data is highly available, and that the information is stored durably.What should a solutions architect do to meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399220,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A social media application is creating new functionality that will convert uploaded images to smaller, thumbnail images. When a user uploads an image through the web interface, the application should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function and store the image in its compressed form in a different S3 bucket.</p><p>The solution architect must develop a stateless, durable solution to process images automatically upon upload.</p><p>Which combination of actions will meet these requirements? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>You can use event notifications to publish an event to a destination when something happens in a bucket. Destinations include Lambda, SNS, and SQS. In this case the event notification can be configured to publish a message to an SQS queue when an object creation event occurs.</p><p>Lambda can be configured to poll the queue looking for new messages. When a message is added to the queue Lambda can process the message which will let the function know which image to resize. The resized image can then be saved to an output bucket.</p><p><strong>CORRECT: </strong>\"Create an Amazon SQS queue. Configure an event notification to add a message to the SQS queue when an image is uploaded to the S3 bucket” is the correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure the Lambda function to use the Amazon SQS queue as the event source. The Lambda function will resize the image and store it in a separate S3 Bucket\" is also the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the S3 Bucket to be an event source for a Lambda Function. When an uploaded image is detected, write the file name to a text file in memory and use the text file to keep track of the images that were processed” is incorrect. This solution saves state in memory which is not durable.</p><p><strong>INCORRECT:</strong> \"Launch an Amazon EC2 instance to connect to an Amazon SQS queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function” is incorrect. A single EC2 instance is not a durable solution, as if the single instance failed the solution would no longer work.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon EventBridge event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon SNS topic with the application owner's email address for further processing” is incorrect. An event notification should be created on the S3 bucket to publish information about object creation events. Destinations can be Lambda, SNS, or SQS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
        "answers": [
          "<p>Create an Amazon SQS queue. Configure an event notification to add a message to the SQS queue when an image is uploaded to the S3 bucket.</p>",
          "<p>Configure the Lambda function to use the Amazon SQS queue as the event source. The Lambda function will resize the image and store it in a separate S3 Bucket.</p>",
          "<p>Configure the S3 Bucket to be an event source for a Lambda Function. When an uploaded image is detected, write the file name to a text file in memory and use the text file to keep track of the images that were processed.</p>",
          "<p>Launch an Amazon EC2 instance to connect to an Amazon SQS queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function.</p>",
          "<p>Configure an Amazon EventBridge event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon SNS topic with the application owner's email address for further processing.</p>"
        ]
      },
      "correct_response": ["a", "b"],
      "section": "AWS Application Integration",
      "question_plain": "A social media application is creating new functionality that will convert uploaded images to smaller, thumbnail images. When a user uploads an image through the web interface, the application should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function and store the image in its compressed form in a different S3 bucket.The solution architect must develop a stateless, durable solution to process images automatically upon upload.Which combination of actions will meet these requirements? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399222,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An e-commerce company wants to ensure all its resources used to host its various Web Applications are tagged using the appropriate application name to allow the company to easily differentiate and group resources. The company wants to minimize effort involved and automate this task.</p><p>What should a solutions architect do to accomplish this with the LEAST operational overhead?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Config enables AWS resource inventory and change management as well as Config Rules to confirm that resources are configured in compliance with policies that you define. This is the easiest way to automate the detection of non-compliant resources.</p><p>You can create custom Systems Manager automation documents to remediate the missing tags. The documents can be configured for automatic remediation in AWS Config.</p><p><strong>CORRECT: </strong>\"Use AWS Config to detect resources that are not properly tagged. Create a Systems Manager automation document for remediation” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Cost Explorer to display any application components that are not properly tagged. Tag those resources using a Python Script” is incorrect. Cost Explorer is not designed for configuration compliance and would not provide the required information.</p><p><strong>INCORRECT:</strong> \"Configure AWS CloudTrail to send events to an Amazon CloudWatch Logs log group. Use insights queries to detect API events that do not include TagResources actions” is incorrect. This is an unworkable and highly inefficient attempt at configuration compliance. Much better to use AWS Config which is designed for this purpose.</p><p><strong>INCORRECT:</strong> \"Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code” is incorrect as this would contain a significant amount of operational overhead.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p>",
        "answers": [
          "<p>Use AWS Config to detect resources that are not properly tagged. Create a Systems Manager automation document for remediation.</p>",
          "<p>Use Cost Explorer to display any application components that are not properly tagged. Tag those resources using a Python Script.</p>",
          "<p>Configure AWS CloudTrail to send events to an Amazon CloudWatch Logs log group. Use insights queries to detect API events that do not include TagResources actions.</p>",
          "<p>Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Management & Governance",
      "question_plain": "An e-commerce company wants to ensure all its resources used to host its various Web Applications are tagged using the appropriate application name to allow the company to easily differentiate and group resources. The company wants to minimize effort involved and automate this task.What should a solutions architect do to accomplish this with the LEAST operational overhead?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399224,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company runs an application using many Amazon EC2 instances for its application servers. The application using Amazon DynamoDB for its data store. The size of this table continuously grows, but the application only requires data from the most recent 30 days. The company needs a solution that minimizes cost and effort.</p><p>Which solution meets these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. TTL is provided at no extra cost to reduce stored data volumes by retaining only the items that remain current for your workload’s needs.</p><p><strong>CORRECT: </strong>\"Add an attribute to each new item created in the table that has a value of the current timestamp plus 30 days. Configure this attribute as the TTL attribute” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the entire solution using an AWS CloudFormation template. Re-deploy the CloudFormation stack every 30 days, and then delete the original stack” is incorrect. This solution requires significant disruption and is highly inefficient.</p><p><strong>INCORRECT:</strong> \"Run a monitoring application from the AWS Marketplace using an EC2 instance configured with a Golden AMI. When a new item is created in the table, configure the monitoring application to use Amazon DynamoDB Streams to store the timestamp. For items with a timestamp older than 30 days, run a script on the EC2 instance” is incorrect as this would require more cost and operational overhead.</p><p><strong>INCORRECT:</strong> \"When a new item is created in the table, Amazon DynamoDB Streams will invoke an AWS Lambda function. Set the Lambda function to delete items in the table that are older than 30 days\" is incorrect. Whilst this is possible, it provides this entails higher operational overhead and cost.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
        "answers": [
          "<p>Deploy the entire solution using an AWS CloudFormation template. Re-deploy the CloudFormation stack every 30 days, and then delete the original stack.</p>",
          "<p>Run a monitoring application from the AWS Marketplace using an EC2 instance configured with a Golden AMI. When a new item is created in the table, configure the monitoring application to use Amazon DynamoDB Streams to store the timestamp. For items with a timestamp older than 30 days, run a script on the EC2 instance.</p>",
          "<p>When a new item is created in the table, Amazon DynamoDB Streams will invoke an AWS Lambda function. Set the Lambda function to delete items in the table that are older than 30 days.</p>",
          "<p>Add an attribute to each new item created in the table that has a value of the current timestamp plus 30 days. Configure this attribute as the TTL attribute.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Database",
      "question_plain": "A company runs an application using many Amazon EC2 instances for its application servers. The application using Amazon DynamoDB for its data store. The size of this table continuously grows, but the application only requires data from the most recent 30 days. The company needs a solution that minimizes cost and effort.Which solution meets these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399226,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A media company is designing a disaster recovery (DR) solution for a business-critical application. The recovery time objective (RTO) should be 4 hours or less. The application is running on Amazon EC2 instances using the fewest possible AWS resources during normal operations.</p><p>Which of the following is recommended to implement the DR solution across regions cost-effectively?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>When you have a few hours to achieve disaster recovery, copying AMI’s across regions is an achievable solution. AWS CloudFormation can then be us</p><p>ed to quickly spin up the instances in the second region when a disaster recovery event occurs. This is the most cost-effective option as only the active site has running instances.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-08-45-2eadec4e7234a3f88da746220e9d2aad.jpg\"><p><strong>CORRECT: </strong>\"Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create Amazon Machine Images (AMI) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts\" is incorrect.</p><p>AWS CloudFormation is more suited to deploying infrastructure than using Lambda with custom scripts.</p><p><strong>INCORRECT:</strong> \"Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times\" is incorrect.</p><p>This approach can work but this is not a cost-effective choice.</p><p><strong>INCORRECT:</strong> \"Launch EC2 instances in a secondary Availability Zone. Always keep the EC2 instances in the secondary Availability Zone active\" is incorrect. As with the previous answer, this is not cost-effective.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws-part-i-strategies-for-recovery-in-the-cloud/\">https://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws-part-i-strategies-for-recovery-in-the-cloud/</a></p><p><a href=\"https://aws.amazon.com/blogs/architecture/creating-a-multi-region-application-with-aws-services-part-1-compute-and-security/\">https://aws.amazon.com/blogs/architecture/creating-a-multi-region-application-with-aws-services-part-1-compute-and-security/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>",
        "answers": [
          "<p>Create Amazon Machine Images (AMI) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts.</p>",
          "<p>Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation.</p>",
          "<p>Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times.</p>",
          "<p>Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary Availability Zone active at all times.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Cloud Architecture & Design",
      "question_plain": "A media company is designing a disaster recovery (DR) solution for a business-critical application. The recovery time objective (RTO) should be 4 hours or less. The application is running on Amazon EC2 instances using the fewest possible AWS resources during normal operations.Which of the following is recommended to implement the DR solution across regions cost-effectively?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399228,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A media company has grown significantly in the past few months and the management team are concerned about compliance, governance, auditing, and security. The management team requires that configuration changes are tracked a history of API calls is recorded.</p><p>What should a solutions architect do to meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>As per definition of AWS CloudTrail and AWS Config:</p><p>CloudTrail is a web service that records AWS API calls for your AWS account and delivers log files to an Amazon S3 bucket. The recorded information includes the identity of the user, the start time of the AWS API call, the source IP address, the request parameters, and the response elements returned by the service.</p><p>AWS Config tracks changes in the configuration of your AWS resources, and it regularly sends updated configuration details to an Amazon S3 bucket that you specify. For each resource type that AWS Config records, it sends a configuration history file every six hours.</p><p><strong>CORRECT: </strong>\"Use AWS Config to track configuration changes and AWS CloudTrail to record API calls\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail to track configuration changes and AWS Config to record API calls \" is incorrect.</p><p>This option is the reverse of what’s needed, AWS config, as the name suggests, is used to track the configuration changes in AWS accounts.</p><p><strong>INCORRECT:</strong> \"Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls\" is incorrect. CloudWatch is used for performance monitoring, not tracking API calls.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls\" is incorrect. CloudTrail is not the right service for tracking configuration changes hence this option is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/APIReference/Welcome.html\">https://docs.aws.amazon.com/awscloudtrail/latest/APIReference/Welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/TrackingChanges.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/TrackingChanges.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p><p><a href=\"https://digitalcloud.training/aws-cloudtrail/\">https://digitalcloud.training/aws-cloudtrail/</a></p>",
        "answers": [
          "<p>Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.</p>",
          "<p>Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.</p>",
          "<p>Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.</p>",
          "<p>Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A media company has grown significantly in the past few months and the management team are concerned about compliance, governance, auditing, and security. The management team requires that configuration changes are tracked a history of API calls is recorded.What should a solutions architect do to meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399230,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A solutions architect in a large finance organization must restrict access for a specific S3 bucket to only users in accounts within the organization in AWS Organizations. This is due to the confidentiality of project reports data.</p><p>Which solution meets these requirements with the LEAST amount of operational overhead?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-13-34-246c03f69e39c1e07b1d3c6f767eb480.jpg\"><p>PrincipalOrgId is used by specifying the Principal element in a <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html\">resource-based policy</a>. You can specify the <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_details.html\">organization ID</a> in the condition element. When you add and remove accounts, policies that include the aws:PrincipalOrgID key automatically include the correct accounts and don't require manual updating.</p><p>For example, the following Amazon S3 bucket policy allows members of any account in the o-xxxxxxxxxxx organization to add an object into the policy-ninja-dev bucket.</p><p><strong>CORRECT: </strong>\"Add the aws:PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy\" is incorrect.</p><p>This condition key ensures that the requester is an account member within the specified organization root or organizational units (OUs) in AWS Organizations. It is not required for this solution.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly\" is incorrect. This option would be required for monitoring but not sharing access.</p><p><strong>INCORRECT:</strong> \"Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy\" is incorrect. Since question is around cross account access, this option wouldn’t work as is.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/control-access-to-aws-resources-by-using-the-aws-organization-of-iam-principals/\">https://aws.amazon.com/blogs/security/control-access-to-aws-resources-by-using-the-aws-organization-of-iam-principals/</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/iam-share-aws-resources-groups-aws-accounts-aws-organizations/\">https://aws.amazon.com/blogs/security/iam-share-aws-resources-groups-aws-accounts-aws-organizations/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
        "answers": [
          "<p>Add the aws:PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.</p>",
          "<p>Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.</p>",
          "<p>Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.</p>",
          "<p>Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Management & Governance",
      "question_plain": "A solutions architect in a large finance organization must restrict access for a specific S3 bucket to only users in accounts within the organization in AWS Organizations. This is due to the confidentiality of project reports data.Which solution meets these requirements with the LEAST amount of operational overhead?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399232,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A law firm has recently moved an on-premises multi-tier web application to AWS. Currently, the web application is based on a containerized solution and is running inside Linux based EC2 instances which connect to a PostgreSQL database hosted on separate but dedicated EC2 instances. The company wishes to optimize operational efficiency and performance.</p><p>Which combination of actions should the solutions architect take? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Amazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL. You already know how MySQL and PostgreSQL combine the speed and reliability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases. The code, tools, and applications you use today with your existing MySQL and PostgreSQL databases can be used with Aurora. With some workloads, Aurora can deliver up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.</p><p>Amazon ECS is a fully managed container orchestration service that makes it easy for you to deploy, manage, and scale containerized applications. This is a better hosting solution for a containerized solution rather than managing the underlying container platform yourself. In the case of Fargate, the solution is serverless, so it massively reduces operational overhead.</p><p><strong>CORRECT: </strong>\"Migrate the PostgreSQL database to Amazon Aurora and Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS)\" are the correct answers (as explained above)</p><p><strong>INCORRECT:</strong> \"Migrate the web application to the same Amazon EC2 instances as the database\" is incorrect. This might reduce cost but doesn’t offer any other advantages.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon CloudFront distribution for the web application content\" is incorrect. CloudFront helps with caching content globally for better performance but does not help reduce the operational overhead or performance of this solution.</p><p><strong>INCORRECT:</strong> \"Set up Amazon ElastiCache between the web application and the PostgreSQL database\" is incorrect. Caching will only help when you have hot data segments and does not reduce the operational overhead of this solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Updates.Versions.html#AuroraMySQL.Updates.UpgradePaths\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Updates.Versions.html#AuroraMySQL.Updates.UpgradePaths</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
        "answers": [
          "<p>Migrate the PostgreSQL database to Amazon Aurora.</p>",
          "<p>Migrate the web application to the same Amazon EC2 instances as the database.</p>",
          "<p>Set up an Amazon CloudFront distribution for the web application content.</p>",
          "<p>Set up Amazon ElastiCache between the web application and the PostgreSQL database.</p>",
          "<p>Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS).</p>"
        ]
      },
      "correct_response": ["a", "e"],
      "section": "AWS Database",
      "question_plain": "A law firm has recently moved an on-premises multi-tier web application to AWS. Currently, the web application is based on a containerized solution and is running inside Linux based EC2 instances which connect to a PostgreSQL database hosted on separate but dedicated EC2 instances. The company wishes to optimize operational efficiency and performance.Which combination of actions should the solutions architect take? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399234,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company has multiple Windows workloads which are .NET application servers and Microsoft SQL Server databases running on Amazon EC2 instances with Windows Server 2016. The company requires a shared file system which is highly available, durable and provides high levels of throughput and IOPS.</p><p>What is the best way to meet this requirement?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-16-52-e0b5162f6cd22832735038248d52fbf0.jpg\"><p>As a fully managed service, FSx for Windows File Server eliminates the administrative overhead of setting up and provisioning file servers and storage volumes. Additionally, Amazon FSx keeps Windows software up to date, detects and addresses hardware failures, and performs backups.</p><p>Amazon FSx also provides rich integration with other AWS services like <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html\">AWS IAM</a>, <a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_microsoft_ad.html\">AWS Directory Service for Microsoft Active Directory</a>, <a href=\"https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces.html\">Amazon WorkSpaces</a>, <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/overview.html\">AWS Key Management Service</a>, and <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html\">AWS CloudTrail</a>.</p><p><strong>CORRECT: </strong>\"Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate all the data to Amazon S3. Set up IAM authentication for users to access files\" is incorrect. Since the workload is Windows specific, S3 wouldn’t really help as an optimal solution though S3 can be still used to backup objects.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon S3 File Gateway, mount the S3 File Gateway on the existing EC2 instances\" is incorrect. Amazon S3 File Gateway provides a seamless way to connect to the cloud to store application data files and backup images as durable objects in Amazon S3 cloud storage with SMB or NFS-based access and local caching. However, this is a solution designed for on-premises servers, not EC2 instances and is not the best option for this scenario.</p><p><strong>INCORRECT:</strong> \"Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS\" is incorrect. EFS cannot be used with Microsoft workloads using the SMB protocol as it only supports Linux and NFS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-file-shares.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-file-shares.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
        "answers": [
          "<p>Migrate all the data to Amazon S3. Set up IAM authentication for users to access files.</p>",
          "<p>Set up an Amazon S3 File Gateway, mount the S3 File Gateway on the existing EC2 instances.</p>",
          "<p>Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.</p>",
          "<p>Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "A company has multiple Windows workloads which are .NET application servers and Microsoft SQL Server databases running on Amazon EC2 instances with Windows Server 2016. The company requires a shared file system which is highly available, durable and provides high levels of throughput and IOPS.What is the best way to meet this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399236,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A media company is running a production workload on thousands of EC2 instances which run a custom solution powered by third-party software. This software is subjected to regular updates and patches by the third-party organization.</p><p>How can a solutions architect patch all the instances quickly to remediate a security exposure?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-18-39-b8bddb759e545b93db5f1bcf39dbfd67.jpg\"><p>Patch Manager automates the process of patching Windows and Linux managed instances. Use this feature of AWS Systems Manager to scan your instances for missing patches or scan and install missing patches. You can install patches individually or to large groups of instances by using Amazon EC2 tags.</p><p><strong>CORRECT: </strong>\"Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function to apply the patch to all EC2 instances\" is incorrect. Since AWS already provides an out of the box solution of creating customizable patch groups enabling easy patching of EC2 instances, writing custom AWS Lambda is not the quickest/easiest solution.</p><p><strong>INCORRECT:</strong> \"Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances\" is incorrect. This is a valid option and would hold in case there’s a specific downtime or maintenance window when the patches are to be applied.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances\" is incorrect. This option wouldn’t work since the requirement is to have the patching done as quickly as possible and this would slow down the process.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/\">https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>",
        "answers": [
          "<p>Create an AWS Lambda function to apply the patch to all EC2 instances.</p>",
          "<p>Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.</p>",
          "<p>Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.</p>",
          "<p>Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Management & Governance",
      "question_plain": "A media company is running a production workload on thousands of EC2 instances which run a custom solution powered by third-party software. This software is subjected to regular updates and patches by the third-party organization.How can a solutions architect patch all the instances quickly to remediate a security exposure?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399238,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company stores its application logs in an Amazon CloudWatch Logs log group. A new policy requires the company to store all application logs in Amazon OpenSearch Service (Amazon Elasticsearch Service) in near-real time.</p><p>Which solution will meet this requirement with the LEAST operational overhead?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>You can configure a CloudWatch Logs log group to stream data it receives to your Amazon OpenSearch Service cluster in near real-time through a CloudWatch Logs subscription. This is the solution that requires the least operational overhead. Subscription filters can also be created for Kinesis, Kinesis Data Firehose, and AWS Lambda.</p><p><strong>CORRECT: </strong>\" Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service) \" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery stream's source. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the delivery stream's destination\" is incorrect.</p><p>This is a possible solution but requires more operational overhead as it includes an additional service which must also be configured and managed.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service)\" is incorrect. This would require more operational overhead as you must write and manage the code for the function yourself.</p><p><strong>INCORRECT:</strong> \"Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon Kinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service)\" is incorrect. Since the requirement is to dump the logs into OpenSearch and no further computation is needed, Firehose is a better candidate here.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p><p><a href=\"https://digitalcloud.training/amazon-opensearch/\">https://digitalcloud.training/amazon-opensearch/</a></p>",
        "answers": [
          "<p>Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).</p>",
          "<p>Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).</p>",
          "<p>Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery stream's source. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the delivery stream's destination.</p>",
          "<p>Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon Kinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Management & Governance",
      "question_plain": "A company stores its application logs in an Amazon CloudWatch Logs log group. A new policy requires the company to store all application logs in Amazon OpenSearch Service (Amazon Elasticsearch Service) in near-real time.Which solution will meet this requirement with the LEAST operational overhead?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399240,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A data analytics company is hosting a data lake which consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization for the latest dataset and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>If you have data in sources other than Amazon S3, you can use Athena Federated Query to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3. With Athena Federated Query, you can run SQL queries across data stored in relational, non-relational, object, and custom data sources.</p><p>Athena uses <em>data source connectors</em> that run on AWS Lambda to run federated queries. A data source connector is a piece of code that can translate between your target data source and Athena. You can think of a connector as an extension of Athena's query engine. Prebuilt Athena data source connectors exist for data sources like Amazon CloudWatch Logs, Amazon DynamoDB, Amazon DocumentDB, and Amazon RDS, and JDBC-compliant relational data sources such MySQL, and PostgreSQL under the Apache 2.0 license.</p><p><strong>CORRECT: </strong>\"Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles\" is incorrect.</p><p>This would have worked for one time data set which only needed visualization. For any new data, analysis would need to be performed again. Also, you connect user and groups in your QuickSight account but not IAM Roles.</p><p><strong>INCORRECT:</strong> \"Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups \" is incorrect.</p><p>As with the previous answer, this option solves the problem of access sharing with resources but does not take care of delta in data. Also, you connect user and groups in your QuickSight account but not IAM Roles.</p><p><strong>INCORRECT:</strong> \"Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports\" is incorrect.</p><p>Amazon Athena should be used with AWS Glue to provide the required functionality as described in the explanation above and the article linked below.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html\">https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-glue/\">https://digitalcloud.training/aws-glue/</a></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
        "answers": [
          "<p>Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.</p>",
          "<p>Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.</p>",
          "<p>Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.</p>",
          "<p>Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Database",
      "question_plain": "A data analytics company is hosting a data lake which consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization for the latest dataset and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399242,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company observed an increase in Amazon EC2 costs in its most recent bill. The billing team noticed unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions architect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause of the vertical scaling.</p><p>How should the solutions architect generate the information with the LEAST operational overhead?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Cost Explorer would be the easiest way to graph this data. Cost Explorer can be accessed easily and has features for filtering billing data and graphing across relevant time periods.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-24-24-b30229192dd6797b5632983864cfeb31.jpg\"><p><strong>CORRECT: </strong>\"Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Budgets to create a budget report and compare EC2 costs based on instance types\" is incorrect.</p><p>AWS Budgets lets you set custom cost and usage budgets that alert you when your budget thresholds are exceeded (or forecasted to be exceeded).</p><p><strong>INCORRECT:</strong> \"Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months\" is incorrect.</p><p>The granularity required is not available in the billing and cost management dashboard unless using the cost and usage report.</p><p><strong>INCORRECT:</strong> \"Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon QuickSight with Amazon S3 as a source to generate an interactive graph based on instance types\" is incorrect. This could provide the required graphs, but it involves much more operational overhead.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/aws-cost-management/aws-cost-explorer/\">https://aws.amazon.com/aws-cost-management/aws-cost-explorer/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cost-management/\">https://digitalcloud.training/aws-cost-management/</a></p>",
        "answers": [
          "<p>Use AWS Budgets to create a budget report and compare EC2 costs based on instance types.</p>",
          "<p>Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types.</p>",
          "<p>Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months.</p>",
          "<p>Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon QuickSight with Amazon S3 as a source to generate an interactive graph based on instance types.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "A company observed an increase in Amazon EC2 costs in its most recent bill. The billing team noticed unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions architect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause of the vertical scaling.How should the solutions architect generate the information with the LEAST operational overhead?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399244,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>As a security measure, a finance-based organization want to introduce additional security measures for an existing application deployed in AWS. The application is serverless and has an Amazon API Gateway in front which is deployed in the us-east-1 Region and the eu-west-1 Region. The company requires the accounts to be secured against SQL injection and cross-site scripting attacks.</p><p>Which solution will meet these requirements with the LEAST amount of administrative effort?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Firewall Manager simplifies your administration and maintenance tasks across multiple accounts and resources for a variety of protections, including AWS WAF, AWS Shield Advanced, Amazon VPC security groups, AWS Network Firewall, and Amazon Route 53 Resolver DNS Firewall. With Firewall Manager, you set up your protections just once and the service automatically applies them across your accounts and resources, even as you add new accounts and resources.</p><p>AWS WAF is used for protecting against malicious web attacks and is the best service to use to protect against SQL injection and cross-site scripting attacks. Used in combination with AWS Firewall Manager this solution protects both Regions and requires the least administrative effort.</p><p><strong>CORRECT: </strong>\"Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage\" is incorrect. This solution requires more administrative effort in rule management.</p><p><strong>INCORRECT:</strong> \"Set up AWS Shield in both Regions. Associate Regional web ACLs with an API stage\" is incorrect. The primary difference between AWS Shield and WAF is that while AWS WAF can mitigate DDoS attacks at layer 7 of the OSI reference model, AWS Shield protects web services from DDoS attacks at layer 3 and 4 of the OSI reference model. In this case AWS WAF should be used.</p><p><strong>INCORRECT:</strong> \"Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage\" is incorrect. As mentioned above, AWS Shield is not an appropriate choice for securing the accounts from SQL injection and cross-site scripting attacks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/fms-chapter.html\">https://docs.aws.amazon.com/waf/latest/developerguide/fms-chapter.html</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>",
        "answers": [
          "<p>Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage</p>",
          "<p>Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.</p>",
          "<p>Set up AWS Shield in both Regions. Associate Regional web ACLs with an API stage.</p>",
          "<p>Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "As a security measure, a finance-based organization want to introduce additional security measures for an existing application deployed in AWS. The application is serverless and has an Amazon API Gateway in front which is deployed in the us-east-1 Region and the eu-west-1 Region. The company requires the accounts to be secured against SQL injection and cross-site scripting attacks.Which solution will meet these requirements with the LEAST amount of administrative effort?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399246,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An e-commerce company has developed a new application which has been successfully deployed on AWS. For an upcoming sale, the company is expecting a huge rise in traffic and while testing for the event they have encountered performance issues in the application when many requests are sent to the application.</p><p>The current application stack is Amazon Aurora PostgreSQL database with an AWS Lambda compute layer fronted by API Gateway. A solutions architect must recommend improvements scalability whilst minimizing the configuration effort.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>With Amazon SQS, you can offload tasks from one component of your application by sending them to a queue and processing them asynchronously. Lambda polls the queue and invokes your Lambda function synchronously with an event that contains the message from the SQS queue. This solution improves scalability as the message bus decouples the processing components of the application meaning it is less likely that the application will suffer outages or lost data.</p><p><strong>CORRECT: </strong>\"Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the database by using native Java Database Connectivity (JDBC) drivers\" is incorrect. You cannot run Lambda code on Amazon EC2 instances.</p><p><strong>INCORRECT:</strong> \"Change the platform from Aurora to Amazon DynamoDB. Provision a DynamoDB Accelerator (DAX) cluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster\" is incorrect. Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB. The question doesn’t talk about hot or frequently accessed data only about an increase in volume so introducing DAX might not completely solve the issues.</p><p><strong>INCORRECT:</strong> \"Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using Amazon Simple Notification Service (Amazon SNS)\" is incorrect. SNS is used for fan-out scenarios when a single event is to be broadcasted among consumers and hence is not a good fit here.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
        "answers": [
          "<p>Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the database by using native Java Database Connectivity (JDBC) drivers.</p>",
          "<p>Change the platform from Aurora to Amazon DynamoDB. Provision a DynamoDB Accelerator (DAX) cluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster.</p>",
          "<p>Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using Amazon Simple Notification Service (Amazon SNS).</p>",
          "<p>Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Application Integration",
      "question_plain": "An e-commerce company has developed a new application which has been successfully deployed on AWS. For an upcoming sale, the company is expecting a huge rise in traffic and while testing for the event they have encountered performance issues in the application when many requests are sent to the application.The current application stack is Amazon Aurora PostgreSQL database with an AWS Lambda compute layer fronted by API Gateway. A solutions architect must recommend improvements scalability whilst minimizing the configuration effort.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399248,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A telemarketing company has developed customer call center functionality on AWS. The company plans to enhance the current application by enabling support for multiple speaker recognition and transcript generation. They also want to query the transcript files to analyze business patterns.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon Transcribe converts audio input into text, which opens the door for various text analytics applications on voice input. For instance, by using Amazon Comprehend on the converted text data from Amazon Transcribe, customers can perform sentiment analysis or extract entities and key phrases.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-39-19-22fe898d923b0e18e74ca7e8d8bc3f06.jpg\"><p><strong>CORRECT: </strong>\"Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use machine learning models for transcript file analysis\" is incorrect.</p><p>Amazon Rekognition Video can detect objects, scenes, faces, celebrities, text, and inappropriate content in videos. You can also search for faces appearing in a video using your own repository or collection of face images.</p><p><strong>INCORRECT:</strong> \"Use Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use SQL queries for transcript file analysis\" is incorrect.</p><p>Amazon Translate can provide automatic translation to enable cross-lingual communications between users for your applications.</p><p><strong>INCORRECT:</strong> \"Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use Amazon Textract for transcript file analysis\" is incorrect.</p><p>As mentioned above, Rekognition is better suited for identifying content in videos. Also,</p><p>Amazon Textract is a machine learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena/\">https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-machine-learning-services/\">https://digitalcloud.training/aws-machine-learning-services/</a></p>",
        "answers": [
          "<p>Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use machine learning models for transcript file analysis.</p>",
          "<p>Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis.</p>",
          "<p>Use Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use SQL queries for transcript file analysis.</p>",
          "<p>Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use Amazon Textract for transcript file analysis.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Machine Learning",
      "question_plain": "A telemarketing company has developed customer call center functionality on AWS. The company plans to enhance the current application by enabling support for multiple speaker recognition and transcript generation. They also want to query the transcript files to analyze business patterns.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399250,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A traffic law enforcement company is building a solution that has thousands of edge devices that collectively generate 1 TB of status alerts each day. These devices provide vehicle information and number plate data whenever alerts detecting red light jumps are detected. Each entry is around 2Kb in size. A solutions architect needs to implement a solution to ingest and store the alerts for future analysis.</p><p>The company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure. Additionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days.</p><p>What is the MOST operationally efficient solution that meets these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Data ingestion is a good use case for since it is scalable and can achieve the volumes required. Also, an S3 lifecycle configuration is appropriate for the requirement for data retention.</p><p><strong>CORRECT: </strong>\"Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days\" is incorrect. Provisioning additional EC2 instances means provisioning infrastructure, and the question states that the company wants to avoid this.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the kinesis Data Firehose stream to deliver the alerts to an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Set up the Amazon Open Search Service (Amazon Elasticsearch Service) cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days\" is incorrect. This option would mean provisioning ECS clusters and since the question is asking for archival of data, S3 is a better fit (data deletion is not desired).</p><p><strong>INCORRECT:</strong> \"Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts and set the message retention period to 14 days. Configure consumers to poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete the message from the SQS queue\" is incorrect.</p><p>With an SQS queue you must have processing components adding and retrieving messages from the queue and this means additional infrastructure to manage. With Kinesis Data Firehose the data is loaded straight to the destination without any need for additional infrastructure.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a></p><p><a href=\"https://aws.amazon.com/kinesis/data-firehose/features/\">https://aws.amazon.com/kinesis/data-firehose/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
        "answers": [
          "<p>Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.</p>",
          "<p>Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.</p>",
          "<p>Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the kinesis Data Firehose stream to deliver the alerts to an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Set up the Amazon Open Search Service (Amazon Elasticsearch Service) cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days.</p>",
          "<p>Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts and set the message retention period to 14 days. Configure consumers to poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete the message from the SQS queue.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Analytics",
      "question_plain": "A traffic law enforcement company is building a solution that has thousands of edge devices that collectively generate 1 TB of status alerts each day. These devices provide vehicle information and number plate data whenever alerts detecting red light jumps are detected. Each entry is around 2Kb in size. A solutions architect needs to implement a solution to ingest and store the alerts for future analysis.The company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure. Additionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days.What is the MOST operationally efficient solution that meets these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399252,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A three-tier web application is composed of a front end hosted on an Amazon EC2 instance in public subnet, application middleware hosted on EC2 in a private subnet and a database hosted on an Amazon RDS MySQL database in a private subnet. The database layer should be restricted to only allow incoming connections from the application.</p><p>Which of the following options makes sure that database can only be accessed by the application layer?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Security groups are stateful. All inbound traffic is blocked by default in custom security groups. If you create an inbound rule allowing traffic in, that traffic is automatically allowed back out again. You cannot block specific IP address using Security groups (instead use Network Access Control Lists).</p><p>In this case the solution is to allow inbound traffic from the security group ID of the security group attached to the application layer. The rule should specify the appropriate protocol and port. This will ensure only the application layer can communicate with the database.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-44-44-d7ca54a2bdcb21dfbe441fc3f6080f3a.jpg\"><p><strong>CORRECT: </strong>\"Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a new route table that excludes the route to the public subnets' CIDR blocks. Associate the route table with the database subnets\" is incorrect. This would simply stop routing from working within the VPC.</p><p><strong>INCORRECT:</strong> \"Create a security group that denies inbound traffic from the security group that is assigned to instances in the public subnets. Attach the security group to the DB instances\" is incorrect.</p><p>You cannot create deny rules with security groups.</p><p><strong>INCORRECT:</strong> \"Create a new peering connection between the public subnets and the private subnets. Create a different peering connection between the private subnets and the database subnets\" is incorrect.</p><p>Peering is used when multiple VPC’s are to be connected with each other hence this is also an incorrect option.</p><p><strong>References</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
        "answers": [
          "<p>Create a new route table that excludes the route to the public subnets' CIDR blocks. Associate the route table with the database subnets.</p>",
          "<p>Create a security group that denies inbound traffic from the security group that is assigned to instances in the public subnets. Attach the security group to the DB instances.</p>",
          "<p>Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances.</p>",
          "<p>Create a new peering connection between the public subnets and the private subnets. Create a different peering connection between the private subnets and the database subnets.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A three-tier web application is composed of a front end hosted on an Amazon EC2 instance in public subnet, application middleware hosted on EC2 in a private subnet and a database hosted on an Amazon RDS MySQL database in a private subnet. The database layer should be restricted to only allow incoming connections from the application.Which of the following options makes sure that database can only be accessed by the application layer?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399254,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A law firm has recently productionized a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database servers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from the AWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can accept IP packets.</p><p>A solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Gateway Load Balancers enable you to deploy, scale, and manage virtual appliances, such as firewalls, intrusion detection and prevention systems, and deep packet inspection systems. It combines a transparent network gateway (that is, a single entry and exit point for all traffic) and distributes traffic while scaling your virtual appliances with the demand.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-18_03-50-23-b89bd576b1370b29356a1ab3a41a3370.jpg\"><p><strong>CORRECT: </strong>\"Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection\" is incorrect.</p><p>Network load balancers work on Layer 4 of the OSI model and work on TCP, UDP and TLS protocols. They are not used for packet inspection.</p><p><strong>INCORRECT:</strong> \"Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection\" is incorrect.</p><p>Application load balancers work on Layer 7 and used with HTTP/HTTPS traffic. They are also not used for packet inspection.</p><p><strong>INCORRECT:</strong> \"Deploy a transit gateway in the inspection VPC. Configure route tables to route the incoming packets through the transit gateway\" is incorrect.</p><p>Transit Gateways are used for routing traffic and connecting networks and VPCs, they are not used for packet inspection purposes. In this case a load balancer is required to distributed connections to the virtual firewall appliances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/introduction.html</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection-using-aws-gateway-load-balancer/\">https://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection-using-aws-gateway-load-balancer/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
        "answers": [
          "<p>Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.</p>",
          "<p>Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.</p>",
          "<p>Deploy a transit gateway in the inspection VPC. Configure route tables to route the incoming packets through the transit gateway.</p>",
          "<p>Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Compute",
      "question_plain": "A law firm has recently productionized a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database servers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from the AWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can accept IP packets.A solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server.Which solution will meet these requirements with the LEAST operational overhead?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399256,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A finance organization has bootstrapped a golden image for their in-house application and the resultant AMI is to be shared across various AWS accounts as a base image. This image is to be used across many applications. The company needs to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 CreateImage API operation is called within the company's account.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>You can create an Amazon EventBridge rule that triggers on an action by an AWS service that does not emit events. In this case you can base the rule on API calls made by AWS CloudTrail. The rule can trigger when the Amazon EC2 CreateImage API is called. The rule can then trigger another service or action.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule for the CreateImage API call. Configure the target as an Amazon SNS topic to send an alert when a Createlmage API call is detected\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS CloudTrail with an Amazon SNS notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on CreateImage when an API call is detected\" is incorrect.</p><p>Athena is a query analysis tool hence this option is incorrect.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a CreateImage API call is detected\" is incorrect.</p><p>Since the question asks about least operational overhead, this option becomes incorrect. This is an achievable solution but involves building custom code in Lambda and requires more effort.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon SQS FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon SNS topic when a CreateImage API call is detected\" is incorrect.</p><p>You cannot configure CloudTrail logs to be sent directly to an SQS queue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-log-api-call.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-log-api-call.html</a></p>",
        "answers": [
          "<p>Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a CreateImage API call is detected.</p>",
          "<p>Configure AWS CloudTrail with an Amazon SNS notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on CreateImage when an API call is detected</p>",
          "<p>Create an Amazon EventBridge rule for the CreateImage API call. Configure the target as an Amazon SNS topic to send an alert when a Createlmage API call is detected.</p>",
          "<p>Configure an Amazon SQS FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon SNS topic when a CreateImage API call is detected.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A finance organization has bootstrapped a golden image for their in-house application and the resultant AMI is to be shared across various AWS accounts as a base image. This image is to be used across many applications. The company needs to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 CreateImage API operation is called within the company's account.Which solution will meet these requirements with the LEAST operational overhead?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399258,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect needs to create a file system that can be concurrently accessed by multiple Amazon EC2 instances across multiple availability zones. The file system needs to support high throughput and the ability to burst. As the data that will be stored on the file system will be sensitive, it must be encrypted at rest and in transit.</p><p>Which storage solution should the Solutions Architect use for the shared file system?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>EFS is a fully-managed service that makes it easy to set up and scale file storage in the Amazon Cloud. EFS file systems are mounted using the NFSv4.1 protocol. EFS is designed to burst to allow high throughput levels for periods of time. EFS also offers the ability to encrypt data at rest and in transit.</p><p><strong>CORRECT: </strong>\"Use the Elastic File System (EFS) and mount the file system using NFS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Add EBS volumes to each EC2 instance and configure data replication\" is incorrect. Adding EBS volumes to each instance and configuring data replication is not the best solution for this scenario and there is no native capability within AWS for performing the replication. Some 3rd party data management software does use this model, however.</p><p><strong>INCORRECT:</strong> \"Use the Elastic Block Store (EBS) and mount the file system at the block level\" is incorrect. EBS is a block-level storage system not a file-level storage system. You cannot mount EBS volumes from multiple instances across AZs.</p><p><strong>INCORRECT:</strong> \"Add EBS volumes to each EC2 instance and use an ELB to distribute data evenly between the volumes\" is incorrect. You cannot use an ELB to distribute data between EBS volumes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html\">https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
        "answers": [
          "<p>Add EBS volumes to each EC2 instance and configure data replication</p>",
          "<p>Use the Elastic Block Store (EBS) and mount the file system at the block level</p>",
          "<p>Use the Elastic File System (EFS) and mount the file system using NFS</p>",
          "<p>Add EBS volumes to each EC2 instance and use an ELB to distribute data evenly between the volumes</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "A Solutions Architect needs to create a file system that can be concurrently accessed by multiple Amazon EC2 instances across multiple availability zones. The file system needs to support high throughput and the ability to burst. As the data that will be stored on the file system will be sensitive, it must be encrypted at rest and in transit.Which storage solution should the Solutions Architect use for the shared file system?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399260,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A finance organization wants to deploy end of day processing applications to a fleet of Amazon EC2 instances with a focus on reducing cost. These applications are stateless and can be re-triggered in case of failure. The company needs a solution that minimizes cost and operational overhead.</p><p>What should a solutions architect do to meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Since by using EC2 Spot Instances, customers can access additional compute capacity between 70%-90% off On-Demand Instance pricing, we can directly eliminate two options utilizing on demand instances.</p><p>Among the two options with spot instances, since the application is stateless, the better idea is to have a containerized approach and utilize EKS to reduce operational overhead.</p><p><strong>CORRECT: </strong>\"Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers\" is incorrect. As mentioned above, EKS gives you more options towards application fleet orchestration which makes it a better choice.</p><p><strong>INCORRECT:</strong> \"Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers\" is incorrect.</p><p>As compared to spot instances, on demand instances are costlier and for end of day processing where failures can be re-triggered and are acceptable, spot instances are a better choice.</p><p><strong>INCORRECT:</strong> \"Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group\" is incorrect.</p><p>As compared to spot instances, on demand instances are more expensive and for end of day processing where failures can be re-triggered and are acceptable, spot instances are a better choice.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/best-practices-for-handling-ec2-spot-instance-interruptions/\">https://aws.amazon.com/blogs/compute/best-practices-for-handling-ec2-spot-instance-interruptions/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
        "answers": [
          "<p>Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.</p>",
          "<p>Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.</p>",
          "<p>Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers.</p>",
          "<p>Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "A finance organization wants to deploy end of day processing applications to a fleet of Amazon EC2 instances with a focus on reducing cost. These applications are stateless and can be re-triggered in case of failure. The company needs a solution that minimizes cost and operational overhead.What should a solutions architect do to meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399262,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>To trace a recent production incident a product manager needs to view logs in the Amazon CloudWatch logs. These logs are linked to events over the course of a week and may be needed in the future if incidents occur again. The product manager doesn’t have administrative access to the AWS account as it is managed by a third-party management company.</p><p>According to principal of least privilege, which option out of the below will fulfill the requirement to provide the necessary access for the product manager?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Below is the sequence for sharing the dashboard from Cloud watch console.</p><p>CloudWatch &gt; Dashboard &gt; Select your board &gt; Share Dashboard&gt;Share your dashboard and require a username and password&gt;Enter mail address</p><p>You can share your CloudWatch dashboards with people who do not have direct access to your AWS account. This enables you to share dashboards across teams, with stakeholders, and with people external to your organization. You can even display dashboards on big screens in team areas or embed them in Wikis and other webpages.</p><p><strong>CORRECT: </strong>\"Share the dashboard from the CloudWatch console. Enter the product manager's email address and complete the sharing steps. Provide a shareable link for the dashboard to the product manager\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentials with the product manager. Share the browser URL of the correct dashboard with the product manager\" is incorrect.</p><p>If the dashboard needs to be shared with additional users, this option increases manual effort every time and hence is not an optimal option.</p><p><strong>INCORRECT:</strong> \"Create an IAM user for the company's employees. Attach the ViewOnly Access AWS managed policy to the IAM user. Share the new login credentials with the product manager. Ask the product manager to navigate to the CloudWatch console and locate the dashboard by name in the Dashboards section\" is incorrect.</p><p>This option also involves lot of manual steps and as the recipients for the dashboard increase in number, manual effort increase and hence this is not an optimal option.</p><p><strong>INCORRECT:</strong> \"Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard\" is incorrect.</p><p>Exposing bastion server isn’t required here for sharing the dashboard. Bastion servers are meant to be jump boxes to allow accesses to EC2 instances which isn’t the ask in the question hence this is also an incorrect option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
        "answers": [
          "<p>Share the dashboard from the CloudWatch console. Enter the client’s email address and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.</p>",
          "<p>Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentials with the product manager. Share the browser URL of the correct dashboard with the product manager.</p>",
          "<p>Create an IAM user for the company's employees. Attach the ViewOnly Access AWS managed policy to the IAM user. Share the new login credentials with the product manager. Ask the product manager to navigate to the CloudWatch console and locate the dashboard by name in the Dashboards section.</p>",
          "<p>Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Management & Governance",
      "question_plain": "To trace a recent production incident a product manager needs to view logs in the Amazon CloudWatch logs. These logs are linked to events over the course of a week and may be needed in the future if incidents occur again. The product manager doesn’t have administrative access to the AWS account as it is managed by a third-party management company.According to principal of least privilege, which option out of the below will fulfill the requirement to provide the necessary access for the product manager?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399264,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the production environment. The software that accesses this data requires consistently high I/O performance.</p><p>A solutions architect needs to minimize the time that is required to clone the production data into the test environment.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all their provisioned performance.</p><p><strong>CORRECT: </strong>\"Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the test environment\" is incorrect. You cannot restore EBS snapshots to instance store volumes. Instance store volumes are ephemeral storage volumes and are not used for data that requires persistence.</p><p><strong>INCORRECT:</strong> “Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots\" is incorrect.</p><p>This solution may take longer and may not have the consistent performance that is offered with the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the test environment\" is incorrect.</p><p>Amazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD ( io1 or io2 ) volume to multiple instances that are in the same Availability Zone. You can attach multiple Multi-Attach enabled volumes to an instance or set of instances. This does not help with the requirements of this solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
        "answers": [
          "<p>Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the test environment.</p>",
          "<p>Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the test environment.</p>",
          "<p>Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots.</p>",
          "<p>Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Storage",
      "question_plain": "A company wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the production environment. The software that accesses this data requires consistently high I/O performance.A solutions architect needs to minimize the time that is required to clone the production data into the test environment.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399266,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A financial services company has a large, multi-Region footprint on AWS. A recent security audit highlighted some issues that must be addressed. The company must track all configuration changes affecting AWS resources and have detailed records of who has accessed the AWS environment. The data should include information such as which user has logged in and which API calls they made</p><p>What actions should a Solutions Architect take to meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Config is a service used to track and remediation any unauthorized configuration changes made with your AWS Account. AWS Config could be used in this example with AWS AWS CloudTrail which keeps detailed logs of all API calls made within the account such as who logged in, which AWS Identity and Access Management (IAM) role is being used and also how they interact with the AWS Cloud.</p><p><strong>CORRECT: </strong>\"Use AWS Config to track configuration changes and AWS CloudTrail to record API calls and track access patterns in the AWS Cloud\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch to track configuration changes and AWS Config to record API calls and track access patterns in the AWS Cloud\" is incorrect. Amazon CloudWatch does not make track configuration changes, it tracks performance metrics and AWS Config does not track API calls, it tracks configuration changes.</p><p><strong>INCORRECT:</strong> \"Use AWS Config to track configuration changes and Amazon EventBridge to record API calls and track access patterns in the AWS Cloud\" is incorrect. Although AWS Config would work in this scenario, <em>Amazon EventBridge</em> is a serverless event bus used to build event-driven- architectures so it cannot be used for tracking API calls.</p><p><strong>INCORRECT:</strong> \"Use Amazon Macie to track configuration changes and Amazon CloudTrail to record API calls and track access patterns in the AWS Cloud\" is incorrect. Amazon Macie is used with Amazon S3 to detect sensitive PII data, which has nothing to do with tracking configuration changes.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/config\">https://aws.amazon.com/config</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p>",
        "answers": [
          "<p>Use Amazon CloudWatch to track configuration changes and AWS Config to record API calls and track access patterns in the AWS Cloud.</p>",
          "<p>Use AWS Config to track configuration changes and AWS CloudTrail to record API calls and track access patterns in the AWS Cloud.</p>",
          "<p>Use AWS Config to track configuration changes and Amazon EventBridge to record API calls and track access patterns in the AWS Cloud.</p>",
          "<p>Use Amazon Macie to track configuration changes and Amazon CloudTrail to record API calls and track access patterns in the AWS Cloud.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Management & Governance",
      "question_plain": "A financial services company has a large, multi-Region footprint on AWS. A recent security audit highlighted some issues that must be addressed. The company must track all configuration changes affecting AWS resources and have detailed records of who has accessed the AWS environment. The data should include information such as which user has logged in and which API calls they madeWhat actions should a Solutions Architect take to meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399268,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A large company is currently using multiple AWS accounts as part of its cloud deployment model, and these accounts are currently structured using AWS Organizations. A Solutions Architect has been tasked with limiting access to an Amazon S3 bucket to only users of accounts that are enrolled with AWS Organizations. The Solutions Architect wants to avoid listing the many dozens of account IDs in the Bucket policy, as there are many accounts the frequent changes.</p><p>Which strategy meets these requirements with the LEAST amount of effort?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The aws:PrincipalOrgID global key provides a simpler alternative to manually listing and updating all the account IDs for all AWS accounts that exist within an Organization. The following Amazon S3 bucket policy allows members of any account in the ‘123456789’ organization to add an object into the ‘mydctbucket’ bucket.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-24_20-46-05-1c8b6106074b932bcc68ed1a200779f1.jpg\"><p><strong>CORRECT: </strong>\"Use the global key of AWS Organizations within a bucket policy using the aws:PrincipalOrgID key to allow access only to accounts which are part of the Organization\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Attribute Based Access Control by referencing Tags of accounts which are either enrolled as part of AWS Organizations, or not\" is incorrect. This could be a viable option, however maintaining an accurate tagging policy as opposed to referencing the PrincipalOrgID would much more difficult.</p><p><strong>INCORRECT:</strong> \"Use AWS Config and AWS Lambda functions to make remediations to the bucket policy as and when new accounts are created and tagged as not being part of AWS Organizations. Update the S3 bucket policy accordingly\" is incorrect.</p><p>Every time an account is added or removed from the organization this workflow would have to fire. This solution would need to be built and maintained, whereas it is much easier to refer to the PrincipalOrgID once and avoid needing to change the Bucket Policy.</p><p><strong>INCORRECT:</strong> \"Add all the non-organizational accounts to an Organizational Unit (OU) and attached a Service Control Policy (SCP) which denies access to the specific Amazon S3 bucket\" is incorrect. You can only use Organization Units (OUs) and Service Control Policies (SCPs) with accounts that are a part of AWS Organizations – meaning this solution could not work.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
        "answers": [
          "<p>Use Attribute Based Access Control by referencing Tags of accounts which are either enrolled as part of AWS Organizations, or not.</p>",
          "<p>Use the global key of AWS Organizations within a bucket policy using the aws:PrincipalOrgID key to allow access only to accounts which are part of the Organization.</p>",
          "<p>Use AWS Config and AWS Lambda functions to make remediations to the bucket policy as and when new accounts are created and tagged as not being part of AWS Organizations. Update the S3 bucket policy accordingly.</p>",
          "<p>Add all the non-organizational accounts to an Organizational Unit (OU) and attached a Service Control Policy (SCP) which denies access to the specific Amazon S3 bucket.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Management & Governance",
      "question_plain": "A large company is currently using multiple AWS accounts as part of its cloud deployment model, and these accounts are currently structured using AWS Organizations. A Solutions Architect has been tasked with limiting access to an Amazon S3 bucket to only users of accounts that are enrolled with AWS Organizations. The Solutions Architect wants to avoid listing the many dozens of account IDs in the Bucket policy, as there are many accounts the frequent changes.Which strategy meets these requirements with the LEAST amount of effort?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399270,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A global financial services company is currently operating a three-tier web application to handle their main customer facing website. This application uses several Amazon EC2 instances behind an Application Load Balancer and connects directly to a DynamoDB table.</p><p>Due to recent customer complaints of slow loading times, their Solutions Architect has been asked to implement changes to solve this problem, without rearchitecting the core application components.</p><p><br></p><p>Which combination of actions should the solutions architect take to accomplish this? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>A CloudFront distribution would cache content in one of the many global edge locations, ensuring that any customer access to the content will be accessing it at a much lower latency compared to using the Application Load Balancer on its own.</p><p>Secondly, DynamoDB has a built-in caching solution known as DynamoDB Accelerator (DAX). If your application is serving traffic from a DynamoDB database and is struggling to scale, you can use the DynamoDB cache to improve application.</p><p><strong>CORRECT: </strong>\"Create a CloudFront distribution and place it in front of the Application Load Balancer\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Set up an Amazon DynamoDB Accelerator (DAX) cluster in front of the DynamoDB table\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the entire application stack to AWS Elastic Beanstalk with both web server and worker environments\" is incorrect.</p><p>Migrating the entire application to AWS Elastic Beanstalk would require rearchitecting and would not necessarily improve the latency of the application for end users.</p><p><strong>INCORRECT:</strong> \"Migrate the DynamoDB database to Amazon Aurora with a multi-AZ deployment model\" is incorrect.</p><p>Refactoring the application to move from a No-SQL database (DynamoDB) to a SQL database (Amazon Aurora) would take a significant amount of application and code changes, due to the fundamental differences between SQL and NoSQL databases.</p><p><strong>INCORRECT:</strong> \"Migrate the web application to be hosted on a containerized solution using AWS Fargate\" is incorrect.</p><p>The application does not currently use containers, and instead uses Amazon EC2 instances. Changing the application to using a containerized compute layer would also require architectural changes and would not be suitable for this use case.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
        "answers": [
          "<p>Migrate the DynamoDB database to Amazon Aurora with a multi-AZ deployment model.</p>",
          "<p>Migrate the entire application stack to AWS Elastic Beanstalk with both web server and worker environments.</p>",
          "<p>Create a CloudFront distribution and place it in front of the Application Load Balancer.</p>",
          "<p>Set up an Amazon DynamoDB Accelerator (DAX) cluster in front of the DynamoDB table.</p>",
          "<p>Migrate the web application to be hosted on a containerized solution using AWS Fargate.</p>"
        ]
      },
      "correct_response": ["c", "d"],
      "section": "AWS Database",
      "question_plain": "A global financial services company is currently operating a three-tier web application to handle their main customer facing website. This application uses several Amazon EC2 instances behind an Application Load Balancer and connects directly to a DynamoDB table.Due to recent customer complaints of slow loading times, their Solutions Architect has been asked to implement changes to solve this problem, without rearchitecting the core application components.Which combination of actions should the solutions architect take to accomplish this? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399272,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company uses several Windows Servers as the operating system of choice for all their application servers hosted in their data center. The company wants to move some file servers into the cloud, and keep some in their data center, mounted to the same File System. The company also wants to maintain extremely low latency access to their on-premises data center, across a private network. The company has an AWS Direct Connect connection set up into the us-east-1 Region.</p><p>What should a solutions architect do to meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The current AWS Direct connect connection will provide the ability to share a file system between on-premises servers and Amazon EC2 instances in the AWS Cloud. Direct Connect provides low latency access to their on-premises data center, and the company’s use of Windows File Servers necessitates the use of an SMB-based Amazon FSx File System.</p><p><strong>CORRECT: </strong>\"Install an SMB client on to the on-premises servers and mount an Amazon FSx file system to the servers. Mount the same file system to the EC2 instances within the Amazon VPC. Use the existing Direct Connect connection to connect the on-premises data center to the Amazon VPC\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate all the data to Amazon DynamoDB Local. Ensure all users have the appropriate IAM permissions to access the relevant files\" is incorrect. This will not give the company the use of a Windows File Server, and instead give them a NoSQL database. DynamoDB Local is not suitable for this use case.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 on Outposts and mount the S3 File Gateway on to the on-premises servers\" is incorrect. Amazon S3 on Outposts would not provide a hybrid cloud experience as required by the customer, and S3 File Gateway uses a Linux based file system, which is incompatible with the Windows setup the company currently uses.</p><p><strong>INCORRECT:</strong> \"Install an NFS client on to the on-premises servers and mount an Amazon EFS file system to the servers. Mount the same file system to the EC2 instances within the Amazon VPC. Use the existing Direct Connect connection to connect the on-premises data center to the Amazon VPC\" is incorrect.</p><p>Amazon EFS is a file system that is accessed using the NFS protocol and is suitable for Linux clients only. This is not natively supported for Window Servers, making this an unsuitable option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/efs-onpremises.html\">https://docs.aws.amazon.com/efs/latest/ug/efs-onpremises.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
        "answers": [
          "<p>Migrate all the data to Amazon DynamoDB Local. Ensure all users have the appropriate IAM permissions to access the relevant files.</p>",
          "<p>Use Amazon S3 on Outposts and mount the S3 File Gateway on to the on-premises servers.</p>",
          "<p>Install an SMB client on to the on-premises servers and mount an Amazon FSx file system to the servers. Mount the same file system to the EC2 instances within the Amazon VPC. Use the existing Direct Connect connection to connect the on-premises data center to the Amazon VPC.</p>",
          "<p>Install an NFS client on to the on-premises servers and mount an Amazon EFS file system to the servers. Mount the same file system to the EC2 instances within the Amazon VPC. Use the existing Direct Connect connection to connect the on-premises data center to the Amazon VPC.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "A company uses several Windows Servers as the operating system of choice for all their application servers hosted in their data center. The company wants to move some file servers into the cloud, and keep some in their data center, mounted to the same File System. The company also wants to maintain extremely low latency access to their on-premises data center, across a private network. The company has an AWS Direct Connect connection set up into the us-east-1 Region.What should a solutions architect do to meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399274,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A financial services company is currently using 500 Amazon EC2 instances to run batch-processing workloads to analyze financial information on a periodic basis. The organization needs to install a third-party tool on all these instances as quickly and as efficiently as possible and will have to carry out similar tasks on an ongoing basis going forward. The solution also needs to scale for the addition of future EC2 instances.</p><p>What should a solutions architect do to meet these requirements in the easiest way possible?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Systems Manager Run command is designed to run commands across a large group of instances without having to SSH into all your instances and run the same command multiple times. You can easily run the same command to all the managed nodes as part of the workload, without having to maintain access keys or individual access for each instance.</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Run Command to run a custom command that installs the tool on all the EC2 instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda Function which will make configuration changes to all of the EC2 instances. Validate the tool has been installed using another Lambda function\" is incorrect. Whilst this may be possible, the code that would be required to create and test this solution would be difficult to design and would not scale effectively as AWS Systems Manager Run Command.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Patch Manager to install the tool on all of the EC2 instances within a single patch\" is incorrect. AWS Systems Manager Patch Manager is designed to apply patches to EC2 instances and is not designed to run commands across a large group of instances.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Maintenance Windows to install the tool on all of the EC2 instances within a set period of time\" is incorrect. AWS Systems Manager Maintenance Windows is designed to select a defined window of time in which you EC2 instances will be patched and is not capable of running commands across multiple instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>",
        "answers": [
          "<p>Create an AWS Lambda Function which will make configuration changes to all the EC2 instances. Validate the tool has been installed using another Lambda function.</p>",
          "<p>Use AWS Systems Manager Patch Manager to install the tool on all the EC2 instances within a single patch.</p>",
          "<p>Use AWS Systems Manager Maintenance Windows to install the tool on all the EC2 instances within a set period of time.</p>",
          "<p>Use AWS Systems Manager Run Command to run a custom command that installs the tool on all the EC2 instances.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Management & Governance",
      "question_plain": "A financial services company is currently using 500 Amazon EC2 instances to run batch-processing workloads to analyze financial information on a periodic basis. The organization needs to install a third-party tool on all these instances as quickly and as efficiently as possible and will have to carry out similar tasks on an ongoing basis going forward. The solution also needs to scale for the addition of future EC2 instances.What should a solutions architect do to meet these requirements in the easiest way possible?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399276,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An e-commerce company operates a containerized microservices application on a fleet of Amazon EC2 instances. As part of their infrastructure improvement efforts, the company plans to migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) for enhanced scalability and management.</p><p>As part of the security protocol, the company has configured the Amazon EKS control plane with endpoint private access enabled and public access disabled. The data plane resides within private subnets. However, the company faces an issue where nodes fail to join the cluster.</p><p>What can be done to allow the nodes to join the EKS cluster?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>When the EKS control plane is configured with private access, and the nodes are in a private subnet, you need to create VPC endpoints for Amazon EKS and ECR. This allows the nodes to communicate with the EKS control plane and pull container images from ECR.</p><p><strong>CORRECT: </strong>\"Set up VPC endpoints for Amazon EKS and ECR to enable nodes to communicate with the control plane\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Modify the associated IAM role to include permissions to the AmazonEKSClusterPolicy\" is incorrect.</p><p>IAM roles are crucial for setting up permissions, but simply modifying the associated IAM role would not solve the issue of nodes not being able to connect to the control plane.</p><p><strong>INCORRECT:</strong> \"Establish VPC peering connection for nodes to access the control plane\" is incorrect.</p><p>VPC peering is not the recommended way to allow nodes in a private subnet to access the EKS control plane. This approach might also incur additional operational overhead.</p><p><strong>INCORRECT:</strong> \"Move nodes to public subnet and configure security group rules for the EC2 nodes\" is incorrect.</p><p>Moving the nodes to public subnets contradicts the original requirement of having the data plane in private subnets. Additionally, this approach might introduce unnecessary security risks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/private-clusters.html\">https://docs.aws.amazon.com/eks/latest/userguide/private-clusters.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
        "answers": [
          "<p>Modify the associated IAM role to include permissions to the AmazonEKSClusterPolicy.</p>",
          "<p>Establish VPC peering connection for nodes to access the control plane.</p>",
          "<p>Move nodes to public subnet and configure security group rules for the EC2 nodes.</p>",
          "<p>Set up VPC endpoints for Amazon EKS and ECR to enable nodes to communicate with the control plane.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Compute",
      "question_plain": "An e-commerce company operates a containerized microservices application on a fleet of Amazon EC2 instances. As part of their infrastructure improvement efforts, the company plans to migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) for enhanced scalability and management.As part of the security protocol, the company has configured the Amazon EKS control plane with endpoint private access enabled and public access disabled. The data plane resides within private subnets. However, the company faces an issue where nodes fail to join the cluster.What can be done to allow the nodes to join the EKS cluster?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399278,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A digital marketing agency manages numerous client websites and apps on AWS. Each AWS resource is supposed to be tagged by the account for tracking and backup purposes. The agency wants to ensure that all AWS resources, including untagged ones, are backed up properly to minimize data loss risks.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>This solution is the most operationally efficient due to the powerful combination of AWS Config and AWS Backup.</p><p><strong>AWS Config</strong>: This service enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. You can use AWS Config to review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. In this scenario, AWS Config can be utilized to identify all resources that lack proper tags.</p><p><strong>Tagging</strong>: Tags can be added to AWS resources programmatically. By tagging resources, you organize them into groups and subgroups, which can be based on purpose, owner, environment, or other criteria. In this context, tagging resources allows AWS Backup to identify and group resources that need to be backed up.</p><p><strong>AWS Backup</strong>: AWS Backup is a fully managed backup service that makes it easy to centralize and automate the back up of data across AWS services. You can use AWS Backup to protect several AWS resource types, including Amazon EBS volumes, Amazon RDS databases, Amazon DynamoDB tables, Amazon EFS file systems, and AWS Storage Gateway volumes. It offers a centralized dashboard where you can manage all backups and allows you to automate and monitor backups across AWS services using policies.</p><p>With AWS Config identifying and tagging untagged resources, and AWS Backup automating the backup of tagged resources, this solution requires minimal operational overhead while ensuring all resources are adequately backed up.</p><p><strong>CORRECT: </strong>\"Use AWS Config to identify all untagged resources and tag them programmatically. Then, use AWS Backup to automate the backup of all AWS resources based on tags\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Manually search for all untagged resources in each AWS service. Once identified, tag them appropriately and set up AWS Backup for each service separately\" is incorrect.</p><p>Searching for untagged resources manually in each service and setting up AWS Backup separately for each one would require a significant amount of operational overhead.</p><p><strong>INCORRECT:</strong> \"Rely on each account owner to identify their untagged resources and then use AWS Backup for backing up\" is incorrect.</p><p>Relying on individual account owners could result in inconsistency and increase the risk of missed resources or backups. Centralized backup management using AWS Backup is more efficient.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda to periodically scan for untagged resources, add necessary tags, and then set up AWS Backup\" is incorrect.</p><p>Although AWS Lambda could be used to scan for untagged resources and add necessary tags, this would require developing and maintaining a custom script. AWS Config can handle this process with less operational overhead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/aws-backup/latest/devguide/assigning-resources.html\">https://docs.aws.amazon.com/aws-backup/latest/devguide/assigning-resources.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p>",
        "answers": [
          "<p>Use AWS Config to identify all untagged resources and tag them programmatically. Then, use AWS Backup to automate the backup of all AWS resources based on tags.</p>",
          "<p>Manually search for all untagged resources in each AWS service. Once identified, tag them appropriately and set up AWS Backup for each service separately.</p>",
          "<p>Rely on each account owner to identify their untagged resources and then use AWS Backup for backing up.</p>",
          "<p>Use AWS Lambda to periodically scan for untagged resources, add necessary tags, and then set up AWS Backup.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Management & Governance",
      "question_plain": "A digital marketing agency manages numerous client websites and apps on AWS. Each AWS resource is supposed to be tagged by the account for tracking and backup purposes. The agency wants to ensure that all AWS resources, including untagged ones, are backed up properly to minimize data loss risks.Which solution will meet these requirements with the LEAST operational overhead?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399280,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An international logistics company has web applications running on AWS in the us-west-2 Region and database servers in the eu-central-1 Region. The applications running in a VPC in us-west-2 need to communicate securely with the databases running in a VPC in eu-central-1.</p><p>Which network design will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The correct solution establishes a VPC peering connection between the two regions, and it properly sets up the inbound rule in the eu-central-1 database security group to allow traffic from the us-west-2 application server IP addresses, which is the correct way to configure this as security groups can't be referenced across regions.</p><p><strong>CORRECT: </strong>\"Configure a VPC peering connection between the us-west-2 VPC and the eu-central-1 VPC. Update the subnet route tables accordingly. Create an inbound rule in the eu-central-1 database security group that allows traffic from the us-west-2 application server IP addresses\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Establish a VPC peering connection between the us-west-2 VPC and the eu-central-1 VPC. Modify the subnet route tables accordingly. Create an inbound rule in the eu-central-1 database security group that references the security group ID of the application servers in us-west-2\" is incorrect.</p><p>You cannot reference a security group from another region. Security groups are region-specific and can only be referenced within the same region.</p><p><strong>INCORRECT:</strong> \"Create a VPC peering connection between the us-west-2 VPC and the eu-central-1 VPC. Add the appropriate routes to the subnet route tables. Create an inbound rule in the us-west-2 application security group that allows traffic from the eu-central-1 database server IP addresses\" is incorrect.</p><p>In this scenario, we want to allow traffic from the application servers in us-west-2 to the database servers in eu-central-1. The inbound rule should be configured in the eu-central-1 database security group to allow this traffic.</p><p><strong>INCORRECT:</strong> \"Establish a transit gateway with a peering attachment between the us-west-2 VPC and the eu-central-1 VPC. After the transit gateways are properly peered and routing is configured, create an inbound rule in the eu-central-1 database security group that references the security group ID of the application servers in us-west-2\" is incorrect.</p><p>You cannot reference a security group from another region. Security groups are region-specific and can only be referenced within the same region.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html\">https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>Establish a VPC peering connection between the us-west-2 VPC and the eu-central-1 VPC. Modify the subnet route tables accordingly. Create an inbound rule in the eu-central-1 database security group that references the security group ID of the application servers in us-west-2.</p>",
          "<p>Configure a VPC peering connection between the us-west-2 VPC and the eu-central-1 VPC. Update the subnet route tables accordingly. Create an inbound rule in the eu-central-1 database security group that allows traffic from the us-west-2 application server IP addresses.</p>",
          "<p>Create a VPC peering connection between the us-west-2 VPC and the eu-central-1 VPC. Add the appropriate routes to the subnet route tables. Create an inbound rule in the us-west-2 application security group that allows traffic from the eu-central-1 database server IP addresses.</p>",
          "<p>Establish a transit gateway with a peering attachment between the us-west-2 VPC and the eu-central-1 VPC. After the transit gateways are properly peered and routing is configured, create an inbound rule in the eu-central-1 database security group that references the security group ID of the application servers in us-west-2.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "An international logistics company has web applications running on AWS in the us-west-2 Region and database servers in the eu-central-1 Region. The applications running in a VPC in us-west-2 need to communicate securely with the databases running in a VPC in eu-central-1.Which network design will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399282,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A global logistics company collects shipment tracking information, which updates every few seconds. The company wishes to perform real-time analysis on these data updates to monitor shipment progress and predict delays, after which they want the data to be ingested into their Amazon S3-based data lake. Which solution will fulfill these requirements with the MOST operational efficiency?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon Kinesis Data Firehose is ideal for ingesting high-velocity data into AWS, like the shipment tracking data in this scenario. It can capture, transform, and load streaming data into data lakes on S3. Kinesis Data Analytics can then analyze this data in real-time, making this the most operationally efficient solution.</p><p><strong>CORRECT: </strong>\"Use Amazon Kinesis Data Firehose for data ingestion and Amazon Kinesis Data Analytics for real-time analysis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams for data ingestion and AWS Lambda for real-time data analysis\" is incorrect.</p><p>Kinesis Data Streams can handle real-time data ingestion and Lambda can perform real-time processing, but this approach requires managing the stream consumers (like AWS Lambda) and ensuring they are scaled properly. This may not be the most operationally efficient solution.</p><p><strong>INCORRECT:</strong> \"Use AWS Direct Connect for data ingestion and Amazon Athena for real-time analysis\" is incorrect.</p><p>AWS Direct Connect is a networking service primarily for establishing dedicated network connections from on-premises to AWS, not typically used for high-velocity data ingestion. Amazon Athena is more suitable for ad-hoc querying on S3 data, not real-time analysis.</p><p><strong>INCORRECT:</strong> \"Use Amazon SQS for data ingestion and Amazon EMR for real-time analysis\" is incorrect.</p><p>Amazon SQS is capable of handling high-throughput workloads, but it's more suited to decoupling and scaling microservices, distributed systems, and serverless applications. Amazon EMR is a managed cluster platform that simplifies running big data frameworks, but it is not suitable for real-time data analysis.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-analytics/\">https://aws.amazon.com/kinesis/data-analytics/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
        "answers": [
          "<p>Use Amazon Kinesis Data Streams for data ingestion and AWS Lambda for real-time data analysis.</p>",
          "<p>Use Amazon Kinesis Data Firehose for data ingestion and Amazon Kinesis Data Analytics for real-time analysis.</p>",
          "<p>Use AWS Direct Connect for data ingestion and Amazon Athena for real-time analysis.</p>",
          "<p>Use Amazon SQS for data ingestion and Amazon EMR for real-time analysis.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Analytics",
      "question_plain": "A global logistics company collects shipment tracking information, which updates every few seconds. The company wishes to perform real-time analysis on these data updates to monitor shipment progress and predict delays, after which they want the data to be ingested into their Amazon S3-based data lake. Which solution will fulfill these requirements with the MOST operational efficiency?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399284,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A multinational enterprise plans to transition from numerous independent AWS accounts to a structured, multi-account AWS setup. The enterprise anticipates creating multiple AWS accounts to cater to various departments. The enterprise seeks to authenticate access to these AWS accounts using a centralized corporate directory service.</p><p>What combination of steps should a solutions architect suggest to meet these needs? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>AWS Organizations provides policy-based management for multiple AWS accounts. With Organizations, you can create member accounts that are part of your organization and centrally manage your accounts.</p><p>AWS Directory Service allows you to connect your AWS resources with an existing on-premises Microsoft Active Directory or to set up a new, stand-alone directory in the AWS Cloud. AWS Identity Center makes it easy to centrally manage access to multiple AWS accounts and business applications and provide users with single sign-on access to all their assigned accounts and applications from one place.</p><p><strong>CORRECT: </strong>\"Create a new AWS Organizations entity with all features enabled. Create the new AWS accounts within the organization\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Deploy AWS Directory Service and integrate it with the corporate directory service. Set up AWS Identity Center for authentication across accounts\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install and configure AWS Control Tower for centralized account management. Incorporate AWS Identity Center to manage identity\" is incorrect.</p><p>AWS Control Tower does have certain benefits, but it doesn't directly cater to the company's need for centralized corporate directory service integration. However, it could be used in conjunction with AWS Identity Center for user access management.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon Cognito identity pool and configure AWS Identity Center to accept Amazon Cognito authentication\" is incorrect.</p><p>Amazon Cognito is primarily used to add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. It isn't typically used in multi-account management scenarios and isn't directly relevant to the requirement for corporate directory service integration.</p><p><strong>INCORRECT:</strong> \"Establish an AWS Transit Gateway for centralized network management, linking AWS accounts\" is incorrect.</p><p>AWS Transit Gateway connects VPCs and on-premises networks through a central hub. It is a network transit hub, not a user authentication and management service. It doesn't directly address the need for centralized corporate directory service integration.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/manage-your-identity-source-ad.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/manage-your-identity-source-ad.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
        "answers": [
          "<p>Create a new AWS Organizations entity with all features enabled. Create the new AWS accounts within the organization.</p>",
          "<p>Install and configure AWS Control Tower for centralized account management. Incorporate AWS Identity Center to manage identity.</p>",
          "<p>Deploy AWS Directory Service and integrate it with the corporate directory service. Set up AWS Identity Center for authentication across accounts.</p>",
          "<p>Set up an Amazon Cognito identity pool and configure AWS Identity Center to accept Amazon Cognito authentication.</p>",
          "<p>Establish an AWS Transit Gateway for centralized network management, linking AWS accounts.</p>"
        ]
      },
      "correct_response": ["a", "c"],
      "section": "AWS Management & Governance",
      "question_plain": "A multinational enterprise plans to transition from numerous independent AWS accounts to a structured, multi-account AWS setup. The enterprise anticipates creating multiple AWS accounts to cater to various departments. The enterprise seeks to authenticate access to these AWS accounts using a centralized corporate directory service.What combination of steps should a solutions architect suggest to meet these needs? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399286,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A software firm is developing a microservices-based application to be deployed on Amazon ECS. This application needs to interact with a resilient, shared filesystem capable of restoring data to a different AWS Region with a Recovery Point Objective (RPO) of 2 hours.</p><p>The filesystem is also expected to provide a mount target in each Availability Zone (AZ) within a Region. The solutions architect intends to employ AWS Backup to oversee the cross-Region data replication.</p><p>Which option will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon Elastic File System (Amazon EFS) provides simple, scalable file storage for use with Amazon EC2 instances and other AWS services. It supports the Network File System (NFS) protocol and can be configured with mount points in multiple AZs.</p><p>EFS can be used with AWS Backup for automated and centralized backup across AWS services, and supports replication to another region, satisfying the given requirement.</p><p>All replication traffic stays on the AWS global backbone, and most changes are replicated within a minute, with an overall Recovery Point Objective (RPO) of 15 minutes for most file systems.</p><p><strong>CORRECT: </strong>\"Amazon Elastic File System (Amazon EFS) with the Standard storage class\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Amazon FSx for Windows File Server with a Multi-AZ deployment\" is incorrect.</p><p>Amazon FSx for Windows File Server provides fully managed, reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server and offers a native Windows file system experience for Windows-based applications. However, it doesn't support cross-Region data replication, making it unsuitable for the given requirement.</p><p><strong>INCORRECT:</strong> \"Amazon FSx for NetApp ONTAP with a Multi-AZ deployment\" is incorrect.</p><p>Amazon FSx for NetApp ONTAP is more suitable to scenarios where you want to migrate applications using ONTAP software and if you’re already using NetApp systems. It is better in this case to use Amazon EFS.</p><p><strong>INCORRECT:</strong> \"Amazon FSx for OpenZFS\" is incorrect.</p><p>Amazon FSx for OpenZFS is suitable if you have a specific requirement to use OpenZFS. For this use case Amazon EFS is a better choice and will likely be more cost-effective.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/efs-replication.html\">https://docs.aws.amazon.com/efs/latest/ug/efs-replication.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
        "answers": [
          "<p>Amazon FSx for Windows File Server with a Multi-AZ deployment.</p>",
          "<p>Amazon FSx for NetApp ONTAP with a Multi-AZ deployment.</p>",
          "<p>Amazon Elastic File System (Amazon EFS) with the Standard storage class.</p>",
          "<p>Amazon FSx for OpenZFS.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "A software firm is developing a microservices-based application to be deployed on Amazon ECS. This application needs to interact with a resilient, shared filesystem capable of restoring data to a different AWS Region with a Recovery Point Objective (RPO) of 2 hours.The filesystem is also expected to provide a mount target in each Availability Zone (AZ) within a Region. The solutions architect intends to employ AWS Backup to oversee the cross-Region data replication.Which option will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399288,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An online game platform company is launching a new game feature that involves a significant update to their existing API hosted on Amazon API Gateway. The company wants to minimize the impact on their existing users, and they need a deployment strategy that allows them to gradually roll out the changes while monitoring for any potential issues.</p><p>What should the company do to achieve this?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The correct answer is to use Amazon API Gateway's canary release deployments. This allows the company to gradually roll out the new API version, initially exposing only a small percentage of their users to the new API. As they monitor the system and confirm that the new API is working as expected, they can increase the percentage of traffic directed to the new version.</p><p><strong>CORRECT: </strong>\"Use an API Gateway canary release deployment. Initially direct a small percentage of user traffic to the new API version. After API verification, promote the canary stage to the production stage\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Update the existing API directly in API Gateway with the new feature and immediately direct all traffic to the updated API\" is incorrect.</p><p>Updating the existing API directly in API Gateway and immediately redirecting all traffic to the updated API is risky. If there are any issues with the new API, it could negatively impact all users, rather than just a small subset of users.</p><p><strong>INCORRECT:</strong> \"Create a completely new API for the new game feature and redirect half of the user traffic to the new API while maintaining the other half on the existing API\" is incorrect.</p><p>Creating a completely new API and redirecting half the user traffic to the new API is not a gradual rollout strategy. This approach would immediately expose many users to potential issues with the new API.</p><p><strong>INCORRECT:</strong> \"Create a new version of the API and use Route 53 to gradually shift DNS queries from the existing API endpoint to the new API endpoint\" is incorrect.</p><p>Using Route 53 to gradually shift DNS queries from the existing API endpoint to the new API endpoint could work, but it is not as simple or efficient as using API Gateway's canary release deployments. DNS changes can also take time to propagate, potentially leading to inconsistent behavior for users.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
        "answers": [
          "<p>Use an API Gateway canary release deployment. Initially direct a small percentage of user traffic to the new API version. After API verification, promote the canary stage to the production stage.</p>",
          "<p>Update the existing API directly in API Gateway with the new feature and immediately direct all traffic to the updated API.</p>",
          "<p>Create a completely new API for the new game feature and redirect half of the user traffic to the new API while maintaining the other half on the existing API.</p>",
          "<p>Create a new version of the API and use Route 53 to gradually shift DNS queries from the existing API endpoint to the new API endpoint.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "An online game platform company is launching a new game feature that involves a significant update to their existing API hosted on Amazon API Gateway. The company wants to minimize the impact on their existing users, and they need a deployment strategy that allows them to gradually roll out the changes while monitoring for any potential issues.What should the company do to achieve this?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399290,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A software development firm uses AWS to run their compute instances across multiple accounts. These instances are individually billed. The company recently purchased an EC2 Reserved Instance (RI) for an ongoing project. However, due to the completion of that project, a significant number of EC2 instances were decommissioned. The company now wishes to utilize the benefits of their unused Reserved Instance across their other AWS accounts.</p><p>Which combination of steps should the company follow to achieve this? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Just like the Savings Plans, the benefits of Reserved Instances can be applied across accounts if those accounts are part of the same AWS Organization and if sharing is enabled. This can be achieved by enabling Reserved Instance sharing in the AWS Management Console for the account that purchased the RI.</p><p>Setting up an AWS Organization from the account that purchased the Reserved Instance allows you to group your accounts. After the organization is set up, you can invite other accounts to join the organization, enabling you to share the benefits of the Reserved Instance across all accounts in the organization.</p><p><strong>CORRECT: </strong>\"Enable Reserved Instance sharing in the billing preferences section of the AWS Management Console for the account that purchased the existing RI\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Establish an AWS Organization in the AWS account that purchased the RI and hosts the remaining active EC2 instances. Invite the other AWS accounts to join this organization from the management account\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"From the AWS Organizations management account, utilize AWS Resource Access Manager (AWS RAM) to share the Reserved Instance with other accounts\" is incorrect.</p><p>AWS RAM does not apply to Reserved Instances. It is used to share other resources like Subnets, Transit Gateways, etc.</p><p><strong>INCORRECT:</strong> \"Enable Reserved Instance sharing in the billing preferences section of the AWS Management Console for the management account\" is incorrect.</p><p>Reserved Instance sharing needs to be enabled in the account that purchased the RI, not the management account.</p><p><strong>INCORRECT:</strong> \"Use AWS Organizations to establish a new payer account and invite the other accounts to join this organization\" is incorrect.</p><p>Creating a new payer account is not necessary. It would be more efficient to use the existing account that purchased the Reserved Instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
        "answers": [
          "<p>Enable Reserved Instance sharing in the billing preferences section of the AWS Management Console for the account that purchased the existing RI.</p>",
          "<p>Establish an AWS Organization in the AWS account that purchased the RI and hosts the remaining active EC2 instances. Invite the other AWS accounts to join this organization from the management account.</p>",
          "<p>From the AWS Organizations management account, utilize AWS Resource Access Manager (AWS RAM) to share the Reserved Instance with other accounts.</p>",
          "<p>Enable Reserved Instance sharing in the billing preferences section of the AWS Management Console for the management account.</p>",
          "<p>Use AWS Organizations to establish a new payer account and invite the other accounts to join this organization.</p>"
        ]
      },
      "correct_response": ["a", "b"],
      "section": "AWS Management & Governance",
      "question_plain": "A software development firm uses AWS to run their compute instances across multiple accounts. These instances are individually billed. The company recently purchased an EC2 Reserved Instance (RI) for an ongoing project. However, due to the completion of that project, a significant number of EC2 instances were decommissioned. The company now wishes to utilize the benefits of their unused Reserved Instance across their other AWS accounts.Which combination of steps should the company follow to achieve this? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399292,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An online education platform uses Amazon CloudFront to distribute learning resources globally. The company wants to ensure that only enrolled students have access to the course materials. These materials are stored in an Amazon S3 bucket. In addition, the company occasionally provides exclusive resources to certain students for research and project work.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>CloudFront signed cookies are a method to control who can access your content. When a user authenticates and is verified as an enrolled student, the application can set a cookie in the student's browser. The cookie contains the same information that can be included in a signed URL but applies to multiple files in one or multiple directories.</p><p><strong>CORRECT: </strong>\"Implement CloudFront signed cookies for authenticated students\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create and provide S3 pre-signed URLs to authenticated students\" is incorrect.</p><p>S3 pre-signed URLs are used to grant temporary access to a specific S3 object. This could be a valid option for individual file access but would be less efficient for multiple files or directories.</p><p><strong>INCORRECT:</strong> \"Utilize Amazon S3 object-level encryption for course materials\" is incorrect.</p><p>Amazon S3 object-level encryption is mainly about securing data at rest, it won't control who can or cannot access the content.</p><p><strong>INCORRECT:</strong> \"Implement CloudFront Field-Level Encryption to block access to non-enrolled students\" is incorrect.</p><p>CloudFront Field-Level Encryption handles sensitive information in HTTP POST requests to help prevent the information from being seen by unauthorized viewers. It's not designed to control access to content.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
        "answers": [
          "<p>Create and provide S3 pre-signed URLs to authenticated students.</p>",
          "<p>Utilize Amazon S3 object-level encryption for course materials.</p>",
          "<p>Implement CloudFront Field-Level Encryption to block access to non-enrolled students.</p>",
          "<p>Implement CloudFront signed cookies for authenticated students.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "An online education platform uses Amazon CloudFront to distribute learning resources globally. The company wants to ensure that only enrolled students have access to the course materials. These materials are stored in an Amazon S3 bucket. In addition, the company occasionally provides exclusive resources to certain students for research and project work.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399294,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A game development company is planning to build a cloud-based game platform on AWS. The player activity patterns are unpredictable and could remain idle for extended periods. Only players who have purchased the game should have the ability to log in and play.</p><p>Which combination of steps will meet these requirements MOST cost-effectively? (Select THREE.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", "", ""],
        "explanation": "<p>AWS Lambda is a cost-effective solution for unpredictable traffic patterns due to its pay-per-use pricing model. DynamoDB is also a cost-effective and highly scalable solution for storing user data. The API Gateway provides a HTTP-based endpoint that can be used to expose the Lambda function.</p><p>AWS Cognito User Pools provide user directory features including sign-up and sign-in services, which are suitable for managing game user authentication.</p><p>AWS Amplify simplifies the process of hosting web applications with automated deployment processes. It also integrates with CloudFront, providing a global content delivery network to efficiently serve the game interface.</p><p><strong>CORRECT: </strong>\"Implement an AWS Lambda function to fetch player information from Amazon DynamoDB. Establish an Amazon API Gateway endpoint to handle RESTful API calls, directing them to the Lambda function\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Cognito User Pools to handle user authentication\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Leverage AWS Amplify to serve the frontend game interface with HTML, CSS, and JS. Use the integrated Amazon CloudFront configuration for distribution\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Set up an Amazon Elastic Container Service (Amazon ECS) service behind an Application Load Balancer to fetch player information from Amazon RDS. Establish an Amazon API Gateway endpoint to handle RESTful API calls, directing them to the ECS service\" is incorrect.</p><p>Using Amazon ECS might be an overkill for this scenario and might not be as cost-effective compared to Lambda and DynamoDB, especially for unpredictable and possibly idle traffic.</p><p><strong>INCORRECT:</strong> \"Use AWS Cognito Identity Pools to handle user authentication\" is incorrect.</p><p>Cognito Identity Pools are used for granting access to AWS resources rather than handling user authentication.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 static web hosting with HTML, CSS, and JS. Use Amazon CloudFront to distribute the frontend game interface\" is incorrect.</p><p>While you could host a static website on S3 and use CloudFront for distribution, AWS Amplify can provide additional capabilities tailored to modern web applications. Furthermore, Amplify's automated deployment processes can provide a more streamlined and efficient approach to managing the game's frontend compared to managing separate S3 and CloudFront configurations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p><p><a href=\"https://aws.amazon.com/amplify/\">https://aws.amazon.com/amplify/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
        "answers": [
          "<p>Implement an AWS Lambda function to fetch player information from Amazon DynamoDB. Establish an Amazon API Gateway endpoint to handle RESTful API calls, directing them to the Lambda function.</p>",
          "<p>Set up an Amazon Elastic Container Service (Amazon ECS) service behind an Application Load Balancer to fetch player information from Amazon RDS. Establish an Amazon API Gateway endpoint to handle RESTful API calls, directing them to the ECS service.</p>",
          "<p>Use AWS Cognito User Pools to handle user authentication.</p>",
          "<p>Use AWS Cognito Identity Pools to handle user authentication.</p>",
          "<p>Leverage AWS Amplify to serve the frontend game interface with HTML, CSS, and JS. Use the integrated Amazon CloudFront configuration for distribution.</p>",
          "<p>Use Amazon S3 static web hosting with HTML, CSS, and JS. Use Amazon CloudFront to distribute the frontend game interface.</p>"
        ]
      },
      "correct_response": ["a", "c", "e"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A game development company is planning to build a cloud-based game platform on AWS. The player activity patterns are unpredictable and could remain idle for extended periods. Only players who have purchased the game should have the ability to log in and play.Which combination of steps will meet these requirements MOST cost-effectively? (Select THREE.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399296,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company has an on-premises server that uses a MySQL database to process and store customer information. The company wants to migrate to an AWS database service to achieve higher availability and to improve application performance. Additionally, the company wants to offload reporting workloads from its primary database to ensure it remains performant.</p><p>Which solution will meet these requirements in the MOST operationally efficient way?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon Aurora with MySQL compatibility is a good fit for achieving high availability and improved performance. Aurora automatically distributes the data across multiple AZs in a single region. Additionally, Aurora allows the creation of up to 15 Aurora Replicas that share the same underlying volume as the primary instance. Directing reporting functions to the Aurora Replica is an effective way to offload reporting workloads from the primary database.</p><p><strong>CORRECT: </strong>\"Use Amazon Aurora with MySQL compatibility. Direct the reporting functions to use one of the Aurora Replicas\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon RDS with MySQL in a Single-AZ deployment. Create a read replica in the same availability zone as the primary DB instance. Direct the reporting functions to the read replica\" is incorrect.</p><p>Though you can use Amazon RDS with MySQL in a Single-AZ deployment and create a read replica, it is not the most operationally efficient option as it does not provide the high availability that Aurora's architecture offers.</p><p><strong>INCORRECT:</strong> \"Use AWS Database Migration Service (AWS DMS) to create an Amazon Aurora DB cluster in multiple AWS Regions. Point the reporting functions toward a separate DB instance from the primary DB instance\" is incorrect.</p><p>Using AWS DMS to create Amazon Aurora DB clusters in multiple AWS Regions would be overkill for the requirements. It could also introduce additional complexity and doesn’t specifically address using a replica for reporting purposes.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 instances to deploy a self-managed MySQL database with a replication setup for reporting purposes. Place instances in multiple availability zones and manage backups and patching manually\" is incorrect.</p><p>Managing your own database on Amazon EC2 instances requires a significant operational overhead as you need to handle backups, patch management, and high availability yourself. This option is not the most operationally efficient compared to using a managed database service like Amazon Aurora.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
        "answers": [
          "<p>Use Amazon RDS with MySQL in a Single-AZ deployment. Create a read replica in the same availability zone as the primary DB instance. Direct the reporting functions to the read replica.</p>",
          "<p>Use AWS Database Migration Service (AWS DMS) to create an Amazon Aurora DB cluster in multiple AWS Regions. Point the reporting functions toward a separate DB instance from the primary DB instance.</p>",
          "<p>Use Amazon Aurora with MySQL compatibility. Direct the reporting functions to use one of the Aurora Replicas.</p>",
          "<p>Use Amazon EC2 instances to deploy a self-managed MySQL database with a replication setup for reporting purposes. Place instances in multiple availability zones and manage backups and patching manually.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Database",
      "question_plain": "A company has an on-premises server that uses a MySQL database to process and store customer information. The company wants to migrate to an AWS database service to achieve higher availability and to improve application performance. Additionally, the company wants to offload reporting workloads from its primary database to ensure it remains performant.Which solution will meet these requirements in the MOST operationally efficient way?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399298,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A financial firm is aiming to leverage AWS Cloud for augmenting its on-premises disaster recovery (DR) architecture. The firm's main application, running on PostgreSQL, is housed on a virtual machine (VM) on-premises. The DR solution needs to align with the application's recovery point objective (RPO) of less than a minute and a recovery time objective (RTO) of within two hours, all while keeping costs to a minimum.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Configuring a warm standby Amazon RDS for PostgreSQL database on AWS and using AWS DMS with change data capture will meet the RTO and RPO requirements. DMS can handle the ongoing replication from the on-premises PostgreSQL to the standby RDS instance, providing a near real-time replica of the data.</p><p>In a DR scenario, this standby instance can be promoted to become the new primary database, meeting the required RTO and RPO.</p><p><strong>CORRECT: </strong>\"Set up a warm standby Amazon RDS for PostgreSQL database on AWS. Configure AWS Database Migration Service (AWS DMS) to use change data capture (CDC)\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure an active-active multi-site setup between the on-premises server and AWS using PostgreSQL with a third-party high availability solution\" is incorrect.</p><p>Setting up an active-active multi-site setup between the on-premises server and AWS using PostgreSQL with a third-party high availability solution might meet the RPO and RTO requirements, but it would likely be more expensive and complex than the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS Elastic Disaster Recovery with continuous replication to act as a pilot light solution on AWS\" is incorrect.</p><p>AWS Elastic Disaster Recovery with continuous replication can provide a DR solution, but for a database, it is typically more efficient to use a service designed for that purpose, like RDS with DMS.</p><p><strong>INCORRECT:</strong> \"Utilize third-party backup software to perform daily backups and store a secondary set of backups in Amazon S3\" is incorrect.</p><p>Using third-party backup software to perform daily backups and storing a secondary set of backups in Amazon S3 would not meet the RPO of less than a minute, as this approach could lead to a data loss up to 24 hours. Also, the process of restoring from a backup might not meet the RTO of within two hours.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
        "answers": [
          "<p>Configure an active-active multi-site setup between the on-premises server and AWS using PostgreSQL with a third-party high availability solution.</p>",
          "<p>Set up a warm standby Amazon RDS for PostgreSQL database on AWS. Configure AWS Database Migration Service (AWS DMS) to use change data capture (CDC).</p>",
          "<p>Use AWS Elastic Disaster Recovery with continuous replication to act as a pilot light solution on AWS.</p>",
          "<p>Utilize third-party backup software to perform daily backups and store a secondary set of backups in Amazon S3.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Database",
      "question_plain": "A financial firm is aiming to leverage AWS Cloud for augmenting its on-premises disaster recovery (DR) architecture. The firm's main application, running on PostgreSQL, is housed on a virtual machine (VM) on-premises. The DR solution needs to align with the application's recovery point objective (RPO) of less than a minute and a recovery time objective (RTO) of within two hours, all while keeping costs to a minimum.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399300,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A multinational podcast company uses Amazon CloudFront for distributing its digital content. The company wants to gradually introduce content across various regions. It also needs to ensure that listeners who are outside the regions to which the content is currently released, cannot access the content.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>By setting geographical restrictions on CloudFront content using a deny list, the company can block access to content for users outside of the released regions. If a user from a blocked region attempts to access the content, they would receive the custom error message, thereby meeting the company's requirements.</p><p><strong>CORRECT: </strong>\"Implement geographical restrictions on CloudFront content using a deny list and create a custom error message\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Establish a new URL for the restricted content, control access with signed URLs and cookies, and set up a custom error message\" is incorrect.</p><p>While signed URLs and cookies can be used to control access to content, they don't inherently consider the geographical location of the users, thus it would not guarantee that only users in the released regions could access the content.</p><p><strong>INCORRECT:</strong> \"Encrypt the company's distributed content data and establish a custom error message\" is incorrect.</p><p>Although encrypting the content data adds a layer of security, it does not restrict access based on the geographical location of the users.</p><p><strong>INCORRECT:</strong> \"Create a new URL for the restricted content and establish an expiration date-based access policy for signed URLs\" is incorrect.</p><p>Time-based access policies with signed URLs can limit access to the content after a certain time, but it does not restrict access based on the geographical location of the users.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
        "answers": [
          "<p>Implement geographical restrictions on CloudFront content using a deny list and create a custom error message.</p>",
          "<p>Establish a new URL for the restricted content, control access with signed URLs and cookies, and set up a custom error message.</p>",
          "<p>Encrypt the company's distributed content data and establish a custom error message.</p>",
          "<p>Create a new URL for the restricted content and establish an expiration date-based access policy for signed URLs.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A multinational podcast company uses Amazon CloudFront for distributing its digital content. The company wants to gradually introduce content across various regions. It also needs to ensure that listeners who are outside the regions to which the content is currently released, cannot access the content.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399302,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A health tech company runs a multi-tier medical records application in the AWS Cloud, which operates across three Availability Zones. The application architecture includes an Application Load Balancer, a cluster of Amazon EC2 instances that handle user session states, and a PostgreSQL database running on an EC2 instance.</p><p>The company anticipates a sharp surge in application traffic due to a new partnership. The company needs to scale to accommodate future application capacity demands and ensure high availability across all three Availability Zones.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>This solution fulfills all the requirements. Amazon RDS with Multi-AZ instances provides high availability and failover support for DB instances. ElastiCache for Redis supports storing session state data and can provide sub-millisecond response times, enabling applications to achieve instant, high-speed reads and writes. Auto Scaling ensures that the application has the correct number of Amazon EC2 instances to handle the load for your application.</p><p><strong>CORRECT: </strong>\"Migrate the PostgreSQL database to Amazon RDS for PostgreSQL with a Multi-AZ DB instance deployment. Use Amazon ElastiCache for Redis with a replication group to manage session data and cache reads. Migrate the application server to an Auto Scaling group across three Availability Zones\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the PostgreSQL database to Amazon Aurora with PostgreSQL compatibility with a single AZ deployment. Use Amazon ElastiCache for Memcached to manage session data and cache reads. Migrate the application server to an Auto Scaling group across three Availability Zones\" is incorrect.</p><p>Aurora is a high-performance database service, but using a single AZ deployment doesn't provide the high availability across multiple AZs the company wants. Also, while ElastiCache for Memcached can be used for caching, it doesn't offer the durability and atomicity that Redis offers, which is particularly useful for session data.</p><p><strong>INCORRECT:</strong> \"Migrate the PostgreSQL database to Amazon DynamoDB. Use DynamoDB Accelerator (DAX) to cache reads. Store the session data in DynamoDB. Migrate the application server to an Auto Scaling group across three Availability Zones\" is incorrect.</p><p>Although DynamoDB is a high-performance, scalable NoSQL database, it is not a drop-in replacement for a relational database like PostgreSQL. It has a different data model and supports a different set of query options. This change could require significant modifications to the application code and may not support the same transactional capabilities as PostgreSQL.</p><p><strong>INCORRECT:</strong> \"Keep the PostgreSQL database on EC2 instance. Use Amazon ElastiCache for Redis to manage session data and cache reads. Migrate the application server to an Auto Scaling group across three Availability Zones\" is incorrect.</p><p>Although the EC2 instance can run the PostgreSQL database, it does not provide the same level of managed service benefits (like automatic patching, backups, and high availability with Multi-AZ deployments) as Amazon RDS. This option would likely result in higher operational overhead and doesn't fully utilize the benefits of managed AWS services.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p><p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p><p><a href=\"https://aws.amazon.com/ec2/autoscaling/\">https://aws.amazon.com/ec2/autoscaling/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
        "answers": [
          "<p>Migrate the PostgreSQL database to Amazon RDS for PostgreSQL with a Multi-AZ DB instance deployment. Use Amazon ElastiCache for Redis with a replication group to manage session data and cache reads. Migrate the application server to an Auto Scaling group across three Availability Zones.</p>",
          "<p>Migrate the PostgreSQL database to Amazon Aurora with PostgreSQL compatibility with a single AZ deployment. Use Amazon ElastiCache for Memcached to manage session data and cache reads. Migrate the application server to an Auto Scaling group across three Availability Zones.</p>",
          "<p>Migrate the PostgreSQL database to Amazon DynamoDB. Use DynamoDB Accelerator (DAX) to cache reads. Store the session data in DynamoDB. Migrate the application server to an Auto Scaling group across three Availability Zones.</p>",
          "<p>Keep the PostgreSQL database on EC2 instance. Use Amazon ElastiCache for Redis to manage session data and cache reads. Migrate the application server to an Auto Scaling group across three Availability Zones.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Database",
      "question_plain": "A health tech company runs a multi-tier medical records application in the AWS Cloud, which operates across three Availability Zones. The application architecture includes an Application Load Balancer, a cluster of Amazon EC2 instances that handle user session states, and a PostgreSQL database running on an EC2 instance.The company anticipates a sharp surge in application traffic due to a new partnership. The company needs to scale to accommodate future application capacity demands and ensure high availability across all three Availability Zones.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399304,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A software development company is creating a microservices-based application using Amazon Elastic Kubernetes Service (Amazon EKS). The company needs to ensure that sensitive configuration data like database credentials and API keys stored in Kubernetes ConfigMaps and Secrets are encrypted at rest.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon EKS supports using AWS KMS keys for envelope encryption of Kubernetes secrets. To meet the requirement of encrypting Kubernetes Secrets at rest, we can use a customer managed AWS KMS key and enable secrets encryption while creating or updating an EKS cluster.</p><p><strong>CORRECT: </strong>\"Create a new AWS Key Management Service (AWS KMS) key. Enable Amazon EKS KMS secrets encryption on the Amazon EKS cluster\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement AWS Secrets Manager to manage, rotate, and store all sensitive data. Integrate it with the Amazon EKS cluster\" is incorrect.</p><p>While AWS Secrets Manager can store and manage sensitive information, it doesn't directly encrypt Kubernetes Secrets and ConfigMaps stored in the etcd key-value store.</p><p><strong>INCORRECT:</strong> \"Create the Amazon EKS cluster with default options. Use the Amazon Elastic File System (Amazon EFS) Container Storage Interface (CSI) driver as an add-on\" is incorrect.</p><p>The EFS CSI driver enables Kubernetes pods to mount EFS file systems, but it does not offer a mechanism for encrypting secrets stored in Kubernetes.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 to store all sensitive data. Enable server-side encryption with a new AWS Key Management Service (AWS KMS) key\" is incorrect.</p><p>While S3 can store sensitive data and encrypt it using KMS, it does not provide a way to directly encrypt Kubernetes ConfigMaps and Secrets stored in etcd.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/enable-kms.html\">https://docs.aws.amazon.com/eks/latest/userguide/enable-kms.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
        "answers": [
          "<p>Implement AWS Secrets Manager to manage, rotate, and store all sensitive data. Integrate it with the Amazon EKS cluster.</p>",
          "<p>Create a new AWS Key Management Service (AWS KMS) key. Enable Amazon EKS KMS secrets encryption on the Amazon EKS cluster.</p>",
          "<p>Create the Amazon EKS cluster with default options. Use the Amazon Elastic File System (Amazon EFS) Container Storage Interface (CSI) driver as an add-on.</p>",
          "<p>Use Amazon S3 to store all sensitive data. Enable server-side encryption with a new AWS Key Management Service (AWS KMS) key.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A software development company is creating a microservices-based application using Amazon Elastic Kubernetes Service (Amazon EKS). The company needs to ensure that sensitive configuration data like database credentials and API keys stored in Kubernetes ConfigMaps and Secrets are encrypted at rest.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399306,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A healthcare company maintains patient records in Amazon S3. To comply with HIPAA regulations, the stored data must not contain any protected health information (PHI). The company recently found out that some objects in the S3 buckets contain PHI. The company needs to automate the detection of PHI in the S3 buckets and notify its compliance team when such data is detected.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data like PHI. An Amazon EventBridge rule can be created to filter specific event types from Macie findings. When Macie identifies PHI in the S3 bucket, the EventBridge rule triggers an Amazon SNS notification to the compliance team.</p><p><strong>CORRECT: </strong>\"Use Amazon Macie. Create an AWS Lambda function to filter the ‘SensitiveData:S3Object/Personal’ event type from Macie findings and trigger an Amazon Simple Notification Service (Amazon SNS) notification to the compliance team\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Macie. Create an Amazon EventBridge rule to filter the ‘SensitiveData:S3Object/Health’ event type from Macie findings and trigger an Amazon Simple Email Service (Amazon SES) notification to the compliance team\" is incorrect.</p><p>The correct filder is ‘SensitiveData:S3Object/Personal’ which includes personally identifiable information (PII) such as passport numbers or driver's license identification numbers, personal health information (PHI) such as health insurance or medical identification numbers, or a combination of PII and PHI.</p><p><strong>INCORRECT:</strong> \"Use AWS Security Hub. Create an Amazon EventBridge rule to filter the ‘Security Hub findings - High severity’ event type and send an Amazon Simple Notification Service (Amazon SNS) notification to the compliance team\" is incorrect.</p><p>AWS Security Hub gives a comprehensive view of high-priority security alerts and compliance status, but it does not offer data-specific detection like PHI in S3 objects.</p><p><strong>INCORRECT:</strong> \"Use AWS Security Hub. Create an AWS Lambda function to filter the ‘Security Hub findings - High severity’ event type and trigger an Amazon Simple Email Service (Amazon SES) notification to the compliance team\" is incorrect.</p><p>AWS Security Hub does not offer detection of specific data types like PHI in S3 objects. Therefore, using it for this purpose would not meet the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/macie/latest/user/findings-types.html\">https://docs.aws.amazon.com/macie/latest/user/findings-types.html</a></p>",
        "answers": [
          "<p>Use Amazon Macie. Create an AWS Lambda function to filter the ‘SensitiveData:S3Object/Personal’ event type from Macie findings and trigger an Amazon Simple Notification Service (Amazon SNS) notification to the compliance team.</p>",
          "<p>Use AWS Security Hub. Create an Amazon EventBridge rule to filter the ‘Security Hub findings - High severity’ event type and send an Amazon Simple Notification Service (Amazon SNS) notification to the compliance team.</p>",
          "<p>Use Amazon Macie. Create an Amazon EventBridge rule to filter the ‘SensitiveData:S3Object/Health’ event type from Macie findings and trigger an Amazon Simple Email Service (Amazon SES) notification to the compliance team.</p>",
          "<p>Use AWS Security Hub. Create an AWS Lambda function to filter the ‘Security Hub findings - High severity’ event type and trigger an Amazon Simple Email Service (Amazon SES) notification to the compliance team.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A healthcare company maintains patient records in Amazon S3. To comply with HIPAA regulations, the stored data must not contain any protected health information (PHI). The company recently found out that some objects in the S3 buckets contain PHI. The company needs to automate the detection of PHI in the S3 buckets and notify its compliance team when such data is detected.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399308,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A travel agency operates a web service in an AWS Region. The service is accessed by customers via a REST API on Amazon API Gateway. The agency uses Amazon Route 53 for DNS and wants to provide individual and secure URLs for each travel agent using the service.</p><p>Which combination of steps will meet these requirements with the LEAST operational complexity? (Select THREE.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", "", ""],
        "explanation": "<p>Registering a wildcard custom domain name in Route 53 and creating a record pointing to API Gateway endpoint allows you to create unique URLs for each customer under the same domain name.</p><p>Requesting a wildcard certificate in the same AWS region as the REST API would provide secure URLs (https) for all customers under the same domain name. This would minimize the operational complexity of managing multiple certificates in different regions.</p><p>By creating a custom domain name in API Gateway and importing the wildcard certificate from ACM, the company can provide secure and unique URLs for each customer. API Gateway's custom domain names provide paths for API methods, helping maintain a consistent experience for customers.</p><p><strong>CORRECT: </strong>\"Register the desired domain with a domain registrar. Set up a wildcard custom domain in a Route 53 hosted zone and create a record in the zone that points to the API Gateway endpoint\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager (ACM) in the same Region\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Establish a custom domain name in API Gateway for the REST API. Import the corresponding certificate from AWS Certificate Manager (ACM)\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Request a wildcard certificate that corresponds to the custom domain name in AWS Certificate Manager (ACM), within a different Region\" is incorrect.</p><p>Requesting a wildcard certificate in a different AWS region than your API Gateway increases operational complexity and doesn't provide any significant benefit.</p><p><strong>INCORRECT:</strong> \"Create separate hosted zones in Route 53 for each travel agent as needed. Set up zone records that point to the API Gateway endpoint\" is incorrect.</p><p>Creating separate hosted zones for each travel agent can significantly increase operational complexity and cost. It would be more efficient to use a single hosted zone with a wildcard domain and use paths for differentiation.</p><p><strong>INCORRECT:</strong> \"Establish separate API endpoints in API Gateway for each travel agent\" is incorrect.</p><p>Creating separate API endpoints for each travel agent can significantly increase the complexity and management overhead. Instead, it would be more efficient to use different paths under the same API endpoint.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/DomainNameFormat.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/DomainNameFormat.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certificate-manager/\">https://digitalcloud.training/aws-certificate-manager/</a></p>",
        "answers": [
          "<p>Register the desired domain with a domain registrar. Set up a wildcard custom domain in a Route 53 hosted zone and create a record in the zone that points to the API Gateway endpoint.</p>",
          "<p>Request a wildcard certificate that corresponds to the custom domain name in AWS Certificate Manager (ACM), within a different Region.</p>",
          "<p>Create separate hosted zones in Route 53 for each travel agent as needed. Set up zone records that point to the API Gateway endpoint.</p>",
          "<p>Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager (ACM) in the same Region.</p>",
          "<p>Establish separate API endpoints in API Gateway for each travel agent.</p>",
          "<p>Establish a custom domain name in API Gateway for the REST API. Import the corresponding certificate from AWS Certificate Manager (ACM).</p>"
        ]
      },
      "correct_response": ["a", "d", "f"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A travel agency operates a web service in an AWS Region. The service is accessed by customers via a REST API on Amazon API Gateway. The agency uses Amazon Route 53 for DNS and wants to provide individual and secure URLs for each travel agent using the service.Which combination of steps will meet these requirements with the LEAST operational complexity? (Select THREE.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399310,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A music streaming company needs to incorporate a third-party song feed. The song feed sends a webhook to notify an external service when new songs are ready for consumption. A developer has written an AWS Lambda function to retrieve songs when the company receives a webhook callback. The developer must expose the Lambda function for the third party to invoke.</p><p>Which solution will meet these requirements with the LEAST operational complexity?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>API Gateway enables you to create, deploy, and manage a RESTful API to expose backend HTTP endpoints, AWS Lambda functions, or other AWS services. You can provide the third party with the API Gateway endpoint, and they can invoke the Lambda function through it. This solution is the most operationally efficient because it requires the fewest resources and management overhead.</p><p><strong>CORRECT: </strong>\"Generate an API Gateway endpoint for the Lambda function. Provide the API Gateway endpoint to the third party for the webhook\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy a Network Load Balancer (NLB) to distribute requests to the Lambda function. Provide the NLB URL to the third party for the webhook\" is incorrect.</p><p>While it is possible to trigger AWS Lambda from an Application Load Balancer (ALB), not NLB, using ALB would add unnecessary complexity to the solution.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Simple Notification Service (Amazon SNS) topic. Link the topic to the Lambda function. Provide the SNS topic ARN to the third party for the webhook\" is incorrect.</p><p>Amazon SNS is a pub/sub messaging service, but it is not meant to expose a public-facing endpoint for third-party webhooks. Also, providing ARN to a third party would not be possible as SNS topics cannot be invoked directly from the internet.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Simple Queue Service (Amazon SQS) queue. Connect the queue to the Lambda function. Provide the ARN of the SQS queue to the third party for the webhook\" is incorrect.</p><p>SQS is a message queuing service used to decouple and scale microservices, distributed systems, and serverless applications. It is not suitable for exposing a public-facing endpoint for third-party webhooks. Moreover, like SNS, SQS cannot be invoked directly from the internet.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
        "answers": [
          "<p>Generate an API Gateway endpoint for the Lambda function. Provide the API Gateway endpoint to the third party for the webhook.</p>",
          "<p>Deploy a Network Load Balancer (NLB) to distribute requests to the Lambda function. Provide the NLB URL to the third party for the webhook.</p>",
          "<p>Create an Amazon Simple Notification Service (Amazon SNS) topic. Link the topic to the Lambda function. Provide the SNS topic ARN to the third party for the webhook.</p>",
          "<p>Create an Amazon Simple Queue Service (Amazon SQS) queue. Connect the queue to the Lambda function. Provide the ARN of the SQS queue to the third party for the webhook.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A music streaming company needs to incorporate a third-party song feed. The song feed sends a webhook to notify an external service when new songs are ready for consumption. A developer has written an AWS Lambda function to retrieve songs when the company receives a webhook callback. The developer must expose the Lambda function for the third party to invoke.Which solution will meet these requirements with the LEAST operational complexity?",
      "related_lectures": []
    }
  ]
}
