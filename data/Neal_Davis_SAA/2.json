{
  "count": 65,
  "next": null,
  "previous": null,
  "results": [
    {
      "_class": "assessment",
      "id": 80399040,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A Solutions Architect must select the most appropriate database service for two use cases. A team of data scientists perform complex queries on a data warehouse that take several hours to complete. Another team of scientists need to run fast, repeat queries and update dashboards for customer support staff.</p><p>Which solution delivers these requirements MOST cost-effectively?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>RedShift is a columnar data warehouse DB that is ideal for running long complex queries. RedShift can also improve performance for repeat queries by caching the result and returning the cached result when queries are re-run. Dashboard, visualization, and business intelligence (BI) tools that execute repeat queries see a significant boost in performance due to result caching.</p><p><strong>CORRECT: </strong>\"RedShift for both use cases\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"RDS for both use cases\" is incorrect. RDS may be a good fit for the fast queries (not for the complex queries) but you now have multiple DBs to manage and multiple sets of data which is not going to be cost-effective.</p><p><strong>INCORRECT:</strong> \"RedShift for the analytics use case and ElastiCache in front of RedShift for the customer support dashboard\" is incorrect. You could put ElastiCache in front of the RedShift DB and this would provide good performance for the fast, repeat queries. However, it is not essential and would add cost to the solution so is not the most cost-effective option available.</p><p><strong>INCORRECT:</strong> \"RedShift for the analytics use case and RDS for the customer support dashboard\" is incorrect as RedShift is a better fit for both use cases.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-redshift-introduces-result-caching-for-sub-second-response-for-repeat-queries/\">https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-redshift-introduces-result-caching-for-sub-second-response-for-repeat-queries/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-redshift/\">https://digitalcloud.training/amazon-redshift/</a></p>",
        "answers": [
          "<p>RedShift for both use cases</p>",
          "<p>RedShift for the analytics use case and RDS for the customer support dashboard</p>",
          "<p>RDS for both use cases</p>",
          "<p>RedShift for the analytics use case and ElastiCache in front of RedShift for the customer support dashboard</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Database",
      "question_plain": "A Solutions Architect must select the most appropriate database service for two use cases. A team of data scientists perform complex queries on a data warehouse that take several hours to complete. Another team of scientists need to run fast, repeat queries and update dashboards for customer support staff.Which solution delivers these requirements MOST cost-effectively?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399042,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A High Performance Computing (HPC) application will be migrated to AWS. The application requires low network latency and high throughput between nodes and will be deployed in a single AZ.</p><p>How should the application be deployed for best inter-node performance?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>A cluster placement group provides low latency and high throughput for instances deployed in a single AZ. It is the best way to provide the performance required for this application.</p><p><strong>CORRECT: </strong>\"In a cluster placement group\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"In a partition placement group\" is incorrect. A partition placement group is used for grouping instances into logical segments. It provides control and visibility into instance placement but is not the best option for performance.</p><p><strong>INCORRECT:</strong> \"In a spread placement group\" is incorrect. A spread placement group is used to spread instances across underlying hardware. It is not the best option for performance.</p><p><strong>INCORRECT:</strong> \"Behind a Network Load Balancer (NLB)\" is incorrect. A network load balancer is used for distributing incoming connections, this does assist with inter-node performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
        "answers": [
          "<p>In a partition placement group</p>",
          "<p>In a cluster placement group</p>",
          "<p>In a spread placement group</p>",
          "<p>Behind a Network Load Balancer (NLB)</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "A High Performance Computing (HPC) application will be migrated to AWS. The application requires low network latency and high throughput between nodes and will be deployed in a single AZ.How should the application be deployed for best inter-node performance?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399044,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>Some objects that are uploaded to Amazon S3 standard storage class are initially accessed frequently for a period of 30 days. Then, objects are infrequently accessed for up to 90 days. After that, the objects are no longer needed.</p><p>How should lifecycle management be configured?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>In this scenario we need to keep the objects in the STANDARD storage class for 30 days as the objects are being frequently accessed. We can configure a lifecycle action that then transitions the objects to INTELLIGENT_TIERING, STANDARD_IA, or ONEZONE_IA. After that we don’t need the objects so they can be expired.</p><p>All other options do not meet the stated requirements or are not supported lifecycle transitions. For example:</p><p>· You cannot transition to REDUCED_REDUNDANCY from any storage class.</p><p>· Transitioning from STANDARD_IA to ONEZONE_IA is possible but we do not want to keep the objects so it incurs unnecessary costs.</p><p>· Transitioning to GLACIER is possible but again incurs unnecessary costs.</p><p><strong>CORRECT: </strong>\"Transition to ONEZONE_IA after 30 days. After 90 days expire the objects \" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Transition to STANDARD_IA after 30 days. After 90 days transition to GLACIER\" is incorrect.</p><p><strong>INCORRECT:</strong> \"Transition to STANDARD_IA after 30 days. After 90 days transition to ONEZONE_IA\" is incorrect.</p><p><strong>INCORRECT:</strong> \"Transition to REDUCED_REDUNDANCY after 30 days. After 90 days expire the objects \" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
        "answers": [
          "<p>Transition to STANDARD_IA after 30 days. After 90 days transition to GLACIER</p>",
          "<p>Transition to STANDARD_IA after 30 days. After 90 days transition to ONEZONE_IA</p>",
          "<p>Transition to ONEZONE_IA after 30 days. After 90 days expire the objects</p>",
          "<p>Transition to REDUCED_REDUNDANCY after 30 days. After 90 days expire the objects</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "Some objects that are uploaded to Amazon S3 standard storage class are initially accessed frequently for a period of 30 days. Then, objects are infrequently accessed for up to 90 days. After that, the objects are no longer needed.How should lifecycle management be configured?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399046,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A solutions architect is designing an application on AWS. The compute layer will run in parallel across EC2 instances. The compute layer should scale based on the number of jobs to be processed. The compute layer is stateless. The solutions architect must ensure that the application is loosely coupled and the job items are durably stored.</p><p>Which design should the solutions architect use?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>In this case we need to find a durable and loosely coupled solution for storing jobs. Amazon SQS is ideal for this use case and can be configured to use dynamic scaling based on the number of jobs waiting in the queue.</p><p>To configure this scaling you can use the <em>backlog per instance</em> metric with the target value being the <em>acceptable backlog per instance</em> to maintain. You can calculate these numbers as follows:</p><p><strong>Backlog per instance</strong>: To calculate your backlog per instance, start with the ApproximateNumberOfMessages queue attribute to determine the length of the SQS queue (number of messages available for retrieval from the queue). Divide that number by the fleet's running capacity, which for an Auto Scaling group is the number of instances in the InService state, to get the backlog per instance.</p><p><strong>Acceptable backlog per instance</strong>: To calculate your target value, first determine what your application can accept in terms of latency. Then, take the acceptable latency value and divide it by the average time that an EC2 instance takes to process a message.</p><p>This solution will scale EC2 instances using Auto Scaling based on the number of jobs waiting in the SQS queue.</p><p><strong>CORRECT: </strong>\"Create an Amazon SQS queue to hold the jobs that needs to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on network usage\" is incorrect as scaling on network usage does not relate to the number of jobs waiting to be processed.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on CPU usage\" is incorrect. Amazon SNS is a notification service so it delivers notifications to subscribers. It does store data durably but is less suitable than SQS for this use case. Scaling on CPU usage is not the best solution as it does not relate to the number of jobs waiting to be processed.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of messages published to the SNS topic\" is incorrect. Amazon SNS is a notification service so it delivers notifications to subscribers. It does store data durably but is less suitable than SQS for this use case. Scaling on the number of notifications in SNS is not possible.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
        "answers": [
          "<p>Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on CPU usage</p>",
          "<p>Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on network usage</p>",
          "<p>Create an Amazon SQS queue to hold the jobs that needs to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue</p>",
          "<p>Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of messages published to the SNS topic</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Application Integration",
      "question_plain": "A solutions architect is designing an application on AWS. The compute layer will run in parallel across EC2 instances. The compute layer should scale based on the number of jobs to be processed. The compute layer is stateless. The solutions architect must ensure that the application is loosely coupled and the job items are durably stored.Which design should the solutions architect use?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399048,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company requires a solution to allow customers to customize images that are stored in an online catalog. The image customization parameters will be sent in requests to Amazon API Gateway. The customized image will then be generated on-demand and can be accessed online.</p><p>The solutions architect requires a highly available solution. Which solution will be MOST cost-effective?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>All solutions presented are highly available. The key requirement that must be satisfied is that the solution should be cost-effective and you must choose the most cost-effective option.</p><p>Therefore, it’s best to eliminate services such as Amazon EC2 and ELB as these require ongoing costs even when they’re not used. Instead, a fully serverless solution should be used. AWS Lambda, Amazon S3 and CloudFront are the best services to use for these requirements.</p><p><strong>CORRECT: </strong>\"Use AWS Lambda to manipulate the original images to the requested customization. Store the original and manipulated images in Amazon S3. Configure an Amazon CloudFront distribution with the S3 bucket as the origin\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 instances to manipulate the original images into the requested customization. Store the original and manipulated images in Amazon S3. Configure an Elastic Load Balancer in front of the EC2 instances\" is incorrect. This is not the most cost-effective option as the ELB and EC2 instances will incur costs even when not used.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda to manipulate the original images to the requested customization. Store the original images in Amazon S3 and the manipulated images in Amazon DynamoDB. Configure an Elastic Load Balancer in front of the Amazon EC2 instances\" is incorrect. This is not the most cost-effective option as the ELB will incur costs even when not used. Also, Amazon DynamoDB will incur RCU/WCUs when running and is not the best choice for storing images.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 instances to manipulate the original images into the requested customization. Store the original images in Amazon S3 and the manipulated images in Amazon DynamoDB. Configure an Amazon CloudFront distribution with the S3 bucket as the origin\" is incorrect. This is not the most cost-effective option as the EC2 instances will incur costs even when not used</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/serverless/\">https://aws.amazon.com/serverless/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
        "answers": [
          "<p>Use Amazon EC2 instances to manipulate the original images into the requested customization. Store the original and manipulated images in Amazon S3. Configure an Elastic Load Balancer in front of the EC2 instances</p>",
          "<p>Use AWS Lambda to manipulate the original images to the requested customization. Store the original and manipulated images in Amazon S3. Configure an Amazon CloudFront distribution with the S3 bucket as the origin</p>",
          "<p>Use AWS Lambda to manipulate the original images to the requested customization. Store the original images in Amazon S3 and the manipulated images in Amazon DynamoDB. Configure an Elastic Load Balancer in front of the Amazon EC2 instances</p>",
          "<p>Use Amazon EC2 instances to manipulate the original images into the requested customization. Store the original images in Amazon S3 and the manipulated images in Amazon DynamoDB. Configure an Amazon CloudFront distribution with the S3 bucket as the origin</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "A company requires a solution to allow customers to customize images that are stored in an online catalog. The image customization parameters will be sent in requests to Amazon API Gateway. The customized image will then be generated on-demand and can be accessed online.The solutions architect requires a highly available solution. Which solution will be MOST cost-effective?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399050,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A solutions architect is finalizing the architecture for a distributed database that will run across multiple Amazon EC2 instances. Data will be replicated across all instances so the loss of an instance will not cause loss of data. The database requires block storage with low latency and throughput that supports up to several million transactions per second per server.</p><p>Which storage solution should the solutions architect use?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>An <em>instance store</em> provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-18_12-54-38-9844f13d68a10a257851c57f0f920791.JPG\"></p><p>Some instance types use NVMe or SATA-based solid state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures.</p><p>In this scenario the data is replicated and fault tolerant so the best option to provide the level of performance required is to use instance store volumes.</p><p><strong>CORRECT: </strong>\"Amazon EC2 instance store\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EBS \" is incorrect. The Elastic Block Store (EBS) is a block storage device but as the data is distributed and fault tolerant a better option for performance would be to use instance stores.</p><p><strong>INCORRECT:</strong> \"Amazon EFS \" is incorrect as EFS is not a block device, it is a filesystem that is accessed using the NFS protocol.</p><p><strong>INCORRECT:</strong> \"Amazon S3\" is incorrect as S3 is an object-based storage system, not a block-based storage system.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
        "answers": [
          "<p>Amazon EBS</p>",
          "<p>Amazon EC2 instance store</p>",
          "<p>Amazon EFS</p>",
          "<p>Amazon S3</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "A solutions architect is finalizing the architecture for a distributed database that will run across multiple Amazon EC2 instances. Data will be replicated across all instances so the loss of an instance will not cause loss of data. The database requires block storage with low latency and throughput that supports up to several million transactions per second per server.Which storage solution should the solutions architect use?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399052,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company is deploying a new web application that will run on Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. The application requires a shared storage solution that offers strong consistency as the content will be regularly updated.</p><p>Which solution requires the LEAST amount of effort?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon EFS is a fully-managed service that makes it easy to set up, scale, and cost-optimize file storage in the Amazon Cloud. EFS file systems are accessible to Amazon EC2 instances via a file system interface (using standard operating system file I/O APIs) and support full file system access semantics (such as strong consistency and file locking).</p><p>EFS is a good solution for when you need to attach a shared filesystem to multiple EC2 instances across multiple Availability Zones.</p><p><strong>CORRECT: </strong>\"Create an Amazon Elastic File System (Amazon EFS) file system and mount it on the individual Amazon EC2 instances\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon S3 bucket to store the web content and use Amazon CloudFront to deliver the content\" is incorrect as this may require more effort in terms of reprogramming the application to use the S3 API.</p><p><strong>INCORRECT:</strong> \"Create a shared Amazon Block Store (Amazon EBS) volume and mount it on the individual Amazon EC2 instances\" is incorrect. Please note that you can multi-attach an EBS volume to multiple EC2 instances but the instances must be in the same AZ.</p><p><strong>INCORRECT:</strong> \"Create a volume gateway using AWS Storage Gateway to host the data and mount it to the Auto Scaling group\" is incorrect as a storage gateway is used on-premises.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/efs/faq/\">https://aws.amazon.com/efs/faq/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
        "answers": [
          "<p>Create an Amazon S3 bucket to store the web content and use Amazon CloudFront to deliver the content</p>",
          "<p>Create an Amazon Elastic File System (Amazon EFS) file system and mount it on the individual Amazon EC2 instances</p>",
          "<p>Create a shared Amazon Block Store (Amazon EBS) volume and mount it on the individual Amazon EC2 instances</p>",
          "<p>Create a volume gateway using AWS Storage Gateway to host the data and mount it to the Auto Scaling group</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Storage",
      "question_plain": "A company is deploying a new web application that will run on Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. The application requires a shared storage solution that offers strong consistency as the content will be regularly updated.Which solution requires the LEAST amount of effort?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399054,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A website runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The website has a mix of dynamic and static content. Customers around the world are reporting performance issues with the website.</p><p>Which set of actions will improve website performance for users worldwide?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon CloudFront is a content delivery network (CDN) that improves website performance by caching content at edge locations around the world. It can serve both dynamic and static content. This is the best solution for improving the performance of the website.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudFront distribution and configure the ALB as an origin. Then update the Amazon Route 53 record to point to the CloudFront distribution\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a latency-based Amazon Route 53 record for the ALB. Then launch new EC2 instances with larger instance sizes and register the instances with the ALB\" is incorrect. Latency routing routes based on the latency between the client and AWS. There is no mention in the answer about creating the new instances in another region therefore the only advantage is in using larger instance sizes. For a dynamic site this adds complexity in keeping the instances in sync.</p><p><strong>INCORRECT:</strong> \"Launch new EC2 instances hosting the same web application in different Regions closer to the users. Use an AWS Transit Gateway to connect customers to the closest region\" is incorrect as Transit Gateway is a service for connecting on-premises networks and VPCs to a single gateway.</p><p><strong>INCORRECT:</strong> \"Migrate the website to an Amazon S3 bucket in the Regions closest to the users. Then create an Amazon Route 53 geolocation record to point to the S3 buckets\" is incorrect as with S3 you can only host static websites, not dynamic websites.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/dynamic-content/\">https://aws.amazon.com/cloudfront/dynamic-content/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
        "answers": [
          "<p>Create an Amazon CloudFront distribution and configure the ALB as an origin. Then update the Amazon Route 53 record to point to the CloudFront distribution</p>",
          "<p>Create a latency-based Amazon Route 53 record for the ALB. Then launch new EC2 instances with larger instance sizes and register the instances with the ALB</p>",
          "<p>Launch new EC2 instances hosting the same web application in different Regions closer to the users. Use an AWS Transit Gateway to connect customers to the closest region</p>",
          "<p>Migrate the website to an Amazon S3 bucket in the Regions closest to the users. Then create an Amazon Route 53 geolocation record to point to the S3 buckets</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A website runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The website has a mix of dynamic and static content. Customers around the world are reporting performance issues with the website.Which set of actions will improve website performance for users worldwide?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399056,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A web application has recently been launched on AWS. The architecture includes two tier with a web layer and a database layer. It has been identified that the web server layer may be vulnerable to cross-site scripting (XSS) attacks.</p><p>What should a solutions architect do to remediate the vulnerability?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The AWS Web Application Firewall (WAF) is available on the Application Load Balancer (ALB). You can use AWS WAF directly on Application Load Balancers (both internal and external) in a VPC, to protect your websites and web services.</p><p>Attackers sometimes insert scripts into web requests in an effort to exploit vulnerabilities in web applications. You can create one or more cross-site scripting match conditions to identify the parts of web requests, such as the URI or the query string, that you want AWS WAF to inspect for possible malicious scripts.</p><p><strong>CORRECT: </strong>\"Create an Application Load Balancer. Put the web layer behind the load balancer and enable AWS WAF\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a Classic Load Balancer. Put the web layer behind the load balancer and enable AWS WAF\" is incorrect as you cannot use AWS WAF with a classic load balancer.</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer. Put the web layer behind the load balancer and enable AWS WAF\" is incorrect as you cannot use AWS WAF with a network load balancer.</p><p><strong>INCORRECT:</strong> \"Create an Application Load Balancer. Put the web layer behind the load balancer and use AWS Shield Standard\" is incorrect as you cannot use AWS Shield to protect against XSS attacks. Shield is used to protect against DDoS attacks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-xss-conditions.html\">https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-xss-conditions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>",
        "answers": [
          "<p>Create a Classic Load Balancer. Put the web layer behind the load balancer and enable AWS WAF</p>",
          "<p>Create a Network Load Balancer. Put the web layer behind the load balancer and enable AWS WAF</p>",
          "<p>Create an Application Load Balancer. Put the web layer behind the load balancer and enable AWS WAF</p>",
          "<p>Create an Application Load Balancer. Put the web layer behind the load balancer and use AWS Shield Standard</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A web application has recently been launched on AWS. The architecture includes two tier with a web layer and a database layer. It has been identified that the web server layer may be vulnerable to cross-site scripting (XSS) attacks.What should a solutions architect do to remediate the vulnerability?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399058,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A multi-tier application runs with eight front-end web servers in an Amazon EC2 Auto Scaling group in a single Availability Zone behind an Application Load Balancer. A solutions architect needs to modify the infrastructure to be highly available without modifying the application.</p><p>Which architecture should the solutions architect choose that provides high availability?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>High availability can be enabled for this architecture quite simply by modifying the existing Auto Scaling group to use multiple availability zones. The ASG will automatically balance the load so you don’t actually need to specify the instances per AZ.</p><p>The architecture for the web tier will look like the one below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-27-38-903c5a1a8a69212049571597cbc31b0f.jpg\"></p><p><strong>CORRECT: </strong>\"Modify the Auto Scaling group to use four instances across each of two Availability Zones\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group that uses four instances across each of two Regions\" is incorrect as EC2 Auto Scaling does not support multiple regions.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling template that can be used to quickly create more instances in another Region\" is incorrect as EC2 Auto Scaling does not support multiple regions.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group that uses four instances across each of two subnets\" is incorrect as the subnets could be in the same AZ.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/ec2/autoscaling/\">https://aws.amazon.com/ec2/autoscaling/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
        "answers": [
          "<p>Create an Auto Scaling group that uses four instances across each of two Regions</p>",
          "<p>Modify the Auto Scaling group to use four instances across each of two Availability Zones</p>",
          "<p>Create an Auto Scaling template that can be used to quickly create more instances in another Region</p>",
          "<p>Create an Auto Scaling group that uses four instances across each of two subnets</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Compute",
      "question_plain": "A multi-tier application runs with eight front-end web servers in an Amazon EC2 Auto Scaling group in a single Availability Zone behind an Application Load Balancer. A solutions architect needs to modify the infrastructure to be highly available without modifying the application.Which architecture should the solutions architect choose that provides high availability?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399060,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company's web application is using multiple Amazon EC2 Linux instances and storing data on Amazon EBS volumes. The company is looking for a solution to increase the resiliency of the application in case of a failure.</p><p>What should a solutions architect do to meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>To increase the resiliency of the application the solutions architect can use Auto Scaling groups to launch and terminate instances across multiple availability zones based on demand. An application load balancer (ALB) can be used to direct traffic to the web application running on the EC2 instances.</p><p>Lastly, the Amazon Elastic File System (EFS) can assist with increasing the resilience of the application by providing a shared file system that can be mounted by multiple EC2 instances from multiple availability zones.</p><p><strong>CORRECT: </strong>\"Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Store data on Amazon EFS and mount a target on each instance\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Launch the application on EC2 instances in each Availability Zone. Attach EBS volumes to each EC2 instance\" is incorrect as the EBS volumes are single points of failure which are not shared with other instances.</p><p><strong>INCORRECT:</strong> \"Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Mount an instance store on each EC2 instance\" is incorrect as instance stores are ephemeral data stores which means data is lost when powered down. Also, instance stores cannot be shared between instances.</p><p><strong>INCORRECT:</strong> \"Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Store data using Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)\" is incorrect as there are data retrieval charges associated with this S3 tier. It is not a suitable storage tier for application files.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/\">https://docs.aws.amazon.com/efs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
        "answers": [
          "<p>Launch the application on EC2 instances in each Availability Zone. Attach EBS volumes to each EC2 instance</p>",
          "<p>Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Mount an instance store on each EC2 instance</p>",
          "<p>Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Store data on Amazon EFS and mount a target on each instance</p>",
          "<p>Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Store data using Amazon S3 One Zone-Infrequent Access (S3 One Zone-A)</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Compute",
      "question_plain": "A company's web application is using multiple Amazon EC2 Linux instances and storing data on Amazon EBS volumes. The company is looking for a solution to increase the resiliency of the application in case of a failure.What should a solutions architect do to meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399062,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A website runs on a Microsoft Windows server in an on-premises data center. The web server is being migrated to Amazon EC2 Windows instances in multiple Availability Zones on AWS. The web server currently uses data stored in an on-premises network-attached storage (NAS) device.</p><p>Which replacement to the NAS file share is MOST resilient and durable?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. It offers single-AZ and multi-AZ deployment options, fully managed backups, and encryption of data at rest and in transit.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-28-32-63d08e70f92de39652bd9053f98f90df.jpg\"></p><p>This is the only solution presented that provides resilient storage for Windows instances.</p><p><strong>CORRECT: </strong>\"Migrate the file share to Amazon FSx for Windows File Server\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Migrate the file share to Amazon Elastic File System (Amazon EFS)\" is incorrect as you cannot use Windows instances with Amazon EFS.</p><p><strong>INCORRECT:</strong> \"Migrate the file share to Amazon EBS\" is incorrect as this is not a shared storage solution for multi-AZ deployments.</p><p><strong>INCORRECT:</strong> \"Migrate the file share to AWS Storage Gateway\" is incorrect as with Storage Gateway replicated files end up on Amazon S3. The replacement storage solution should be a file share, not an object-based storage system.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
        "answers": [
          "<p>Migrate the file share to Amazon EBS</p>",
          "<p>Migrate the file share to AWS Storage Gateway</p>",
          "<p>Migrate the file share to Amazon FSx for Windows File Server</p>",
          "<p>Migrate the file share to Amazon Elastic File System (Amazon EFS)</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "A website runs on a Microsoft Windows server in an on-premises data center. The web server is being migrated to Amazon EC2 Windows instances in multiple Availability Zones on AWS. The web server currently uses data stored in an on-premises network-attached storage (NAS) device.Which replacement to the NAS file share is MOST resilient and durable?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399064,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A company is planning a migration for a high performance computing (HPC) application and associated data from an on-premises data center to the AWS Cloud. The company uses tiered storage on premises with hot high-performance parallel storage to support the application during periodic runs of the application, and more economical cold storage to hold the data when the application is not actively running.</p><p>Which combination of solutions should a solutions architect recommend to support the storage needs of the application? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Amazon FSx for Lustre provides a high-performance file system optimized for fast processing of workloads such as machine learning, high-performance computing (HPC), video processing, financial modeling, and electronic design automation (EDA).</p><p>These workloads commonly require data to be presented via a fast and scalable file system interface, and typically have data sets stored on long-term data stores like Amazon S3.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-29-14-9ca4fa2525b286cbb51131ad2546dea3.jpg\"></p><p>Amazon FSx works natively with Amazon S3, making it easy to access your S3 data to run data processing workloads. Your S3 objects are presented as files in your file system, and you can write your results back to S3. This lets you run data processing workloads on FSx for Lustre and store your long-term data on S3 or on-premises data stores.</p><p>Therefore, the best combination for this scenario is to use S3 for cold data and FSx for Lustre for the parallel HPC job.</p><p><strong>CORRECT: </strong>\"Amazon S3 for cold data storage\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Amazon FSx for Lustre for high-performance parallel storage\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EFS for cold data storage\" is incorrect as FSx works natively with S3 which is also more economical.</p><p><strong>INCORRECT:</strong> \"Amazon S3 for high-performance parallel storage\" is incorrect as S3 is not suitable for running high-performance computing jobs.</p><p><strong>INCORRECT:</strong> \"Amazon FSx for Windows for high-performance parallel storage\" is incorrect as FSx for Lustre should be used for HPC use cases and use cases that require storing data on S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/lustre/\">https://aws.amazon.com/fsx/lustre/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
        "answers": [
          "<p>Amazon S3 for cold data storage</p>",
          "<p>Amazon EFS for cold data storage</p>",
          "<p>Amazon S3 for high-performance parallel storage</p>",
          "<p>Amazon FSx for Lustre for high-performance parallel storage</p>",
          "<p>Amazon FSx for Windows for high-performance parallel storage</p>"
        ]
      },
      "correct_response": ["a", "d"],
      "section": "AWS Storage",
      "question_plain": "A company is planning a migration for a high performance computing (HPC) application and associated data from an on-premises data center to the AWS Cloud. The company uses tiered storage on premises with hot high-performance parallel storage to support the application during periodic runs of the application, and more economical cold storage to hold the data when the application is not actively running.Which combination of solutions should a solutions architect recommend to support the storage needs of the application? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399066,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A web application that allows users to upload and share documents is running on a single Amazon EC2 instance with an Amazon EBS volume. To increase availability the architecture has been updated to use an Auto Scaling group of several instances across Availability Zones behind an Application Load Balancer. After the change users can only see a subset of the documents.</p><p>What is the BEST method for a solutions architect to modify the solution so users can see all documents?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The problem that is being described is that the users are uploading the documents to an individual EC2 instance with a local EBS volume. Therefore, as EBS volumes cannot be shared across AZs, the data is stored separately and the ALB will be distributing incoming connections to different instances / data sets.</p><p>The simple resolution is to implement a shared storage layer for the documents so that they can be stored in one place and seen by any user who connects no matter which instance they connect to.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-30-06-e730f4c928621d8e81593b31b92020f6.jpg\"></p><p><strong>CORRECT: </strong>\"Copy the data from all EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Run a script to synchronize the data between Amazon EBS volumes\" is incorrect. This is a complex and messy approach. A better solution is to use a shared storage layer.</p><p><strong>INCORRECT:</strong> \"Use Sticky Sessions with the ALB to ensure users are directed to the same EC2 instance in a session\" is incorrect as this will just “stick” a user to the same instance. They won’t see documents uploaded to other instances / EBS volumes.</p><p><strong>INCORRECT:</strong> \"Configure the Application Load Balancer to send the request to all servers. Return each document from the correct server\" is incorrect as there is no mechanism here for selecting a specific document. The requirement also requests that all documents are visible.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html\">https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
        "answers": [
          "<p>Run a script to synchronize the data between Amazon EBS volumes</p>",
          "<p>Use Sticky Sessions with the ALB to ensure users are directed to the same EC2 instance in a session</p>",
          "<p>Copy the data from all EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS</p>",
          "<p>Configure the Application Load Balancer to send the request to all servers. Return each document from the correct server</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "A web application that allows users to upload and share documents is running on a single Amazon EC2 instance with an Amazon EBS volume. To increase availability the architecture has been updated to use an Auto Scaling group of several instances across Availability Zones behind an Application Load Balancer. After the change users can only see a subset of the documents.What is the BEST method for a solutions architect to modify the solution so users can see all documents?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399068,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application uses Amazon EC2 instances and an Amazon RDS MySQL database. The database is not currently encrypted. A solutions architect needs to apply encryption to the database for all new and existing data.</p><p>How should this be accomplished?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>There are some <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations\">limitations for encrypted Amazon RDS DB Instances</a>: you can't modify an existing unencrypted Amazon RDS DB instance to make the instance encrypted, and you can't create an encrypted read replica from an unencrypted instance.</p><p>However, you can use the Amazon RDS snapshot feature to encrypt an unencrypted snapshot that's taken from the RDS database that you want to encrypt. Restore a new RDS DB instance from the encrypted snapshot to deploy a new encrypted DB instance. Finally, switch your connections to the new DB instance.</p><p><strong>CORRECT: </strong>\"Take a snapshot of the RDS instance. Create an encrypted copy of the snapshot. Restore the RDS instance from the encrypted snapshot\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon ElastiCache cluster and encrypt data using the cache nodes\" is incorrect as you cannot encrypt an RDS database using an ElastiCache cache node.</p><p><strong>INCORRECT:</strong> \"Enable encryption for the database using the API. Take a full snapshot of the database. Delete old snapshots\" is incorrect as you cannot enable encryption for an existing database.</p><p><strong>INCORRECT:</strong> \"Create an RDS read replica with encryption at rest enabled. Promote the read replica to master and switch the application over to the new master. Delete the old RDS instance\" is incorrect as you cannot create an encrypted read replica from an unencrypted database instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-encrypt-instance-mysql-mariadb/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-encrypt-instance-mysql-mariadb/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
        "answers": [
          "<p>Create an Amazon ElastiCache cluster and encrypt data using the cache nodes</p>",
          "<p>Enable encryption for the database using the API. Take a full snapshot of the database. Delete old snapshots</p>",
          "<p>Take a snapshot of the RDS instance. Create an encrypted copy of the snapshot. Restore the RDS instance from the encrypted snapshot</p>",
          "<p>Create an RDS read replica with encryption at rest enabled. Promote the read replica to master and switch the application over to the new master. Delete the old RDS instance</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Database",
      "question_plain": "An application uses Amazon EC2 instances and an Amazon RDS MySQL database. The database is not currently encrypted. A solutions architect needs to apply encryption to the database for all new and existing data.How should this be accomplished?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399070,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company have 500 TB of data in an on-premises file share that needs to be moved to Amazon S3 Glacier. The migration must not saturate the company’s low-bandwidth internet connection and the migration must be completed within a few weeks.</p><p>What is the MOST cost-effective solution?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>As the company’s internet link is low-bandwidth uploading directly to Amazon S3 (ready for transition to Glacier) would saturate the link. The best alternative is to use AWS Snowball appliances. The Snowball edge appliance can hold up to 80 TB of data so 7 devices would be required to migrate 500 TB of data.</p><p>Snowball moves data into AWS using a hardware device and the data is then copied into an Amazon S3 bucket of your choice. From there, lifecycle policies can transition the S3 objects to Amazon S3 Glacier.</p><p><strong>CORRECT: </strong>\"Order 7 AWS Snowball appliances and select an Amazon S3 bucket as the destination. Create a lifecycle policy to transition the S3 objects to Amazon S3 Glacier\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Order 7 AWS Snowball appliances and select an S3 Glacier vault as the destination. Create a bucket policy to enforce a VPC endpoint\" is incorrect as you cannot set a Glacier vault as the destination, it must be an S3 bucket. You also can’t enforce a VPC endpoint using a bucket policy.</p><p><strong>INCORRECT:</strong> \"Create an AWS Direct Connect connection and migrate the data straight into Amazon Glacier\" is incorrect as this is not the most cost-effective option and takes time to setup.</p><p><strong>INCORRECT:</strong> \"Use AWS Global Accelerator to accelerate upload and optimize usage of the available bandwidth\" is incorrect as this service is not used for accelerating or optimizing the upload of data from on-premises networks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/snowball/latest/developer-guide/specifications.html\">https://docs.aws.amazon.com/snowball/latest/developer-guide/specifications.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
        "answers": [
          "<p>Create an AWS Direct Connect connection and migrate the data straight into Amazon Glacier</p>",
          "<p>Order 7 AWS Snowball appliances and select an S3 Glacier vault as the destination. Create a bucket policy to enforce a VPC endpoint</p>",
          "<p>Use AWS Global Accelerator to accelerate upload and optimize usage of the available bandwidth</p>",
          "<p>Order 7 AWS Snowball appliances and select an Amazon S3 bucket as the destination. Create a lifecycle policy to transition the S3 objects to Amazon S3 Glacier</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Migration & Transfer",
      "question_plain": "A company have 500 TB of data in an on-premises file share that needs to be moved to Amazon S3 Glacier. The migration must not saturate the company’s low-bandwidth internet connection and the migration must be completed within a few weeks.What is the MOST cost-effective solution?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399072,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company has refactored a legacy application to run as two microservices using Amazon ECS. The application processes data in two parts and the second part of the process takes longer than the first.</p><p>How can a solutions architect integrate the microservices and allow them to scale independently?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>This is a good use case for Amazon SQS. The microservices must be decoupled so they can scale independently. An Amazon SQS queue will enable microservice 1 to add messages to the queue. Microservice 2 can then pick up the messages and process them. This ensures that if there’s a spike in traffic on the frontend, messages do not get lost due to the backend process not being ready to process them.</p><p><strong>CORRECT: </strong>\"Implement code in microservice 1 to send data to an Amazon SQS queue. Implement code in microservice 2 to process messages from the queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement code in microservice 1 to send data to an Amazon S3 bucket. Use S3 event notifications to invoke microservice 2\" is incorrect as a message queue would be preferable to an S3 bucket.</p><p><strong>INCORRECT:</strong> \"Implement code in microservice 1 to publish data to an Amazon SNS topic. Implement code in microservice 2 to subscribe to this topic\" is incorrect as notifications to topics are pushed to subscribers. In this case we want the second microservice to pickup the messages when ready (pull them).</p><p><strong>INCORRECT:</strong> \"Implement code in microservice 1 to send data to Amazon Kinesis Data Firehose. Implement code in microservice 2 to read from Kinesis Data Firehose\" is incorrect as this is not how Firehose works. Firehose sends data directly to destinations, it is not a message queue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
        "answers": [
          "<p>Implement code in microservice 1 to send data to an Amazon S3 bucket. Use S3 event notifications to invoke microservice 2</p>",
          "<p>Implement code in microservice 1 to publish data to an Amazon SNS topic. Implement code in microservice 2 to subscribe to this topic</p>",
          "<p>Implement code in microservice 1 to send data to Amazon Kinesis Data Firehose. Implement code in microservice 2 to read from Kinesis Data Firehose</p>",
          "<p>Implement code in microservice 1 to send data to an Amazon SQS queue. Implement code in microservice 2 to process messages from the queue</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Application Integration",
      "question_plain": "A company has refactored a legacy application to run as two microservices using Amazon ECS. The application processes data in two parts and the second part of the process takes longer than the first.How can a solutions architect integrate the microservices and allow them to scale independently?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399074,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A solutions architect is designing a two-tier web application. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the company.</p><p>How should security groups be configured in this situation? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>In this scenario an inbound rule is required to allow traffic from any internet client to the web front end on SSL/TLS port 443. The source should therefore be set to 0.0.0.0/0 to allow any inbound traffic.</p><p>To secure the connection from the web frontend to the database tier, an outbound rule should be created from the public EC2 security group with a destination of the private EC2 security group. The port should be set to 1433 for MySQL. The private EC2 security group will also need to allow inbound traffic on 1433 from the public EC2 security group.</p><p>This configuration can be seen in the diagram:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-31-06-6dcaf8d88c9ec27f837343a0ae2630f9.jpg\"></p><p><strong>CORRECT: </strong>\"Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0 and to allow outbound traffic on port 1433 to the RDS\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0\" is incorrect as this is configured backwards.</p><p><strong>INCORRECT:</strong> \"Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier\" is incorrect as the MySQL database instance does not need to send outbound traffic on either of these ports.</p><p><strong>INCORRECT:</strong> \"Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier\" is incorrect as the database tier does not need to allow inbound traffic on port 443.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0 and to allow outbound traffic on port 1433 to the RDS</p>",
          "<p>Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0</p>",
          "<p>Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier</p>",
          "<p>Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier</p>",
          "<p>Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier</p>"
        ]
      },
      "correct_response": ["a", "c"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A solutions architect is designing a two-tier web application. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the company.How should security groups be configured in this situation? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399076,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A solutions architect has created a new AWS account and must secure AWS account root user access.</p><p>Which combination of actions will accomplish this? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>There are several security best practices for securing the root user account:</p><p>· Lock away root user access keys OR delete them if possible</p><p>· Use a strong password</p><p>· Enable multi-factor authentication (MFA)</p><p>The root user automatically has full privileges to the account and these privileges cannot be restricted so it is extremely important to follow best practice advice about securing the root user account.</p><p><strong>CORRECT: </strong>\"Ensure the root user uses a strong password\" is the correct answer.</p><p><strong>CORRECT: </strong>\"Enable multi-factor authentication to the root user\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Store root user access keys in an encrypted Amazon S3 bucket\" is incorrect as the best practice is to lock away or delete the root user access keys. An S3 bucket is not a suitable location for storing them, even if encrypted.</p><p><strong>INCORRECT:</strong> \"Add the root user to a group containing administrative permissions\" is incorrect as this does not restrict access and is unnecessary.</p><p><strong>INCORRECT:</strong> \"Delete the root user account\" is incorrect as you cannot delete the root user account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
        "answers": [
          "<p>Ensure the root user uses a strong password</p>",
          "<p>Enable multi-factor authentication for the root user</p>",
          "<p>Store root user access keys in an encrypted Amazon S3 bucket</p>",
          "<p>Add the root user to a group containing administrative permissions</p>",
          "<p>Delete the root user account</p>"
        ]
      },
      "correct_response": ["a", "b"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A solutions architect has created a new AWS account and must secure AWS account root user access.Which combination of actions will accomplish this? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399078,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company allows its developers to attach existing IAM policies to existing IAM roles to enable faster experimentation and agility. However, the security operations team is concerned that the developers could attach the existing administrator policy, which would allow the developers to circumvent any other security policies.</p><p>How should a solutions architect address this issue?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The permissions boundary for an IAM entity (user or role) sets the maximum permissions that the entity can have. This can change the effective permissions for that user or role. The effective permissions for an entity are the permissions that are granted by all the policies that affect the user or role. Within an account, the permissions for an entity can be affected by identity-based policies, resource-based policies, permissions boundaries, Organizations SCPs, or session policies.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-18_13-16-47-d821741ed0b16e76b6ea87b337c0aeeb.JPG\"></p><p>Therefore, the solutions architect can set an IAM permissions boundary on the developer IAM role that explicitly denies attaching the administrator policy.</p><p><strong>CORRECT: </strong>\"Set an IAM permissions boundary on the developer IAM role that explicitly denies attaching the administrator policy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic to send an alert every time a developer creates a new policy\" is incorrect as this would mean investigating every incident which is not an efficient solution.</p><p><strong>INCORRECT:</strong> \"Use service control policies to disable IAM activity across all accounts in the organizational unit\" is incorrect as this would prevent the developers from being able to work with IAM completely.</p><p><strong>INCORRECT:</strong> \"Prevent the developers from attaching any policies and assign all IAM duties to the security operations team\" is incorrect as this is not necessary. The requirement is to allow developers to work with policies, the solution needs to find a secure way of achieving this.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
        "answers": [
          "<p>Create an Amazon SNS topic to send an alert every time a developer creates a new policy</p>",
          "<p>Use service control policies to disable IAM activity across all accounts in the organizational unit</p>",
          "<p>Prevent the developers from attaching any policies and assign all IAM duties to the security operations team</p>",
          "<p>Set an IAM permissions boundary on the developer IAM role that explicitly denies attaching the administrator policy</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A company allows its developers to attach existing IAM policies to existing IAM roles to enable faster experimentation and agility. However, the security operations team is concerned that the developers could attach the existing administrator policy, which would allow the developers to circumvent any other security policies.How should a solutions architect address this issue?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399080,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A solutions architect is optimizing a website for real-time streaming and on-demand videos. The website’s users are located around the world and the solutions architect needs to optimize the performance for both the real-time and on-demand streaming.</p><p>Which service should the solutions architect choose?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon CloudFront can be used to stream video to users across the globe using a wide variety of protocols that are layered on top of HTTP. This can include both on-demand video as well as real time streaming video.</p><p><strong>CORRECT: </strong>\"Amazon CloudFront\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS Global Accelerator\" is incorrect as this would be an expensive way of getting the content closer to users compared to using CloudFront. As this is a use case for CloudFront and there are so many edge locations it is the better option.</p><p><strong>INCORRECT:</strong> \"Amazon Route 53\" is incorrect as you still need a solution for getting the content closer to users.</p><p><strong>INCORRECT:</strong> \"Amazon S3 Transfer Acceleration\" is incorrect as this is used to accelerate uploads of data to Amazon S3 buckets.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/streaming/\">https://aws.amazon.com/cloudfront/streaming/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-streaming-video.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-streaming-video.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
        "answers": [
          "<p>Amazon CloudFront</p>",
          "<p>AWS Global Accelerator</p>",
          "<p>Amazon Route 53</p>",
          "<p>Amazon S3 Transfer Acceleration</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A solutions architect is optimizing a website for real-time streaming and on-demand videos. The website’s users are located around the world and the solutions architect needs to optimize the performance for both the real-time and on-demand streaming.Which service should the solutions architect choose?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399082,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A shared services VPC is being setup for use by several AWS accounts. An application needs to be securely shared from the shared services VPC. The solution should not allow consumers to connect to other instances in the VPC.</p><p>How can this be setup with the least administrative effort? (choose 2)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>VPCs can be shared among multiple AWS accounts. Resources can then be shared amongst those accounts. However, to restrict access so that consumers cannot connect to other instances in the VPC the best solution is to use PrivateLink to create an endpoint for the application. The endpoint type will be an interface endpoint and it uses an NLB in the shared services VPC.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-25_01-23-21-8df694147b53b155434524cea44ac356.jpg\"></p><p><strong>CORRECT: </strong>\"Create a Network Load Balancer (NLB)\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Use AWS PrivateLink to expose the application as an endpoint service\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS ClassicLink to expose the application as an endpoint service\" is incorrect. ClassicLink allows you to link EC2-Classic instances to a VPC in your account, within the same region. This solution does not include EC2-Classic which is now deprecated (replaced by VPC).</p><p><strong>INCORRECT:</strong> \"Setup VPC peering between each AWS VPC\" is incorrect. VPC peering could be used along with security groups to restrict access to the application and other instances in the VPC. However, this would be administratively difficult as you would need to ensure that you maintain the security groups as resources and addresses change.</p><p><strong>INCORRECT:</strong> \"Configure security groups to restrict access\" is incorrect. This could be used in conjunction with VPC peering but better method is to use PrivateLink for this use case.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-virtual-private-clouds-can-now-be-shared-with-other-aws-accounts/\">https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-virtual-private-clouds-can-now-be-shared-with-other-aws-accounts/</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/\">https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/aws-privatelink.pdf\">https://d1.awsstatic.com/whitepapers/aws-privatelink.pdf</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
        "answers": [
          "<p>Create a Network Load Balancer (NLB)</p>",
          "<p>Use AWS PrivateLink to expose the application as an endpoint service</p>",
          "<p>Use AWS ClassicLink to expose the application as an endpoint service</p>",
          "<p>Setup VPC peering between each AWS VPC</p>",
          "<p>Configure security groups to restrict access</p>"
        ]
      },
      "correct_response": ["a", "b"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A shared services VPC is being setup for use by several AWS accounts. An application needs to be securely shared from the shared services VPC. The solution should not allow consumers to connect to other instances in the VPC.How can this be setup with the least administrative effort? (choose 2)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399084,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A web application is being deployed on an Amazon ECS cluster using the Fargate launch type. The application is expected to receive a large volume of traffic initially. The company wishes to ensure that performance is good for the launch and that costs reduce as demand decreases</p><p>What should a solutions architect recommend?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon ECS uses the AWS Application Auto Scaling service to scales tasks. This is configured through Amazon ECS using Amazon ECS Service Auto Scaling.</p><p>A Target Tracking Scaling policy increases or decreases the number of tasks that your service runs based on a target value for a specific metric. For example, in the image below the tasks will be scaled when the average CPU breaches 80% (as reported by CloudWatch):</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-11-27_10-01-16-332b18084ef8decec084431b6cc2ec4c.jpg\"></p><p><strong>CORRECT: </strong>\"Use Amazon ECS Service Auto Scaling with target tracking policies to scale when an Amazon CloudWatch alarm is breached\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 Auto Scaling with simple scaling policies to scale when an Amazon CloudWatch alarm is breached\" is incorrect</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 Auto Scaling to scale out on a schedule and back in once the load decreases\" is incorrect</p><p><strong>INCORRECT:</strong> \"Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon CloudWatch alarm\" is incorrect</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
        "answers": [
          "<p>Use Amazon EC2 Auto Scaling to scale out on a schedule and back in once the load decreases.</p>",
          "<p>Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon CloudWatch alarm.</p>",
          "<p>Use Amazon ECS Service Auto Scaling with target tracking policies to scale when an Amazon CloudWatch alarm is breached.</p>",
          "<p>Use Amazon EC2 Auto Scaling with simple scaling policies to scale when an Amazon CloudWatch alarm is breached.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Compute",
      "question_plain": "A web application is being deployed on an Amazon ECS cluster using the Fargate launch type. The application is expected to receive a large volume of traffic initially. The company wishes to ensure that performance is good for the launch and that costs reduce as demand decreasesWhat should a solutions architect recommend?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399086,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company runs several NFS file servers in an on-premises data center. The NFS servers must run periodic backups to Amazon S3 using automatic synchronization for small volumes of data.</p><p>Which solution meets these requirements and is MOST cost-effective?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data between on-premises systems and AWS Storage services, as well as between AWS Storage services. DataSync can copy data between Network File System (NFS) shares, or Server Message Block (SMB) shares, self-managed object storage, <a href=\"https://aws.amazon.com/snowcone/\">AWS Snowcone</a>, <a href=\"https://aws.amazon.com/s3/\">Amazon Simple Storage Service (Amazon S3)</a> buckets, <a href=\"https://aws.amazon.com/efs/\">Amazon Elastic File System (Amazon EFS)</a> file systems, and <a href=\"https://aws.amazon.com/fsx/windows/\">Amazon FSx for Windows File Server</a> file systems.</p><p>This is the most cost-effective solution from the answer options available.</p><p><strong>CORRECT: </strong>\"Set up an AWS DataSync agent on the on-premises servers and sync the data to Amazon S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Set up an SFTP sync using AWS Transfer for SFTP to sync data from on premises to Amazon S3\" is incorrect. This solution does not provide the scheduled synchronization features of AWS DataSync and is more expensive.</p><p><strong>INCORRECT:</strong> \"Set up AWS Glue to extract the data from the NFS shares and load it into Amazon S3\" is incorrect. AWS Glue is an ETL service and cannot be used for copying data to Amazon S3 from NFS shares.</p><p><strong>INCORRECT:</strong> \"Set up an AWS Direct Connect connection between the on-premises data center and AWS and copy the data to Amazon S3\" is incorrect. An AWS Direct Connect connection is an expensive option and no solution is provided for automatic synchronization.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/datasync/features/\">https://aws.amazon.com/datasync/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
        "answers": [
          "<p>Set up AWS Glue to extract the data from the NFS shares and load it into Amazon S3.</p>",
          "<p>Set up an AWS DataSync agent on the on-premises servers and sync the data to Amazon S3.</p>",
          "<p>Set up an SFTP sync using AWS Transfer for SFTP to sync data from on premises to Amazon S3.</p>",
          "<p>Set up an AWS Direct Connect connection between the on-premises data center and AWS and copy the data to Amazon S3.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Storage",
      "question_plain": "A company runs several NFS file servers in an on-premises data center. The NFS servers must run periodic backups to Amazon S3 using automatic synchronization for small volumes of data.Which solution meets these requirements and is MOST cost-effective?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399088,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An organization plans to deploy a higher performance computing (HPC) workload on AWS using Linux. The HPC workload will use many Amazon EC2 instances and will generate a large quantity of small output files that must be stored in persistent storage for future use.</p><p>A Solutions Architect must design a solution that will enable the EC2 instances to access data using native file system interfaces and to store output files in cost-effective long-term storage.</p><p>Which combination of AWS services meets these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon FSx for Lustre is ideal for high performance computing (HPC) workloads running on Linux instances. FSx for Lustre provides a native file system interface and works as any file system does with your Linux operating system.</p><p>When linked to an Amazon S3 bucket, FSx for Lustre transparently presents objects as files, allowing you to run your workload without managing data transfer from S3.</p><p>This solution provides all requirements as it enables Linux workloads to use the native file system interfaces and to use S3 for long-term and cost-effective storage of output files.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-11-27_10-07-01-3904cc64f7ccc25ee4a538dcf782af68.jpg\"></p><p><strong>CORRECT: </strong>\"Amazon FSx for Lustre with Amazon S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon FSx for Windows File Server with Amazon S3\" is incorrect. This service should be used with Windows instances and does not integrate with S3.</p><p><strong>INCORRECT:</strong> \"Amazon EBS volumes with Amazon S3 Glacier\" is incorrect. EBS volumes do not provide the shared, high performance storage solution using file system interfaces.</p><p><strong>INCORRECT:</strong> \"AWS DataSync with Amazon S3 Intelligent tiering\" is incorrect. AWS DataSync is used for migrating / synchronizing data.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/lustre/\">https://aws.amazon.com/fsx/lustre/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
        "answers": [
          "<p>Amazon FSx for Lustre with Amazon S3.</p>",
          "<p>Amazon FSx for Windows File Server with Amazon S3.</p>",
          "<p>Amazon EBS volumes with Amazon S3 Glacier.</p>",
          "<p>AWS DataSync with Amazon S3 Intelligent tiering.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Storage",
      "question_plain": "An organization plans to deploy a higher performance computing (HPC) workload on AWS using Linux. The HPC workload will use many Amazon EC2 instances and will generate a large quantity of small output files that must be stored in persistent storage for future use.A Solutions Architect must design a solution that will enable the EC2 instances to access data using native file system interfaces and to store output files in cost-effective long-term storage.Which combination of AWS services meets these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399090,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application has been deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). A Solutions Architect must improve the security posture of the application and minimize the impact of a DDoS attack on resources.</p><p>Which of the following solutions is MOST effective?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>A rate-based rule tracks the rate of requests for each originating IP address, and triggers the rule action on IPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span.</p><p>You can use this type of rule to put a temporary block on requests from an IP address that's sending excessive requests. By default, AWS WAF aggregates requests based on the IP address from the web request origin, but you can configure the rule to use an IP address from an HTTP header, like X-Forwarded-For, instead.</p><p><strong>CORRECT: </strong>\"Configure an AWS WAF ACL with rate-based rules. Enable the WAF ACL on the Application Load Balancer\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a custom AWS Lambda function that monitors for suspicious traffic and modifies a network ACL when a potential DDoS attack is identified\" is incorrect. There’s not description here of how Lambda is going to monitor for traffic.</p><p><strong>INCORRECT:</strong> \"Enable VPC Flow Logs and store them in Amazon S3. Use Amazon Athena to parse the logs and identify and block potential DDoS attacks\" is incorrect. Amazon Athena is not able to block DDoS attacks, another service would be needed.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the Application Load Balancer and configure Amazon CloudWatch to monitor the access logs and trigger a Lambda function when potential attacks are identified. Configure the Lambda function to modify the ALBs security group and block the attack\" is incorrect. Access logs are exported to S3 but not to CloudWatch. Also, it would not be possible to block an attack from a specific IP using a security group (while still allowing any other source access) as they do not support deny rules.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>",
        "answers": [
          "<p>Configure an AWS WAF ACL with rate-based rules. Enable the WAF ACL on the Application Load Balancer.</p>",
          "<p>Create a custom AWS Lambda function that monitors for suspicious traffic and modifies a network ACL when a potential DDoS attack is identified.</p>",
          "<p>Enable VPC Flow Logs and store them in Amazon S3. Use Amazon Athena to parse the logs and identify and block potential DDoS attacks.</p>",
          "<p>Enable access logs on the Application Load Balancer and configure Amazon CloudWatch to monitor the access logs and trigger a Lambda function when potential attacks are identified. Configure the Lambda function to modify the ALBs security group and block the attack.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "An application has been deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). A Solutions Architect must improve the security posture of the application and minimize the impact of a DDoS attack on resources.Which of the following solutions is MOST effective?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399092,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An automotive company plans to implement IoT sensors in manufacturing equipment that will send data to AWS in real time. The solution must receive events in an ordered manner from each asset and ensure that the data is saved for future processing.</p><p>Which solution would be MOST efficient?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon Kinesis Data Streams is the ideal service for receiving streaming data. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream. Therefore, a separate partition (rather than shard) should be used for each equipment asset.</p><p>Amazon Kinesis Firehose can be used to receive streaming data from Data Streams and then load the data into Amazon S3 for future processing.</p><p><strong>CORRECT: </strong>\"Use Amazon Kinesis Data Streams for real-time events with a partition for each equipment asset. Use Amazon Kinesis Data Firehose to save data to Amazon S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams for real-time events with a shard for each equipment asset. Use Amazon Kinesis Data Firehose to save data to Amazon EBS\" is incorrect. A partition should be used rather than a shard as explained above.</p><p><strong>INCORRECT:</strong> \"Use an Amazon SQS FIFO queue for real-time events with one queue for each equipment asset. Trigger an AWS Lambda function for the SQS queue to save data to Amazon EFS\" is incorrect. Amazon SQS cannot be used for real-time use cases.</p><p><strong>INCORRECT:</strong> \"Use an Amazon SQS standard queue for real-time events with one queue for each equipment asset. Trigger an AWS Lambda function from the SQS queue to save data to Amazon S3\" is incorrect. Amazon SQS cannot be used for real-time use cases.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p><p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
        "answers": [
          "<p>Use Amazon Kinesis Data Streams for real-time events with a partition for each equipment asset. Use Amazon Kinesis Data Firehose to save data to Amazon S3.</p>",
          "<p>Use Amazon Kinesis Data Streams for real-time events with a shard for each equipment asset. Use Amazon Kinesis Data Firehose to save data to Amazon EBS.</p>",
          "<p>Use an Amazon SQS FIFO queue for real-time events with one queue for each equipment asset. Trigger an AWS Lambda function for the SQS queue to save data to Amazon EFS.</p>",
          "<p>Use an Amazon SQS standard queue for real-time events with one queue for each equipment asset. Trigger an AWS Lambda function from the SQS queue to save data to Amazon S3.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Application Integration",
      "question_plain": "An automotive company plans to implement IoT sensors in manufacturing equipment that will send data to AWS in real time. The solution must receive events in an ordered manner from each asset and ensure that the data is saved for future processing.Which solution would be MOST efficient?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399094,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An IoT sensor is being rolled out to thousands of a company’s existing customers. The sensors will stream high volumes of data each second to a central location. A solution must be designed to ingest and store the data for analytics. The solution must provide near-real time performance and millisecond responsiveness.</p><p>Which solution should a Solutions Architect recommend?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>A Kinesis data stream is a set of shards. Each shard contains a sequence of data records. A <strong>consumer</strong> is an application that processes the data from a Kinesis data stream. You can map a Lambda function to a shared-throughput consumer (standard iterator), or to a dedicated-throughput consumer with enhanced fan-out.</p><p>Amazon DynamoDB is the best database for this use case as it supports near-real time performance and millisecond responsiveness.</p><p><strong>CORRECT: </strong>\"Ingest the data into an Amazon Kinesis Data Stream. Process the data with an AWS Lambda function and then store the data in Amazon DynamoDB\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Ingest the data into an Amazon Kinesis Data Stream. Process the data with an AWS Lambda function and then store the data in Amazon RedShift\" is incorrect. Amazon RedShift cannot provide millisecond responsiveness.</p><p><strong>INCORRECT:</strong> \"Ingest the data into an Amazon SQS queue. Process the data using an AWS Lambda function and then store the data in Amazon RedShift\" is incorrect. Amazon SQS does not provide near real-time performance and RedShift does not provide millisecond responsiveness.</p><p><strong>INCORRECT:</strong> \"Ingest the data into an Amazon SQS queue. Process the data using an AWS Lambda function and then store the data in Amazon DynamoDB\" is incorrect. Amazon SQS does not provide near real-time performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
        "answers": [
          "<p>Ingest the data into an Amazon SQS queue. Process the data using an AWS Lambda function and then store the data in Amazon RedShift.</p>",
          "<p>Ingest the data into an Amazon Kinesis Data Stream. Process the data with an AWS Lambda function and then store the data in Amazon DynamoDB.</p>",
          "<p>Ingest the data into an Amazon SQS queue. Process the data using an AWS Lambda function and then store the data in Amazon DynamoDB.</p>",
          "<p>Ingest the data into an Amazon Kinesis Data Stream. Process the data with an AWS Lambda function and then store the data in Amazon RedShift.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Application Integration",
      "question_plain": "An IoT sensor is being rolled out to thousands of a company’s existing customers. The sensors will stream high volumes of data each second to a central location. A solution must be designed to ingest and store the data for analytics. The solution must provide near-real time performance and millisecond responsiveness.Which solution should a Solutions Architect recommend?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399096,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company runs a number of core enterprise applications in an on-premises data center. The data center is connected to an Amazon VPC using AWS Direct Connect. The company will be creating additional AWS accounts and these accounts will also need to be quickly, and cost-effectively connected to the on-premises data center in order to access the core applications.</p><p>What deployment changes should a Solutions Architect implement to meet these requirements with the LEAST operational overhead?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Transit Gateway connects VPCs and on-premises networks through a central hub. With AWS Transit Gateway, you can quickly add Amazon VPCs, AWS accounts, VPN capacity, or AWS Direct Connect gateways to meet unexpected demand, without having to wrestle with complex connections or massive routing tables. This is the operationally least complex solution and is also cost-effective.</p><p>The image below depicts how transit gateway can assist with simplifying network deployments:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-11-27_10-12-08-05293542d6a09dfb24fcf7e9a59f123e.jpg\"></p><p><strong>CORRECT: </strong>\"Configure AWS Transit Gateway between the accounts. Assign Direct Connect to the transit gateway and route network traffic to the on-premises servers\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a VPN connection between each new account and the Direct Connect VPC. Route the network traffic to the on-premises servers\" is incorrect. You cannot connect VPCs using AWS managed VPNs and would need to configure a software VPN and then complex routing configurations. This is not the best solution.</p><p><strong>INCORRECT:</strong> \"Create a Direct Connect connection in each new account. Route the network traffic to the on-premises servers\" is incorrect. This is an expensive solution as you would need to have multiple Direct Connect links.</p><p><strong>INCORRECT:</strong> \"Configure VPC endpoints in the Direct Connect VPC for all required services. Route the network traffic to the on-premises servers\" is incorrect. You cannot create VPC endpoints for all services and this would be a complex solution for those you can.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-direct-connect/\">https://digitalcloud.training/aws-direct-connect/</a></p>",
        "answers": [
          "<p>Create a Direct Connect connection in each new account. Route the network traffic to the on-premises servers.</p>",
          "<p>Configure VPC endpoints in the Direct Connect VPC for all required services. Route the network traffic to the on-premises servers.</p>",
          "<p>Create a VPN connection between each new account and the Direct Connect VPC. Route the network traffic to the on-premises servers.</p>",
          "<p>Configure AWS Transit Gateway between the accounts. Assign Direct Connect to the transit gateway and route network traffic to the on-premises servers.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A company runs a number of core enterprise applications in an on-premises data center. The data center is connected to an Amazon VPC using AWS Direct Connect. The company will be creating additional AWS accounts and these accounts will also need to be quickly, and cost-effectively connected to the on-premises data center in order to access the core applications.What deployment changes should a Solutions Architect implement to meet these requirements with the LEAST operational overhead?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399098,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A solutions architect has been tasked with designing a highly resilient hybrid cloud architecture connecting an on-premises data center and AWS. The network should include AWS Direct Connect (DX).</p><p>Which DX configuration offers the HIGHEST resiliency?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The most resilient solution is to configure DX connections at multiple DX locations. This ensures that any issues impacting a single DX location do not affect availability of the network connectivity to AWS.</p><p>Take note of the following AWS recommendations for resiliency:</p><p><em>AWS recommends connecting from multiple data centers for physical location redundancy. When designing remote connections, consider using redundant hardware and telecommunications providers. Additionally, it is a best practice to use dynamically routed, active/active connections for automatic load balancing and failover across redundant network connections. Provision sufficient network capacity to ensure that the failure of one network connection does not overwhelm and degrade redundant connections.</em></p><p>The diagram below is an example of an architecture that offers high resiliency:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-11-27_10-13-36-8f8a2d3dea9984331c8894ec400a4303.jpg\"></p><p><strong>CORRECT: </strong>\"Configure DX connections at multiple DX locations\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure a DX connection with an encrypted VPN on top of it\" is incorrect. A VPN that is separate to the DX connection can be a good backup. But a VPN on top of the DX connection does not help. Also, encryption provides security but not resilience.</p><p><strong>INCORRECT:</strong> \"Configure multiple public VIFs on top of a DX connection\" is incorrect. Virtual interfaces do not add resiliency as resiliency must be designed into the underlying connection.</p><p><strong>INCORRECT:</strong> \"Configure multiple private VIFs on top of a DX connection\" is incorrect. Virtual interfaces do not add resiliency as resiliency must be designed into the underlying connection.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-direct-connect/\">https://digitalcloud.training/aws-direct-connect/</a></p>",
        "answers": [
          "<p>Configure a DX connection with an encrypted VPN on top of it.</p>",
          "<p>Configure multiple public VIFs on top of a DX connection.</p>",
          "<p>Configure multiple private VIFs on top of a DX connection.</p>",
          "<p>Configure DX connections at multiple DX locations.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A solutions architect has been tasked with designing a highly resilient hybrid cloud architecture connecting an on-premises data center and AWS. The network should include AWS Direct Connect (DX).Which DX configuration offers the HIGHEST resiliency?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399100,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A group of business analysts perform read-only SQL queries on an Amazon RDS database. The queries have become quite numerous and the database has experienced some performance degradation. The queries must be run against the latest data. A Solutions Architect must solve the performance problems with minimal changes to the existing web application.</p><p>What should the Solutions Architect recommend?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The performance issues can be easily resolved by offloading the SQL queries the business analysts are performing to a read replica. This ensures that data that is being queries is up to date and the existing web application does not require any modifications to take place.</p><p><strong>CORRECT: </strong>\"Create a read replica of the primary database and instruct the business analysts to direct queries to the replica\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Export the data to Amazon S3 and instruct the business analysts to run their queries using Amazon Athena\" is incorrect. The data must be the latest data and this method would therefore require constant exporting of the data.</p><p><strong>INCORRECT:</strong> \"Load the data into an Amazon Redshift cluster and instruct the business analysts to run their queries against the cluster\" is incorrect. This is another solution that requires exporting the loading the data which means over time it will become out of date.</p><p><strong>INCORRECT:</strong> \"Load the data into Amazon ElastiCache and instruct the business analysts to run their queries against the ElastiCache endpoint\" is incorrect. It will be much easier to create a read replica. ElastiCache requires updates to the application code so should be avoided in this example.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
        "answers": [
          "<p>Export the data to Amazon S3 and instruct the business analysts to run their queries using Amazon Athena.</p>",
          "<p>Load the data into an Amazon Redshift cluster and instruct the business analysts to run their queries against the cluster.</p>",
          "<p>Load the data into Amazon ElastiCache and instruct the business analysts to run their queries against the ElastiCache endpoint.</p>",
          "<p>Create a read replica of the primary database and instruct the business analysts to direct queries to the replica.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Database",
      "question_plain": "A group of business analysts perform read-only SQL queries on an Amazon RDS database. The queries have become quite numerous and the database has experienced some performance degradation. The queries must be run against the latest data. A Solutions Architect must solve the performance problems with minimal changes to the existing web application.What should the Solutions Architect recommend?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399102,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company is planning to upload a large quantity of sensitive data to Amazon S3. The company’s security department require that the data is encrypted before it is uploaded.</p><p>Which option meets these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The requirement is that the objects must be encrypted before they are uploaded. The only option presented that meets this requirement is to use client-side encryption. You then have two options for the keys you use to perform the encryption:</p><p> • Use a customer master key (CMK) stored in AWS Key Management Service (AWS KMS).</p><p> • Use a master key that you store within your application.</p><p>In this case the correct answer is to use an AWS KMS key. Note that you cannot use client-side encryption with keys managed by Amazon S3.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-00-48-9dad6f62c8e7c9414b51a788e346c78c.jpg\"></p><p><strong>CORRECT: </strong>\"Use client-side encryption with a master key stored in AWS KMS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use client-side encryption with Amazon S3 managed encryption keys\" is incorrect. You cannot use S3 managed keys with client-side encryption.</p><p><strong>INCORRECT:</strong> \"Use server-side encryption with customer-provided encryption keys\" is incorrect. With this option the encryption takes place after uploading to S3.</p><p><strong>INCORRECT:</strong> \"Use server-side encryption with keys stored in KMS\" is incorrect. With this option the encryption takes place after uploading to S3.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSideEncryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
        "answers": [
          "<p>Use server-side encryption with customer-provided encryption keys.</p>",
          "<p>Use client-side encryption with a master key stored in AWS KMS.</p>",
          "<p>Use client-side encryption with Amazon S3 managed encryption keys.</p>",
          "<p>Use server-side encryption with keys stored in KMS.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Storage",
      "question_plain": "A company is planning to upload a large quantity of sensitive data to Amazon S3. The company’s security department require that the data is encrypted before it is uploaded.Which option meets these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399104,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application running on Amazon ECS processes data and then writes objects to an Amazon S3 bucket. The application requires permissions to make the S3 API calls.</p><p>How can a Solutions Architect ensure the application has the required permissions?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task. Applications must sign their AWS API requests with AWS credentials, and this feature provides a strategy for managing credentials for your applications to use, similar to the way that Amazon EC2 instance profiles provide credentials to EC2 instances.</p><p>You define the IAM role to use in your task definitions, or you can use a taskRoleArn override when running a task manually with the RunTask API operation.</p><p>Note that there are instances roles and task roles that you can assign in ECS when using the EC2 launch type. The task role is better when you need to assign permissions for just that specific task:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-02-13-cca043ef28407cf8376847f12e461e32.jpg\"></p><p><strong>CORRECT: </strong>\"Create an IAM role that has read/write permissions to the bucket and update the task definition to specify the role as the taskRoleArn\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Update the S3 policy in IAM to allow read/write access from Amazon ECS, and then relaunch the container\" is incorrect. Policies must be assigned to tasks using IAM Roles and this is not mentioned here.</p><p><strong>INCORRECT:</strong> \"Create a set of Access Keys with read/write permissions to the bucket and update the task credential ID\" is incorrect. You cannot update the task credential ID with access keys and roles should be used instead.</p><p><strong>INCORRECT:</strong> \"Attach an IAM policy with read/write permissions to the bucket to an IAM group and add the container instances to the group\" is incorrect. You cannot add container instances to an IAM group.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
        "answers": [
          "<p>Update the S3 policy in IAM to allow read/write access from Amazon ECS, and then relaunch the container.</p>",
          "<p>Create a set of Access Keys with read/write permissions to the bucket and update the task credential ID.</p>",
          "<p>Create an IAM role that has read/write permissions to the bucket and update the task definition to specify the role as the taskRoleArn.</p>",
          "<p>Attach an IAM policy with read/write permissions to the bucket to an IAM group and add the container instances to the group.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "An application running on Amazon ECS processes data and then writes objects to an Amazon S3 bucket. The application requires permissions to make the S3 API calls.How can a Solutions Architect ensure the application has the required permissions?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399106,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application upgrade caused some issues with stability. The application owner enabled logging and has generated a 5 GB log file in an Amazon S3 bucket. The log file must be securely shared with the application vendor to troubleshoot the issues.</p><p>What is the MOST secure way to share the log file?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>A presigned URL gives you access to the object identified in the URL. When you create a presigned URL, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (PUT for uploading objects), and an expiration date and time. The presigned URLs are valid only for the specified duration. That is, you must start the action before the expiration date and time.</p><p>This is the most secure way to provide the vendor with time-limited access to the log file in the S3 bucket.</p><p><strong>CORRECT: </strong>\"Generate a presigned URL and ask the vendor to download the log file before the URL expires\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an IAM user for the vendor to provide access to the S3 bucket and the application. Enforce multi-factor authentication\" is incorrect. This is less secure as you have to create an account to access AWS and then ensure you lock down the account appropriately.</p><p><strong>INCORRECT:</strong> \"Create access keys using an administrative account and share the access key ID and secret access key with the vendor\" is incorrect. This is extremely insecure as the access keys will provide administrative permissions to AWS and should never be shared.</p><p><strong>INCORRECT:</strong> \"Enable default encryption for the bucket and public access. Provide the S3 URL of the file to the vendor\" is incorrect. Encryption does not assist here as the bucket would be public and anyone could access it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
        "answers": [
          "<p>Create access keys using an administrative account and share the access key ID and secret access key with the vendor.</p>",
          "<p>Enable default encryption for the bucket and public access. Provide the S3 URL of the file to the vendor.</p>",
          "<p>Create an IAM user for the vendor to provide access to the S3 bucket and the application. Enforce multi-factor authentication.</p>",
          "<p>Generate a presigned URL and ask the vendor to download the log file before the URL expires.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Storage",
      "question_plain": "An application upgrade caused some issues with stability. The application owner enabled logging and has generated a 5 GB log file in an Amazon S3 bucket. The log file must be securely shared with the application vendor to troubleshoot the issues.What is the MOST secure way to share the log file?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399108,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company has a file share on a Microsoft Windows Server in an on-premises data center. The server uses a local network attached storage (NAS) device to store several terabytes of files. The management team require a reduction in the data center footprint and to minimize storage costs by moving on-premises storage to AWS.</p><p>What should a Solutions Architect do to meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>An AWS Storage Gateway File Gateway provides your applications a file interface to seamlessly store files as objects in Amazon S3, and access them using industry standard file protocols. This removes the files from the on-premises NAS device and provides a method of directly mounting the file share for on-premises servers and clients.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-04-55-d6065baf4b15649e450a4fb5cd195708.jpg\"></p><p><strong>CORRECT: </strong>\"Configure an AWS Storage Gateway file gateway\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure an AWS Storage Gateway as a volume gateway\" is incorrect. A volume gateway uses block-based protocols. In this case we are replacing a NAS device which uses file-level protocols so the best option is a file gateway.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EFS volume and use an IPSec VPN\" is incorrect. EFS can be mounted over a VPN but it would have more latency than using a storage gateway.</p><p><strong>INCORRECT:</strong> \"Create an Amazon S3 bucket and an S3 gateway endpoint\" is incorrect. S3 is an object-level storage system so is not suitable for this use case. A gateway endpoint is a method of accessing S3 using private addresses from your VPC, not from your data center.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/storagegateway/faqs/\">https://aws.amazon.com/storagegateway/faqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>",
        "answers": [
          "<p>Create an Amazon EFS volume and use an IPSec VPN.</p>",
          "<p>Configure an AWS Storage Gateway file gateway.</p>",
          "<p>Create an Amazon S3 bucket and an S3 gateway endpoint.</p>",
          "<p>Configure an AWS Storage Gateway as a volume gateway.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Storage",
      "question_plain": "A company has a file share on a Microsoft Windows Server in an on-premises data center. The server uses a local network attached storage (NAS) device to store several terabytes of files. The management team require a reduction in the data center footprint and to minimize storage costs by moving on-premises storage to AWS.What should a Solutions Architect do to meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399110,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company uses a Microsoft Windows file share for storing documents and media files. Users access the share using Microsoft Windows clients and are authenticated using the company’s Active Directory. The chief information officer wants to move the data to AWS as they are approaching capacity limits. The existing user authentication and access management system should be used.</p><p>How can a Solutions Architect meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon FSx for Windows File Server makes it easy for you to launch and scale reliable, performant, and secure shared file storage for your applications and end users. With Amazon FSx, you can launch highly durable and available file systems that can span multiple availability zones (AZs) and can be accessed from up to thousands of compute instances using the industry-standard Server Message Block (SMB) protocol.</p><p>It provides a rich set of administrative and security features, and integrates with Microsoft Active Directory (AD). To serve a wide spectrum of workloads, Amazon FSx provides high levels of file system throughput and IOPS and consistent sub-millisecond latencies.</p><p>You can also mount FSx file systems from on-premises using a VPN or Direct Connect connection. This topology is depicted in the image below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-06-44-85272267dfd4d12b26b830545075dc49.jpg\"></p><p><strong>CORRECT: </strong>\"Move the documents and media files to an Amazon FSx for Windows File Server file system\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Move the documents and media files to an Amazon FSx for Lustre file system\" is incorrect. FSx for Lustre is not suitable for migrating a Microsoft Windows File Server implementation.</p><p><strong>INCORRECT:</strong> \"Move the documents and media files to an Amazon Elastic File System and use POSIX permissions\" is incorrect. EFS can be used from on-premises over a VPN or DX connection but POSIX permissions are very different to Microsoft permissions and mean a different authentication and access management solution is required.</p><p><strong>INCORRECT:</strong> \"Move the documents and media files to an Amazon Simple Storage Service bucket and apply bucket ACLs\" is incorrect. S3 with bucket ACLs would be changing to an object-based storage system and a completely different authentication and access management solution.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/windows/features/?nc=sn&amp;loc=2\">https://aws.amazon.com/fsx/windows/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
        "answers": [
          "<p>Move the documents and media files to an Amazon FSx for Windows File Server file system.</p>",
          "<p>Move the documents and media files to an Amazon Elastic File System and use POSIX permissions.</p>",
          "<p>Move the documents and media files to an Amazon FSx for Lustre file system.</p>",
          "<p>Move the documents and media files to an Amazon Simple Storage Service bucket and apply bucket ACLs.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Storage",
      "question_plain": "A company uses a Microsoft Windows file share for storing documents and media files. Users access the share using Microsoft Windows clients and are authenticated using the company’s Active Directory. The chief information officer wants to move the data to AWS as they are approaching capacity limits. The existing user authentication and access management system should be used.How can a Solutions Architect meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399112,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company requires a solution for replicating data to AWS for disaster recovery. Currently, the company uses scripts to copy data from various sources to a Microsoft Windows file server in the on-premises data center. The company also requires that a small amount of recent files are accessible to administrators with low latency.</p><p>What should a Solutions Architect recommend to meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The best solution here is to use an AWS Storage Gateway File Gateway virtual appliance in the on-premises data center. This can be accessed the same protocols as the existing Microsoft Windows File Server (SMB/CIFS). Therefore, the script simply needs to be updated to point to the gateway.</p><p>The file gateway will then store data on Amazon S3 and has a local cache for data that can be accessed at low latency. The file gateway provides an excellent method of enabling file protocol access to low cost S3 object storage.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-10-11-1743a212516d41e3c51c5f97fc6d09c5.jpg\"></p><p><strong>CORRECT: </strong>\"Update the script to copy data to an AWS Storage Gateway for File Gateway virtual appliance instead of the on-premises file server\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Update the script to copy data to an Amazon EBS volume instead of the on-premises file server\" is incorrect. This would also need an attached EC2 instance running Windows to be able to mount using the same protocols and would not offer any local low-latency access.</p><p><strong>INCORRECT:</strong> \"Update the script to copy data to an Amazon EFS volume instead of the on-premises file server\" is incorrect. This solution would not provide a local cache.</p><p><strong>INCORRECT:</strong> \"Update the script to copy data to an Amazon S3 Glacier archive instead of the on-premises file server\" is incorrect. This would not provide any immediate access with low-latency.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
        "answers": [
          "<p>Update the script to copy data to an AWS Storage Gateway for File Gateway virtual appliance instead of the on-premises file server.</p>",
          "<p>Update the script to copy data to an Amazon EBS volume instead of the on-premises file server.</p>",
          "<p>Update the script to copy data to an Amazon EFS volume instead of the on-premises file server.</p>",
          "<p>Update the script to copy data to an Amazon S3 Glacier archive instead of the on-premises file server.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Storage",
      "question_plain": "A company requires a solution for replicating data to AWS for disaster recovery. Currently, the company uses scripts to copy data from various sources to a Microsoft Windows file server in the on-premises data center. The company also requires that a small amount of recent files are accessible to administrators with low latency.What should a Solutions Architect recommend to meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399114,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company runs an application in an Amazon VPC that requires access to an Amazon Elastic Container Service (Amazon ECS) cluster that hosts an application in another VPC. The company’s security team requires that all traffic must not traverse the internet.</p><p>Which solution meets this requirement?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The correct solution is to use AWS PrivateLink in a service provider model. In this configuration a network load balancer will be implemented in the service provider VPC (the one with the ECS cluster in this example), and a PrivateLink endpoint will be created in the consumer VPC (the one with the company’s application).</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-11-25-32f12f4c4d3d65de75af283af88992aa.jpg\"></p><p><strong>CORRECT: </strong>\"Create a Network Load Balancer in one VPC and an AWS PrivateLink endpoint for Amazon ECS in another VPC\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer and AWS PrivateLink endpoint for Amazon ECS in the VPC that hosts the ECS cluster\" is incorrect. The endpoint should be in the consumer VPC, not the service provider VPC (see the diagram above).</p><p><strong>INCORRECT:</strong> \"Configure a gateway endpoint for Amazon ECS. Update the route table to include an entry pointing to the ECS cluster\" is incorrect. You cannot use a gateway endpoint to connect to a private service. Gateway endpoints are only for S3 and DynamoDB.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon Route 53 private hosted zone for each VPC. Use private records to resolve internal IP addresses in each VPC\" is incorrect. This does not provide a mechanism for resolving each other’s addresses and there’s no method of internal communication using private IPs such as VPC peering.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/privatelink/\">https://aws.amazon.com/privatelink/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>Create a Network Load Balancer and AWS PrivateLink endpoint for Amazon ECS in the VPC that hosts the ECS cluster.</p>",
          "<p>Configure a gateway endpoint for Amazon ECS. Update the route table to include an entry pointing to the ECS cluster.</p>",
          "<p>Configure an Amazon Route 53 private hosted zone for each VPC. Use private records to resolve internal IP addresses in each VPC.</p>",
          "<p>Create a Network Load Balancer in one VPC and an AWS PrivateLink endpoint for Amazon ECS in another VPC.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A company runs an application in an Amazon VPC that requires access to an Amazon Elastic Container Service (Amazon ECS) cluster that hosts an application in another VPC. The company’s security team requires that all traffic must not traverse the internet.Which solution meets this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399116,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application stores transactional data in an Amazon S3 bucket. The data is analyzed for the first week and then must remain immediately available and highly available for occasional analysis.</p><p>What is the MOST cost-effective storage solution that meets the requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The transition should be to Standard-IA rather than One Zone-IA. Though One Zone-IA would be cheaper, it also offers lower availability and the question states the objects “must remain immediately available”. Therefore the availability is a consideration.</p><p>Though there is no minimum duration when storing data in S3 Standard, you cannot transition to Standard IA within 30 days. This can be seen when trying to create a lifecycle rule:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-12-38-75ade2c799e4d5cf4b201792e2ee34b6.jpg\"></p><p><strong>Therefore, the best solution is to transition after 30 days.</strong></p><p><strong>CORRECT: </strong>\"Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days\" is incorrect as explained above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
        "answers": [
          "<p>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days.</p>",
          "<p>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days.</p>",
          "<p>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.</p>",
          "<p>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "An application stores transactional data in an Amazon S3 bucket. The data is analyzed for the first week and then must remain immediately available and highly available for occasional analysis.What is the MOST cost-effective storage solution that meets the requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399118,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A highly sensitive application runs on Amazon EC2 instances using EBS volumes. The application stores data temporarily on Amazon EBS volumes during processing before saving results to an Amazon RDS database. The company’s security team mandate that the sensitive data must be encrypted at rest.</p><p>Which solution should a Solutions Srchitect recommend to meet this requirement?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>As the data is stored both in the EBS volumes (temporarily) and the RDS database, both the EBS and RDS volumes must be encrypted at rest. This can be achieved by enabling encryption at creation time of the volume and AWS KMS keys can be used to encrypt the data. This solution meets all requirements.</p><p><strong>CORRECT: </strong>\"Configure encryption for the Amazon EBS volumes and Amazon RDS database with AWS KMS keys\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS Certificate Manager to generate certificates that can be used to encrypt the connections between the EC2 instances and RDS\" is incorrect. This would encrypt the data in-transit but not at-rest.</p><p><strong>INCORRECT:</strong> \"Use Amazon Data Lifecycle Manager to encrypt all data as it is stored to the EBS volumes and RDS database\" is incorrect. DLM is used for automating the process of taking and managing snapshots for EBS volumes.</p><p><strong>INCORRECT:</strong> \"Configure SSL/TLS encryption using AWS KMS customer master keys (CMKs) to encrypt database volumes\" is incorrect. You cannot configure SSL/TLS encryption using KMS CMKs or use SSL/TLS to encrypt data at rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
        "answers": [
          "<p>Configure encryption for the Amazon EBS volumes and Amazon RDS database with AWS KMS keys.</p>",
          "<p>Use AWS Certificate Manager to generate certificates that can be used to encrypt the connections between the EC2 instances and RDS.</p>",
          "<p>Use Amazon Data Lifecycle Manager to encrypt all data as it is stored to the EBS volumes and RDS database.</p>",
          "<p>Configure SSL/TLS encryption using AWS KMS customer master keys (CMKs) to encrypt database volumes.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A highly sensitive application runs on Amazon EC2 instances using EBS volumes. The application stores data temporarily on Amazon EBS volumes during processing before saving results to an Amazon RDS database. The company’s security team mandate that the sensitive data must be encrypted at rest.Which solution should a Solutions Srchitect recommend to meet this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399120,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company runs an eCommerce application that uses an Amazon Aurora database. The database performs well except for short periods when monthly sales reports are run. A Solutions Architect has reviewed metrics in Amazon CloudWatch and found that the Read Ops and CPUUtilization metrics are spiking during the periods when the sales reports are run.</p><p>What is the MOST cost-effective solution to solve this performance issue?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The simplest and most cost-effective option is to use an Aurora Replica. The replica can serve read operations which will mean the reporting application can run reports on the replica endpoint without causing any performance impact on the production database.</p><p><strong>CORRECT: </strong>\"Create an Aurora Replica and use the replica endpoint for reporting\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable storage Auto Scaling for the Amazon Aurora database\" is incorrect. Aurora storage automatically scales based on volumes, there is no storage auto scaling feature for Aurora.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Redshift data warehouse and run the reporting there\" is incorrect. This would be less cost-effective and require more work in copying the data to the data warehouse.</p><p><strong>INCORRECT:</strong> \"Modify the Aurora database to use an instance class with more CPU\" is incorrect. This may not resolve the storage performance issues and could be more expensive depending on instances sizes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
        "answers": [
          "<p>Create an Amazon Redshift data warehouse and run the reporting there.</p>",
          "<p>Modify the Aurora database to use an instance class with more CPU.</p>",
          "<p>Create an Aurora Replica and use the replica endpoint for reporting.</p>",
          "<p>Enable storage Auto Scaling for the Amazon Aurora database.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Database",
      "question_plain": "A company runs an eCommerce application that uses an Amazon Aurora database. The database performs well except for short periods when monthly sales reports are run. A Solutions Architect has reviewed metrics in Amazon CloudWatch and found that the Read Ops and CPUUtilization metrics are spiking during the periods when the sales reports are run.What is the MOST cost-effective solution to solve this performance issue?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399122,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A company runs an application on Amazon EC2 instances which requires access to sensitive data in an Amazon S3 bucket. All traffic between the EC2 instances and the S3 bucket must not traverse the internet and must use private IP addresses. Additionally, the bucket must only allow access from services in the VPC.</p><p>Which combination of actions should a Solutions Architect take to meet these requirements? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Private access to public services such as Amazon S3 can be achieved by creating a VPC endpoint in the VPC. For S3 this would be a gateway endpoint. The bucket policy can then be configured to restrict access to the S3 endpoint only which will ensure that only services originating from the VPC will be granted access.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-02-25_12-15-18-8ca86e9211911c46e6ebba4877ad1dca.jpg\"></p><p><strong>CORRECT: </strong>\"Create a VPC endpoint for Amazon S3\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Apply a bucket policy to restrict access to the S3 endpoint\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Enable default encryption on the bucket\" is incorrect. This will encrypt data at rest but does not restrict access.</p><p><strong>INCORRECT:</strong> \"Create a peering connection to the S3 bucket VPC\" is incorrect. You cannot create a peering connection to S3 as it is a public service and does not run in a VPC.</p><p><strong>INCORRECT:</strong> \"Apply an IAM policy to a VPC peering connection\" is incorrect. You cannot apply an IAM policy to a peering connection.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>Create a VPC endpoint for Amazon S3.</p>",
          "<p>Apply a bucket policy to restrict access to the S3 endpoint.</p>",
          "<p>Enable default encryption on the bucket.</p>",
          "<p>Create a peering connection to the S3 bucket VPC.</p>",
          "<p>Apply an IAM policy to a VPC peering connection.</p>"
        ]
      },
      "correct_response": ["a", "b"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "A company runs an application on Amazon EC2 instances which requires access to sensitive data in an Amazon S3 bucket. All traffic between the EC2 instances and the S3 bucket must not traverse the internet and must use private IP addresses. Additionally, the bucket must only allow access from services in the VPC.Which combination of actions should a Solutions Architect take to meet these requirements? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399124,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A company wants to migrate a legacy web application from an on-premises data center to AWS. The web application consists of a web tier, an application tier, and a MySQL database. The company does not want to manage instances or clusters.</p><p>Which combination of services should a solutions architect include in the overall architecture? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>Amazon RDS is a managed service and you do not need to manage the instances. This is an ideal backend for the application and you can run a MySQL database on RDS without any refactoring. For the application components these can run on Docker containers with AWS Fargate. Fargate is a serverless service for running containers on AWS.</p><p><strong>CORRECT: </strong>\"AWS Fargate\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Amazon RDS for MySQL\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon DynamoDB\" is incorrect. This is a NoSQL database and would be incompatible with the relational MySQL DB.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 Spot Instances\" is incorrect. This would require managing instances.</p><p><strong>INCORRECT:</strong> \"Amazon Kinesis Data Streams\" is incorrect. This is a service for streaming data.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/\">https://aws.amazon.com/rds/</a></p><p><a href=\"https://aws.amazon.com/fargate/\">https://aws.amazon.com/fargate/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
        "answers": [
          "<p>Amazon DynamoDB</p>",
          "<p>Amazon RDS for MySQL</p>",
          "<p>Amazon EC2 Spot Instances</p>",
          "<p>Amazon Kinesis Data Streams</p>",
          "<p>AWS Fargate</p>"
        ]
      },
      "correct_response": ["b", "e"],
      "section": "AWS Database",
      "question_plain": "A company wants to migrate a legacy web application from an on-premises data center to AWS. The web application consists of a web tier, an application tier, and a MySQL database. The company does not want to manage instances or clusters.Which combination of services should a solutions architect include in the overall architecture? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399126,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by midmorning</p><p>How should the scaling be changed to address the staff complaints and keep costs to a minimum?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Though this sounds like a good use case for scheduled actions, both answers using scheduled actions will have 20 instances running regardless of actual demand. A better option to be more cost effective is to use a target tracking action that triggers at a lower CPU threshold.</p><p>With this solution the scaling will occur before the CPU utilization gets to a point where performance is affected. This will result in resolving the performance issues whilst minimizing costs. Using a reduced cooldown period will also more quickly terminate unneeded instances, further reducing costs.</p><p><strong>CORRECT: </strong>\"Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens\" is incorrect as this is not the most cost-effective option. Note you can choose min, max, or desired for a scheduled action.</p><p><strong>INCORRECT:</strong> \"Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens\" is incorrect as this is not the most cost-effective option. Note you can choose min, max, or desired for a scheduled action.</p><p><strong>INCORRECT:</strong> \"Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period\" is incorrect as AWS recommend you use target tracking in place of step scaling for most use cases.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
        "answers": [
          "<p>Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens</p>",
          "<p>Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period</p>",
          "<p>Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period</p>",
          "<p>Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Compute",
      "question_plain": "A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by midmorningHow should the scaling be changed to address the staff complaints and keep costs to a minimum?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399128,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application has been migrated to Amazon EC2 Linux instances. The EC2 instances run several 1-hour tasks on a schedule. There is no common programming language among these tasks, as they were written by different teams. Currently, these tasks run on a single instance, which raises concerns about performance and scalability. To resolve these concerns, a solutions architect must implement a solution.</p><p>Which solution will meet these requirements with the LEAST Operational overhead?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The best solution is to create an AMI of the EC2 instance, and then use it as a template for which to launch additional instances using an Auto Scaling Group. This removes the issues of performance, scalability, and redundancy by allowing the EC2 instances to automatically scale and be launched across multiple Availability Zones.</p><p><strong>CORRECT: </strong>\"Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events)\" is incorrect. AWS Batch is designed to run jobs across multiple instances, there would be less operational overhead by creating an AMI instead.</p><p><strong>INCORRECT:</strong> \"Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs\" is incorrect. Converting your EC2 instances to containers is not the easiest way to achieve this task.</p><p><strong>INCORRECT:</strong> \"Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events)\" is incorrect. The maximum execution time for a Lambda function is 15 minutes, making it unsuitable for tasks running on a one-hour schedule.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
        "answers": [
          "<p>Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).</p>",
          "<p>Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs.</p>",
          "<p>Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events).</p>",
          "<p>Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Compute",
      "question_plain": "An application has been migrated to Amazon EC2 Linux instances. The EC2 instances run several 1-hour tasks on a schedule. There is no common programming language among these tasks, as they were written by different teams. Currently, these tasks run on a single instance, which raises concerns about performance and scalability. To resolve these concerns, a solutions architect must implement a solution.Which solution will meet these requirements with the LEAST Operational overhead?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399130,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company hosts a serverless application on AWS. The application consists of Amazon API Gateway, AWS Lambda, and Amazon RDS for PostgreSQL. During times of peak traffic and when traffic spikes are experienced, the company notices an increase in application errors caused by database connection timeouts. The company is looking for a solution that will reduce the number of application failures with the least amount of code changes.</p><p>What should a solutions architect do to meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon RDS Proxy is a fully managed, highly available database proxy for Amazon Relational Database Service (RDS) that makes applications more scalable, more resilient to database failures, and more secure. Amazon RDS Proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability.</p><p>Amazon RDS Proxy can be enabled for most applications with no code changes so this solution requires the least amount of code changes.</p><p><strong>CORRECT: </strong>\"Enable an RDS Proxy instance on your RDS Database\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Reduce the concurrency rate for your Lambda Function\" is incorrect. Concurrency is the number of requests that your function is serving at any given time. The errors are caused by an increase in connection timeouts, so editing the concurrency of your Lambda function would not solve the problem.</p><p><strong>INCORRECT:</strong> \"Change the class of the instance of your database to allow more connections\" is incorrect. Resizing the instance might help, but there will be some inevitable downtime with a PostgreSQL database on RDS. RDS Proxy is specifically designed for this reason and would incur no downtime.</p><p><strong>INCORRECT:</strong> \"Change the database to an Amazon DynamoDB database with on-demand scaling\" is incorrect as this would require significant application changes to accommodate the NoSQL database structure.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/proxy/\">https://aws.amazon.com/rds/proxy/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-database/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-database/</a></p>",
        "answers": [
          "<p>Reduce the concurrency rate for your Lambda Function.</p>",
          "<p>Enable an RDS Proxy instance on your RDS Database.</p>",
          "<p>Change the class of the instance of your database to allow more connections.</p>",
          "<p>Change the database to an Amazon DynamoDB database with on-demand scaling.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Database",
      "question_plain": "A company hosts a serverless application on AWS. The application consists of Amazon API Gateway, AWS Lambda, and Amazon RDS for PostgreSQL. During times of peak traffic and when traffic spikes are experienced, the company notices an increase in application errors caused by database connection timeouts. The company is looking for a solution that will reduce the number of application failures with the least amount of code changes.What should a solutions architect do to meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399132,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A gaming company uses a web application to display scores. An Application Load Balancer is used to distribute load across Amazon EC2 instances which run the application. The application stores data in an Amazon RDS for MySQL database. Users are experiencing long delays and interruptions due to poor database read performance. It is important for the company to improve the user experience while minimizing changes to the application's architecture.</p><p>What should a solutions architect do to meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon ElastiCache is a fully managed, in-memory caching service supporting flexible, real-time use cases. You can use ElastiCache for caching, which accelerates application and database performance, or as a primary data store for use cases that don't require durability like session stores, gaming leaderboards, streaming, and analytics. ElastiCache is compatible with Redis and Memcached.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-27_10-08-02-b5493317752977ece44ad2ece006f1c3.jpg\"><p>As the issues in this instance are caused by poor read performance, a caching solution would offload reads from the primary database instance and allow the application to perform better.<br><strong>CORRECT: </strong>\"Use Amazon ElastiCache to cache the database layer” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Connect the database and the application layer using RDS Proxy” is incorrect. RDS proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability. It does not however specifically improve read performance like a caching layer would.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda instead of Amazon EC2 for the compute layer\" is incorrect. AWS Lambda would not be a suitable use case for hosting leaderboards, as the maximum timeout is 15 minutes, and the issue lies with the database layer, not the compute later.</p><p><strong>INCORRECT:</strong> \"Use an Amazon DynamoDB table instead of RDS\" is incorrect. Migrating to DynamoDB would not help the load of reads on the database and changing the schema of the database would cause massive changes to the application’s architecture.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
        "answers": [
          "<p>Use Amazon ElastiCache to cache the database layer.</p>",
          "<p>Connect the database and the application layer using RDS Proxy.</p>",
          "<p>Use AWS Lambda instead of Amazon EC2 for the compute layer.</p>",
          "<p>Use an Amazon DynamoDB table instead of RDS.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Database",
      "question_plain": "A gaming company uses a web application to display scores. An Application Load Balancer is used to distribute load across Amazon EC2 instances which run the application. The application stores data in an Amazon RDS for MySQL database. Users are experiencing long delays and interruptions due to poor database read performance. It is important for the company to improve the user experience while minimizing changes to the application's architecture.What should a solutions architect do to meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399134,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A solutions architect is required to move 750 TB of data from a branch office's network-attached file system to Amazon S3 Glacier. The branch office’s internet connection is poor, and the solution must not saturate the connection. Normal business traffic loads must not be affected by the migration.</p><p>What is the MOST cost-effective solution?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The AWS Snow Family consists of several physical devices which can be used for edge computing, and to migrate large amounts of data into Amazon S3.</p><p>The process for using AWS Snowball is as follows:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-27_10-10-17-cd2ebeb567b249ba6ff7c90a61e6d34c.jpg\"><p>The solutions architect will need 10 Snowball devices to fulfill the capacity required in this instance as each Snowball contains up to 80TB of usable storage.</p><p><strong>CORRECT: </strong>\"Order 10 AWS Snowball appliances and select an Amazon S3 bucket as the destination. Create a lifecycle policy to transition the S3 objects to Amazon S3 Glacier” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a site-to-site VPN connection directly to an Amazon S3 bucket, Enforce the connection with an VPC Endpoint\" is incorrect, as although this would achieve the solution, it would be using the branch’s internet connection and would saturate it - preventing normal business activities from taking place.</p><p><strong>INCORRECT:</strong> \"Order 10 AWS Snowball appliances and point these appliances to an S3 Glacier vault and put in place a bucket policy which will only allow access via a VPC endpoint\" is incorrect as S3 Glacier is not a viable destination for Snowball, and you need to place it into S3 Standard first then transition the data in Glacier after the fact.</p><p><strong>INCORRECT:</strong> \"Copy the files directly from the network-attached file system to Amazon S3. Build a lifecycle policy to move the S3 objects across storage classes into Amazon S3 Glacier\" is incorrect. It is not possible to mount third-party NAS appliance to an S3 bucket. You could use a service like AWS DataSync to move data from the network attached file system into S3, however this would still be traveling over the branch’s internet line.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
        "answers": [
          "<p>Create a site-to-site VPN connection directly to an Amazon S3 bucket, Enforce the connection with an VPC Endpoint.</p>",
          "<p>Order 10 AWS Snowball appliances and point these appliances to an S3 Glacier vault and put in place a bucket policy which will only allow access via a VPC endpoint.</p>",
          "<p>Copy the files directly from the network-attached file system to Amazon S3. Build a lifecycle policy to move the S3 objects across storage classes into Amazon S3 Glacier.</p>",
          "<p>Order 10 AWS Snowball appliances and select an Amazon S3 bucket as the destination. Create a lifecycle policy to transition the S3 objects to Amazon S3 Glacier.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Migration & Transfer",
      "question_plain": "A solutions architect is required to move 750 TB of data from a branch office's network-attached file system to Amazon S3 Glacier. The branch office’s internet connection is poor, and the solution must not saturate the connection. Normal business traffic loads must not be affected by the migration.What is the MOST cost-effective solution?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399136,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>To accelerate experimentation and agility, a company allows developers to apply existing IAM policies to existing IAM roles. Nevertheless, the security operations team is concerned that the developers could attach the existing administrator policy, circumventing any other security policies.</p><p>How should a solutions architect address this issue?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Setting a permissions boundary is the easiest and safest way to ensure that any IAM users cannot assume any elevated permissions. A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.</p><p><strong>CORRECT: </strong>\"Set a permissions boundary on the developer IAM role that denies attaching administrator access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Send an alert every time a developer creates a new policy using an Amazon SNS topic” is incorrect. This does not explicitly prevent any developers from attaching the policy, only sending a notification.</p><p><strong>INCORRECT:</strong> \"Disable IAM activity across all organizational accounts using service control policies\" is incorrect. If all IAM activity was disabled across all accounts within the Organizational unit, each IAM user would not be able to do anything within the account.</p><p><strong>INCORRECT:</strong> \"Assign all IAM duties to the security operations team and prevent developers from attaching policies\" is incorrect. The easiest way to do this is to use a permissions boundary, to make sure the permissions are being administered appropriately.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
        "answers": [
          "<p>Send an alert every time a developer creates a new policy using an Amazon SNS topic.</p>",
          "<p>Disable IAM activity across all organizational accounts using service control policies.</p>",
          "<p>Assign all IAM duties to the security operations team and prevent developers from attaching policies.</p>",
          "<p>Set a permissions boundary on the developer IAM role that denies attaching administrator access.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "To accelerate experimentation and agility, a company allows developers to apply existing IAM policies to existing IAM roles. Nevertheless, the security operations team is concerned that the developers could attach the existing administrator policy, circumventing any other security policies.How should a solutions architect address this issue?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399138,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An Amazon EC2 instance runs in a VPC network, and the network must be secured by a solutions architect. The EC2 instances contain highly sensitive data and have been launched in private subnets. Company policy restricts EC2 instances that run in the VPC from accessing the internet. The instances need to access the software repositories using a third-party URL to download and install software product updates. All other internet traffic must be blocked, with no exceptions.</p><p>Which solution meets these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The AWS Network Firewall is a managed service that makes it easy to deploy essential network protections for all your Amazon Virtual Private Clouds, and you can then use domain list rules to block HTTP or HTTPS traffic to domains identified as low-reputation, or that are known or suspected to be associated with malware or botnets.</p><p><strong>CORRECT: </strong>\"Configure the route table for the private subnet so that it routes the outbound traffic to an AWS Network Firewall firewall then configure domain list rule groups\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF web ACL. Filter traffic requests based on source and destination IP address ranges with custom rules\" is incorrect. AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. It is designed to protect your applications from malicious traffic, not your VPC.</p><p><strong>INCORRECT:</strong> \"Establish strict inbound rules for your security groups. Specify the URLs of the authorized software repositories on the internet in your outbound rule\" is incorrect. You cannot specify URLs in security group rules so this would not work.</p><p><strong>INCORRECT:</strong> \"Place an Application Load Balancer in front of your EC2 instances. Direct all outbound traffic to the ALB. For outbound access to the internet, use a URL-based rule listener in the ALB's target group\" is incorrect. The ALB would not work as this sits within the VPC and is unable to control traffic entering and leaving the VPC itself.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/network-firewall/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc\">https://aws.amazon.com/network-firewall/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
        "answers": [
          "<p>Configure the route table for the private subnet so that it routes the outbound traffic to an AWS Network Firewall firewall then configure domain list rule groups.</p>",
          "<p>Create an AWS WAF web ACL. Filter traffic requests based on source and destination IP address ranges with custom rules.</p>",
          "<p>Establish strict inbound rules for your security groups. Specify the URLs of the authorized software repositories on the internet in your outbound rule.</p>",
          "<p>Place an Application Load Balancer in front of your EC2 instances. Direct all outbound traffic to the ALB. For outbound access to the internet, use a URL-based rule listener in the ALB's target group.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "An Amazon EC2 instance runs in a VPC network, and the network must be secured by a solutions architect. The EC2 instances contain highly sensitive data and have been launched in private subnets. Company policy restricts EC2 instances that run in the VPC from accessing the internet. The instances need to access the software repositories using a third-party URL to download and install software product updates. All other internet traffic must be blocked, with no exceptions.Which solution meets these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399140,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A small Python application is used by a company to process JSON documents and output the results to a SQL database which currently lives on-premises. The application is run thousands of times every day, and the company wants to move the application to the AWS Cloud. To maximize scalability and minimize operational overhead, the company needs a highly available solution.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Firstly, S3 is a highly available and durable place to store these JSON documents that will be written once and read many times (WORM). As this application runs thousands of times per day, AWS Lambda would be ideal to use as it will scale whenever the application needs to be ran, and Python is a runtime environment that is natively supported by AWS Lambda, whenever the events arrive in the S3 bucket, and this could be easily achieved using S3 event notifications. Finally Amazon Aurora is a highly available and durable AWS managed database. Amazon Aurora automatically maintains six copies of your data across three Availability Zones (AZs) to adhere to your redundancy requirements.</p><p><strong>CORRECT: </strong>\"Put the JSON documents in an Amazon S3 bucket. As documents arrive in the S3 bucket, create an AWS Lambda function that runs Python code to process them. Use Amazon Aurora DB clusters to store the results\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Build an S3 bucket to place the JSON documents in. Run the Python code on multiple Amazon EC2 instances to process the documents. Store the results in a database using the Amazon Aurora Database engine” is incorrect.</p><p>Multiple EC2 instances could work, however if you wanted to use EC2 to process the JSON documents you would need to either leave the EC2 instances running all the time (not cost effective) or have them spin up and spin down thousands of times per day (this would be slow and not ideal).</p><p><strong>INCORRECT:</strong> \"Create an Amazon Elastic Block Store (Amazon EBS) volume for the JSON documents. Attach the volume to multiple Amazon EC2 instances using the EBS Multi-Attach feature. Process the documents with Python code on the EC2 instances and then extract the results to an Amazon RDS DB instance\" is incorrect.</p><p>EBS is not optimized for write once read many use-cases, and if you wanted to use EC2 to process the JSON documents you would need to either leave the EC2 instances running all the time (not cost effective) or have them spin up and spin down thousands of times per day (this would be slow and not ideal).</p><p><strong>INCORRECT:</strong> \"The JSON documents should be queued as messages in the Amazon Simple Queue Service (Amazon SQS). Using the Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type, deploy the Python code as a container. The container can be used to process SQS messages. Using Amazon RDS, store the results” is incorrect.</p><p>A queue within Amazon SQS is not designed to be used for write once read many solutions, and it is designed to be used to decouple separate layers of your architecture. Secondly, ECS for EC2 is not ideal as you would need to either leave the EC2 instances running all the time (not cost effective) or have them spin up and spin down thousands of times per day (this would be slow and not ideal) if you wanted to use ECS for EC2.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
        "answers": [
          "<p>Build an S3 bucket to place the JSON documents in. Run the Python code on multiple Amazon EC2 instances to process the documents. Store the results in a database using the Amazon Aurora Database engine.</p>",
          "<p>Put the JSON documents in an Amazon S3 bucket. As documents arrive in the S3 bucket, create an AWS Lambda function that runs Python code to process them. Use Amazon Aurora DB clusters to store the results.</p>",
          "<p>Create an Amazon Elastic Block Store (Amazon EBS) volume for the JSON documents. Attach the volume to multiple Amazon EC2 instances using the EBS Multi-Attach feature. Process the documents with Python code on the EC2 instances and then extract the results to an Amazon RDS DB instance.</p>",
          "<p>The JSON documents should be queued as messages in the Amazon Simple Queue Service (Amazon SQS). Using the Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type, deploy the Python code as a container. The container can be used to process SQS messages. Using Amazon RDS, store the results.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Database",
      "question_plain": "A small Python application is used by a company to process JSON documents and output the results to a SQL database which currently lives on-premises. The application is run thousands of times every day, and the company wants to move the application to the AWS Cloud. To maximize scalability and minimize operational overhead, the company needs a highly available solution.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399142,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>There are badge readers located at every entrance of an organization’s warehouses. A message is sent over HTTPS when badges are scanned to indicate who tried to access the entrance.</p><p>A solutions architect must design a system to process these messages. A highly available solution is required. The solution must store results in a durable data store for later analysis.</p><p>Which system architecture should the solutions architect recommend?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon API Gateway would be ideal for providing a secure entry point for your application, and for traffic to be sent via HTTPS. AWS Lambda would integrate seamlessly with API Gateway to process the data, as an event-driven solution like this would be perfect when designing a scalable system based on sporadic use. Finally, DynamoDB is highly scalable and is a perfect repository for data to be stored for future analysis.</p><p><strong>CORRECT: </strong>\"Set up an HTTPS endpoint in Amazon API Gateway. To process the messages and save the results to Amazon DynamoDB, configure an API Gateway endpoint to invoke an AWS Lambda function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon EC2 instance to serve as the HTTPS endpoint and to process messages. An Amazon S3 bucket should be configured for the EC2 instance to save the results\" is incorrect. As the action of a badge being read to initiate access to a warehouse should only take a few seconds, spinning up an EC2 instance to serve as a HTTPS endpoint would take minutes, and is not suitable for this use case.</p><p><strong>INCORRECT:</strong> \"Direct incoming messages from the sensor to an AWS Lambda function using Amazon Route 53. Create a Lambda function that processes messages and saves results to Amazon DynamoDB” is incorrect. Amazon Route 53 is a managed DNS service, and DNS is not required in this instance as the badge reader does not have a DNS name.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon S3 gateway endpoint in your VPC. Connect the facility network to the VPC via a Site-to-Site VPN connection so that sensor data can be written directly to an S3 bucket\" is incorrect. VPC endpoints are designed to facilitate traffic across the AWS backbone network between AWS services and are not used to create connections between external endpoints outside of the AWS network and an Amazon S3 bucket.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
        "answers": [
          "<p>Create an Amazon EC2 instance to serve as the HTTPS endpoint and to process messages. An Amazon S3 bucket should be configured for the EC2 instance to save the results.</p>",
          "<p>Set up an HTTPS endpoint in Amazon API Gateway. To process the messages and save the results to Amazon DynamoDB, configure an API Gateway endpoint to invoke an AWS Lambda function.</p>",
          "<p>Direct incoming messages from the sensor to an AWS Lambda function using Amazon Route 53. Create a Lambda function that processes messages and saves results to Amazon DynamoDB.</p>",
          "<p>Set up an Amazon S3 gateway endpoint in your VPC. Connect the facility network to the VPC via a Site-to-Site VPN connection so that sensor data can be written directly to an S3 bucket.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "There are badge readers located at every entrance of an organization’s warehouses. A message is sent over HTTPS when badges are scanned to indicate who tried to access the entrance.A solutions architect must design a system to process these messages. A highly available solution is required. The solution must store results in a durable data store for later analysis.Which system architecture should the solutions architect recommend?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399144,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company needs to store data from an application. Data in the application changes frequently. All levels of stored data must be audited under a new regulation which the company adheres to.</p><p>Application storage capacity is running out on the company's on-premises infrastructure. To comply with the new regulation, a solutions architect must offload some data securely to AWS to relieve the on-premises capacity issues.</p><p>Which solution will meet these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Storage Gateway is a set of hybrid cloud storage services that provide on-premises access to virtually unlimited cloud storage. Secondly AWS CloudTrail monitors and records account activity across your AWS infrastructure, giving you control over storage, analysis, and remediation actions.</p><p><strong>CORRECT: </strong>\"Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Move existing data to Amazon S3 using AWS DataSync. Log data events using AWS CloudTrail” is incorrect. AWS DataSync is a secure, online service that automates and accelerates moving data between on-premises and AWS storage service and is not designed as a hybrid storage service.</p><p><strong>INCORRECT:</strong> \"The existing data can be transferred to Amazon S3 with the help of Amazon S3 Transfer Acceleration. Log data events using AWS CloudTrail” is incorrect. Amazon S3 Transfer Acceleration is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration is designed to optimize transfer speeds from across the world into S3 buckets and is not a migration service.</p><p><strong>INCORRECT:</strong> \"Move the existing data to Amazon S3 with AWS Snowcone. Using AWS CloudTrail, you can log management events\" is incorrect. AWS Snowcone is not suitable as a hybrid cloud service.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/storagegateway/\">https://aws.amazon.com/storagegateway/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>",
        "answers": [
          "<p>Move existing data to Amazon S3 using AWS DataSync. Log data events using AWS CloudTrail.</p>",
          "<p>Move the existing data to Amazon S3 with AWS Snowcone. Using AWS CloudTrail, you can log management events.</p>",
          "<p>The existing data can be transferred to Amazon S3 with the help of Amazon S3 Transfer Acceleration. Log data events using AWS CloudTrail.</p>",
          "<p>Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Migration & Transfer",
      "question_plain": "A company needs to store data from an application. Data in the application changes frequently. All levels of stored data must be audited under a new regulation which the company adheres to.Application storage capacity is running out on the company's on-premises infrastructure. To comply with the new regulation, a solutions architect must offload some data securely to AWS to relieve the on-premises capacity issues.Which solution will meet these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399146,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An organization manages its own MySQL databases, which are hosted on Amazon EC2 instances. In response to changes in demand, replication and scaling are manually managed by the company. It is essential for the company to have a way to add and remove compute capacity as needed from the database tier. The solution also must offer improved performance, scaling, and durability with minimal effort from operations.</p><p>Which solution meets these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon Aurora provides automatic scaling for MySQL databases. Amazon Aurora provides built-in security, continuous backups, serverless compute, up to 15 read replicas, automated multi-Region replication, and integrations with other AWS services. Aurora Serverless reduces any effort from operations also to provision any servers to manage the database cluster.</p><p><strong>CORRECT: </strong>\"Migrate the databases to Amazon Aurora Serverless (Aurora MySQL)” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the databases to Amazon Aurora Serverless (Aurora PostgreSQL)” is incorrect. Although PostgreSQL is an option for Aurora, the database schema would have to be changed to allow PostgreSQL compatibility.</p><p><strong>INCORRECT:</strong> \"Consolidate the databases into a single MySQL database. Use larger EC2 instances for the larger database” is incorrect. Databases run on a larger EC2 instance would not provide improved performance, scaling, and durability.</p><p><strong>INCORRECT:</strong> \"For the database tier, create an EC2 Auto Scaling group. Create a new database environment and migrate the existing databases” is incorrect. This would improve the scalability of the solution but would still have to be heavily managed by the organization, something that would not be needed with Aurora Serverless.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/serverless/\">https://aws.amazon.com/rds/aurora/serverless/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
        "answers": [
          "<p>Migrate the databases to Amazon Aurora Serverless (Aurora MySQL).</p>",
          "<p>Migrate the databases to Amazon Aurora Serverless (Aurora PostgreSQL).</p>",
          "<p>Consolidate the databases into a single MySQL database. Use larger EC2 instances for the larger database.</p>",
          "<p>For the database tier, create an EC2 Auto Scaling group. Create a new database environment and migrate the existing databases.</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Database",
      "question_plain": "An organization manages its own MySQL databases, which are hosted on Amazon EC2 instances. In response to changes in demand, replication and scaling are manually managed by the company. It is essential for the company to have a way to add and remove compute capacity as needed from the database tier. The solution also must offer improved performance, scaling, and durability with minimal effort from operations.Which solution meets these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399148,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>Data from 45 TB of data is used for reporting by a company. The company wants to move this data from on premises into the AWS cloud. A custom application in the company's data center runs a weekly data transformation job and the company plans to pause the application until the data transfer is complete and needs to begin the transfer process as soon as possible.</p><p>The data center bandwidth is saturated, and a solutions architect has been tasked to transfer the data and must configure the transformation job to continue to run in the AWS Cloud.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>As the network is saturated, the solutions architect will have to use a physical solution, i.e. a member of the snow family to achieve this requirement quickly. As the data transformation job needs to be completed in the cloud, using AWS Glue will suit this requirement also. AWS Glue is a managed data transformation service.</p><p><strong>CORRECT: </strong>\"Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. and create a custom transformation job by using AWS Glue\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The data can be moved using AWS DataSync. Using AWS Glue, create a custom transformation job” is incorrect. As the network is saturated, AWS DataSync will not work as it is primarily an online data transfer service to transfer data between a data center and AWS.</p><p><strong>INCORRECT:</strong> \"The data will be moved using an AWS Snowcone device. The transformation application should be deployed to the device” is incorrect. You would not be able to deploy a transformation service locally to the Snowcone device as it is not optimized for compute operations.</p><p><strong>INCORRECT:</strong> \"Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Transfer the data to the device. Launch a new EC2 instance to run the transformation application” is incorrect. Using an EC2 instance instead of a managed service like AWS Glue will include more operational overhead for the organization.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-glue/\">https://digitalcloud.training/aws-glue/</a></p>",
        "answers": [
          "<p>The data can be moved using AWS DataSync. Using AWS Glue, create a custom transformation job.</p>",
          "<p>The data will be moved using an AWS Snowcone device. The transformation application should be deployed to the device.</p>",
          "<p>Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. and create a custom transformation job by using AWS Glue.</p>",
          "<p>Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Transfer the data to the device. Launch a new EC2 instance to run the transformation application.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Migration & Transfer",
      "question_plain": "Data from 45 TB of data is used for reporting by a company. The company wants to move this data from on premises into the AWS cloud. A custom application in the company's data center runs a weekly data transformation job and the company plans to pause the application until the data transfer is complete and needs to begin the transfer process as soon as possible.The data center bandwidth is saturated, and a solutions architect has been tasked to transfer the data and must configure the transformation job to continue to run in the AWS Cloud.Which solution will meet these requirements with the LEAST operational overhead?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399150,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>IAM permissions-related Access Denied errors and Unauthorized errors need to be analyzed and troubleshooted by a company. AWS CloudTrail has been enabled at the company.</p><p>Which solution will meet these requirements with the LEAST effort?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>CloudTrail logs are stored natively within an S3 bucket , which can then be easily integrated with Amazon QuickSight. Amazon QuickSight is a data visualization tool which will show any IAM permissions-related Access Denied errors and Unauthorized errors. </p><p><strong>CORRECT: </strong>\"Search CloudTrail logs with Amazon QuickSight. Create a dashboard to identify the errors” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a custom script and execute it against CloudTrail logs to find errors using AWS Batch” is incorrect. Writing custom scripts is inevitably more effort than using the native connection between AWS CloudTrail and Amazon QuickSight.</p><p><strong>INCORRECT:</strong> \"Search CloudTrail logs with Amazon RedShift. Create a dashboard to identify the errors” is incorrect. Amazon RedShift would not be a simple way of achieving this outcome.</p><p><strong>INCORRECT:</strong> “Write custom scripts to query CloudTrail logs using AWS Glue” is incorrect. AWS Batch requires configuring several EC2 instances to run jobs for you. This, and writing custom scripts will significantly increase the effort involved.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/logging-using-cloudtrail.html\">https://docs.aws.amazon.com/quicksight/latest/user/logging-using-cloudtrail.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-analytics-services/\">https://digitalcloud.training/aws-analytics-services/</a></p>",
        "answers": [
          "<p>Create a custom script and execute it against CloudTrail logs to find errors using AWS Batch</p>",
          "<p>Search CloudTrail logs with Amazon RedShift. Create a dashboard to identify the errors</p>",
          "<p>Write custom scripts to query CloudTrail logs using AWS Glue</p>",
          "<p>Search CloudTrail logs with Amazon QuickSight. Create a dashboard to identify the errors.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Analytics",
      "question_plain": "IAM permissions-related Access Denied errors and Unauthorized errors need to be analyzed and troubleshooted by a company. AWS CloudTrail has been enabled at the company.Which solution will meet these requirements with the LEAST effort?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399152,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company migrated a two-tier application from its on-premises data center to AWS Cloud. A Multi-AZ Amazon RDS for Oracle deployment is used for the data tier, along with 12 TB of General Purpose SSD Amazon EBS storage. With an average document size of 6 MB, the application processes, and stores documents as binary large objects (blobs) in the database.</p><p>Over time, the database size has grown, which has reduced performance and increased storage costs. A highly available and resilient solution is needed to improve database performance.</p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. The key in this question is the reference to binary large objects (blobs) which are stored in the database. Amazon S3 is an easy to use and very cost-effective solution for Write Once Read Many (WORM) applications and use cases.</p><p><strong>CORRECT: </strong>\"Set up an Amazon S3 bucket. The application should be updated to use S3 buckets to store documents. Store the object metadata in the existing database” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Increase the RDS DB instance size. Increase the storage capacity to 24 TiB. Change the storage type to Provisioned IOPS” is incorrect. Doing this will increase the performance of your application, however the cost will go up and not go down.</p><p><strong>INCORRECT:</strong> \"Reduce the size of the RDS DB instance. Increase the storage capacity to 24 TiB. Magnetic storage should be selected” is incorrect. Reducing the instance size will only decrease the performance of your application, alongside changing your EBS volume to a Magnetic volume.</p><p><strong>INCORRECT:</strong> \"Create a table in Amazon DynamoDB and update the application to use DynamoDB. Migrate Oracle data to DynamoDB using AWS Database Migration Service (AWS DMS)” is incorrect. DynamoDB is more expensive than Amazon S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/\">https://aws.amazon.com/s3/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
        "answers": [
          "<p>Reduce the size of the RDS DB instance. Increase the storage capacity to 24 TiB. Magnetic storage should be selected.</p>",
          "<p>Increase the RDS DB instance size. Increase the storage capacity to 24 TiB. Change the storage type to Provisioned IOPS.</p>",
          "<p>Set up an Amazon S3 bucket. The application should be updated to use S3 buckets to store documents. Store the object metadata in the existing database.</p>",
          "<p>Create a table in Amazon DynamoDB and update the application to use DynamoDB. Migrate Oracle data to DynamoDB using AWS Database Migration Service (AWS DMS).</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Storage",
      "question_plain": "A company migrated a two-tier application from its on-premises data center to AWS Cloud. A Multi-AZ Amazon RDS for Oracle deployment is used for the data tier, along with 12 TB of General Purpose SSD Amazon EBS storage. With an average document size of 6 MB, the application processes, and stores documents as binary large objects (blobs) in the database.Over time, the database size has grown, which has reduced performance and increased storage costs. A highly available and resilient solution is needed to improve database performance.Which solution will meet these requirements MOST cost-effectively?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399154,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A stock trading startup company has a custom web application to sell trading data to its users online. The company uses Amazon DynamoDB to store its data and wants to build a new service that sends an alert to the managers of four internal teams every time a new trading event is recorded. The company does not want this new service to affect the performance of the current application.</p><p>What should a solutions architect do to meet these requirements with the LEAST amount of operational overhead?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real time. This is the native way to handle this within DynamoDB, therefore will incur the least amount of operational overhead.</p><p><strong>CORRECT: </strong>\"On the table, enable Amazon DynamoDB Streams. Subscriptions can be made to a single Amazon Simple Notification Service (Amazon SNS) topic using triggers” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Write new event data to the table using DynamoDB transactions. The transactions should be configured to notify internal teams” is incorrect. With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing TransactWriteItems or TransactGetItems operation. The following sections describe API operations, capacity management, best practices, and other details about using transactional operations in DynamoDB. This is not suitable for this use case.</p><p><strong>INCORRECT:</strong> \"Use the current application to publish a message to four Amazon Simple Notification Service (Amazon SNS) topics. Each team should subscribe to one topic” is incorrect. Using four separate SNS topics will take a significant amount of overhead, and this functionality can be managed natively within DynamoDB using DynamoDB streams.</p><p><strong>INCORRECT:</strong> \"Create a custom attribute for each record to flag new items. A cron job can be written to scan the table every minute for new items and notify an Amazon Simple Queue Service (Amazon SQS) queue” is incorrect. Writing a CRON job also takes significant overhead compared to using DynamoDB streams.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
        "answers": [
          "<p>Write new event data to the table using DynamoDB transactions. The transactions should be configured to notify internal teams.</p>",
          "<p>Use the current application to publish a message to four Amazon Simple Notification Service (Amazon SNS) topics. Each team should subscribe to one topic.</p>",
          "<p>On the table, enable Amazon DynamoDB Streams. Subscriptions can be made to a single Amazon Simple Notification Service (Amazon SNS) topic using triggers.</p>",
          "<p>Create a custom attribute for each record to flag new items. A cron job can be written to scan the table every minute for new items and notify an Amazon Simple Queue Service (Amazon SQS) queue.</p>"
        ]
      },
      "correct_response": ["c"],
      "section": "AWS Database",
      "question_plain": "A stock trading startup company has a custom web application to sell trading data to its users online. The company uses Amazon DynamoDB to store its data and wants to build a new service that sends an alert to the managers of four internal teams every time a new trading event is recorded. The company does not want this new service to affect the performance of the current application.What should a solutions architect do to meet these requirements with the LEAST amount of operational overhead?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399156,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company is architecting a shared storage solution for an AWS-hosted gaming application. The company needs the ability to use Lustre clients to access data. The solution must be fully managed.</p><p>Which solution meets these requirements?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Amazon FSx for Lustre provides fully managed shared storage with the scalability and performance of the popular Lustre file system. It is fully managed and will allow the company to attach the file system to the origin server and connect the application server to the file system.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-27_10-35-12-bdf1d22b1d9df57fe18a62c76e1321e6.jpg\"><p><strong>CORRECT: </strong>\"Create an Amazon FSx for Lustre file system. Connect the file system to the origin server. Ensure that the file system is connected to the application server” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Assign the AWS DataSync task to share the data as a mountable file system. Sync the file system with the application server” is incorrect. The solution requires a managed Lustre file system, so this would not work.</p><p><strong>INCORRECT:</strong> \"Create a file gateway with AWS Storage Gateway. Create a client-side file share using the required protocol. Share the file with the application server” is incorrect. The solution requires a managed Lustre file system, so this would not work.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Elastic File System (Amazon EFS) file system and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system” is incorrect. The solution requires a managed Lustre file system, so this would not work.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/lustre/\">https://aws.amazon.com/fsx/lustre/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
        "answers": [
          "<p>Assign the AWS DataSync task to share the data as a mountable file system. Sync the file system with the application server.</p>",
          "<p>Create a file gateway with AWS Storage Gateway. Create a client-side file share using the required protocol. Share the file with the application server.</p>",
          "<p>Create an Amazon Elastic File System (Amazon EFS) file system and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.</p>",
          "<p>Create an Amazon FSx for Lustre file system. Connect the file system to the origin server. Ensure that the file system is connected to the application server.</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Storage",
      "question_plain": "A company is architecting a shared storage solution for an AWS-hosted gaming application. The company needs the ability to use Lustre clients to access data. The solution must be fully managed.Which solution meets these requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399158,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company is storing a large quantity of small files in an Amazon S3 bucket. An application running on an Amazon EC2 instance needs permissions to access and process the files in the S3 bucket.</p><p>Which action will MOST securely grant the EC2 instance access to the S3 bucket?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>IAM roles should be used in place of storing credentials on Amazon EC2 instances. This is the most secure way to provide permissions to EC2 as no credentials are stored and short-lived credentials are obtained using AWS STS. Additionally, the policy attached to the role should provide least privilege permissions.</p><p><strong>CORRECT: </strong>\"Create an IAM role with least privilege permissions and attach it to the EC2 instance profile\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Generate access keys and store the credentials on the EC2 instance for use in making API calls\" is incorrect. This is not best practice, IAM roles are preferred.</p><p><strong>INCORRECT:</strong> \"Create an IAM user for the application with specific permissions to the S3 bucket\" is incorrect. Instances should use IAM Roles for delegation not user accounts.</p><p><strong>INCORRECT:</strong> \"Create a bucket ACL on the S3 bucket and configure the EC2 instance ID as a grantee\" is incorrect. You cannot configure an EC2 instance ID on a bucket ACL and bucket ACLs cannot be used to restrict access in this scenario.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
        "answers": [
          "<p>Create a bucket ACL on the S3 bucket and configure the EC2 instance ID as a grantee.</p>",
          "<p>Create an IAM role with least privilege permissions and attach it to the EC2 instance profile.</p>",
          "<p>Create an IAM user for the application with specific permissions to the S3 bucket.</p>",
          "<p>Generate access keys and store the credentials on the EC2 instance for use in making API calls.</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A company is storing a large quantity of small files in an Amazon S3 bucket. An application running on an Amazon EC2 instance needs permissions to access and process the files in the S3 bucket.Which action will MOST securely grant the EC2 instance access to the S3 bucket?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399160,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company has acquired another business and needs to migrate their 50TB of data into AWS within 1 month. They also require a secure, reliable and private connection to the AWS cloud.</p><p>How are these requirements best accomplished?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>AWS Direct Connect provides a secure, reliable and private connection. However, lead times are often longer than 1 month so it cannot be used to migrate data within the timeframes. Therefore, it is better to use AWS Snowball to move the data and order a Direct Connect connection to satisfy the other requirement later on. In the meantime the organization can use an AWS VPN for secure, private access to their VPC.</p><p><strong>CORRECT: </strong>\"Migrate data using AWS Snowball. Provision an AWS VPN initially and order a Direct Connect link\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Provision an AWS Direct Connect connection and migrate the data over the link\" is incorrect due to the lead time for installation.</p><p><strong>INCORRECT:</strong> \"Launch a Virtual Private Gateway (VPG) and migrate the data over the AWS VPN\" is incorrect. A VPG is the AWS-side of an AWS VPN. A VPN does not provide a private connection and is not reliable as you can never guarantee the latency over the Internet</p><p><strong>INCORRECT:</strong> \"Provision an AWS VPN CloudHub connection and migrate the data over redundant links\" is incorrect. AWS VPN CloudHub is a service for connecting multiple sites into your VPC over VPN connections. It is not used for aggregating links and the limitations of Internet bandwidth from the company where the data is stored will still be an issue. It also uses the public Internet so is not a private or reliable connection.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p><p><a href=\"https://aws.amazon.com/directconnect/\">https://aws.amazon.com/directconnect/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-direct-connect/\">https://digitalcloud.training/aws-direct-connect/</a></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
        "answers": [
          "<p>Provision an AWS Direct Connect connection and migrate the data over the link</p>",
          "<p>Migrate data using AWS Snowball. Provision an AWS VPN initially and order a Direct Connect link</p>",
          "<p>Launch a Virtual Private Gateway (VPG) and migrate the data over the AWS VPN</p>",
          "<p>Provision an AWS VPN CloudHub connection and migrate the data over redundant links</p>"
        ]
      },
      "correct_response": ["b"],
      "section": "AWS Migration & Transfer",
      "question_plain": "A company has acquired another business and needs to migrate their 50TB of data into AWS within 1 month. They also require a secure, reliable and private connection to the AWS cloud.How are these requirements best accomplished?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399162,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "<p>A website is running on Amazon EC2 instances and access is restricted to a limited set of IP ranges. A solutions architect is planning to migrate static content from the website to an Amazon S3 bucket configured as an origin for an Amazon CloudFront distribution. Access to the static content must be restricted to the same set of IP addresses.</p><p>Which combination of steps will meet these requirements? (Select TWO.)</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "<p>To prevent users from circumventing the controls implemented on CloudFront (using WAF or presigned URLs / signed cookies) you can use an origin access identity (OAI). An OAI is a special CloudFront user that you associate with a distribution.</p><p>The next step is to change the permissions either on your Amazon S3 bucket or on the files in your bucket so that only the origin access identity has read permission (or read and download permission). This can be implemented through a bucket policy.</p><p>To control access at the CloudFront layer the AWS Web Application Firewall (WAF) can be used. With WAF you must create an ACL that includes the IP restrictions required and then associate the web ACL with the CloudFront distribution.</p><p><strong>CORRECT: </strong>\"Create an origin access identity (OAI) and associate it with the distribution. Change the permissions in the bucket policy so that only the OAI can read the objects\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Create an AWS WAF web ACL that includes the same IP restrictions that exist in the EC2 security group. Associate this new web ACL with the CloudFront distribution\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create an origin access identity (OAI) and associate it with the distribution. Generate presigned URLs that limit access to the OAI\" is incorrect. Presigned URLs can be used to protect access to CloudFront but they cannot be used to limit access to an OAI.</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF web ACL that includes the same IP restrictions that exist in the EC2 security group. Associate this new web ACL with the Amazon S3 bucket\" is incorrect. The Web ACL should be associated with CloudFront, not S3.</p><p><strong>INCORRECT:</strong> \"Attach the existing security group that contains the IP restrictions to the Amazon CloudFront distribution\" is incorrect. You cannot attach a security group to a CloudFront distribution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>",
        "answers": [
          "<p>Create an origin access identity (OAI) and associate it with the distribution. Change the permissions in the bucket policy so that only the OAI can read the objects.</p>",
          "<p>Create an origin access identity (OAI) and associate it with the distribution. Generate presigned URLs that limit access to the OAI.</p>",
          "<p>Create an AWS WAF web ACL that includes the same IP restrictions that exist in the EC2 security group. Associate this new web ACL with the Amazon S3 bucket.</p>",
          "<p>Create an AWS WAF web ACL that includes the same IP restrictions that exist in the EC2 security group. Associate this new web ACL with the CloudFront distribution.</p>",
          "<p>Attach the existing security group that contains the IP restrictions to the Amazon CloudFront distribution.</p>"
        ]
      },
      "correct_response": ["a", "d"],
      "section": "AWS Security, Identity, & Compliance",
      "question_plain": "A website is running on Amazon EC2 instances and access is restricted to a limited set of IP ranges. A solutions architect is planning to migrate static content from the website to an Amazon S3 bucket configured as an origin for an Amazon CloudFront distribution. Access to the static content must be restricted to the same set of IP addresses.Which combination of steps will meet these requirements? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399164,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An application is running on Amazon EC2 behind an Elastic Load Balancer (ELB). Content is being published using Amazon CloudFront and you need to restrict the ability for users to circumvent CloudFront and access the content directly through the ELB.</p><p>How can you configure this solution?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>The only way to get this working is by using a VPC Security Group for the ELB that is configured to allow only the internal service IP ranges associated with CloudFront. As these are updated from time to time, you can use AWS Lambda to automatically update the addresses. This is done using a trigger that is triggered when AWS issues an SNS topic update when the addresses are changed.</p><p><strong>CORRECT: </strong>\"Create a VPC Security Group for the ELB and use AWS Lambda to automatically update the CloudFront internal service IP addresses when they change\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Origin Access Identity (OAI) and associate it with the distribution\" is incorrect. You can use an OAI to restrict access to content in Amazon S3 but not on EC2 or ELB.</p><p><strong>INCORRECT:</strong> \"Use signed URLs or signed cookies to limit access to the content\" is incorrect. Signed cookies and URLs are used to limit access to files but this does not stop people from circumventing CloudFront and accessing the ELB directly.</p><p><strong>INCORRECT:</strong> \"Use a Network ACL to restrict access to the ELB\" is incorrect. A Network ACL can be used to restrict access to an ELB but it is recommended to use security groups and this solution is incomplete as it does not account for the fact that the internal service IP ranges change over time.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-automatically-update-your-security-groups-for-amazon-cloudfront-and-aws-waf-by-using-aws-lambda/\">https://aws.amazon.com/blogs/security/how-to-automatically-update-your-security-groups-for-amazon-cloudfront-and-aws-waf-by-using-aws-lambda/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
        "answers": [
          "<p>Create an Origin Access Identity (OAI) and associate it with the distribution</p>",
          "<p>Use signed URLs or signed cookies to limit access to the content</p>",
          "<p>Use a Network ACL to restrict access to the ELB</p>",
          "<p>Create a VPC Security Group for the ELB and use AWS Lambda to automatically update the CloudFront internal service IP addresses when they change</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Networking & Content Delivery",
      "question_plain": "An application is running on Amazon EC2 behind an Elastic Load Balancer (ELB). Content is being published using Amazon CloudFront and you need to restrict the ability for users to circumvent CloudFront and access the content directly through the ELB.How can you configure this solution?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399166,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>A company has divested a single business unit and needs to move the AWS account owned by the business unit to another AWS Organization. How can this be achieved?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>Accounts can be migrated between organizations. To do this you must have root or IAM access to both the member and master accounts. Resources will remain under the control of the migrated account.</p><p><strong>CORRECT: </strong>\"Migrate the account using the AWS Organizations console\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a new account in the destination AWS Organization and migrate resources\" is incorrect. You do not need to create a new account in the destination AWS Organization as you can just migrate the existing account.</p><p><strong>INCORRECT:</strong> \"Create a new account in the destination AWS Organization and share the original resources using AWS Resource Access Manager\" is incorrect. You do not need to create a new account in the destination AWS Organization as you can just migrate the existing account.</p><p><strong>INCORRECT:</strong> \"Migrate the account using AWS CloudFormation\" is incorrect. You do not need to use AWS CloudFormation. You can use the Organizations API or AWS CLI for when there are many accounts to migrate and therefore you could use CloudFormation for any additional automation but it is not necessary for this scenario.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/\">https://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
        "answers": [
          "<p>Create a new account in the destination AWS Organization and migrate resources</p>",
          "<p>Create a new account in the destination AWS Organization and share the original resources using AWS Resource Access Manager</p>",
          "<p>Migrate the account using AWS CloudFormation</p>",
          "<p>Migrate the account using the AWS Organizations console</p>"
        ]
      },
      "correct_response": ["d"],
      "section": "AWS Management & Governance",
      "question_plain": "A company has divested a single business unit and needs to move the AWS account owned by the business unit to another AWS Organization. How can this be achieved?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 80399168,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "<p>An Amazon RDS PostgreSQL database is configured as Multi-AZ. A solutions architect needs to scale read performance and the solution must be configured for high availability. What is the most cost-effective solution?</p>",
        "relatedLectureIds": "",
        "feedbacks": ["", "", "", ""],
        "explanation": "<p>You can create a read replica as a Multi-AZ DB instance. Amazon RDS creates a standby of your replica in another Availability Zone for failover support for the replica. Creating your read replica as a Multi-AZ DB instance is independent of whether the source database is a Multi-AZ DB instance.</p><p><strong>CORRECT: </strong>\"Create a read replica as a Multi-AZ DB instance\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Deploy a read replica in a different AZ to the master DB instance\" is incorrect as this does not provide high availability for the read replica</p><p><strong>INCORRECT:</strong> \"Deploy a read replica using Amazon ElastiCache\" is incorrect as ElastiCache is not used to create read replicas of RDS database.</p><p><strong>INCORRECT:</strong> \"Deploy a read replica in the same AZ as the master DB instance\" is incorrect as this solution does not include HA for the read replica.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/\">https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
        "answers": [
          "<p>Create a read replica as a Multi-AZ DB instance</p>",
          "<p>Deploy a read replica in a different AZ to the master DB instance</p>",
          "<p>Deploy a read replica using Amazon ElastiCache</p>",
          "<p>Deploy a read replica in the same AZ as the master DB instance</p>"
        ]
      },
      "correct_response": ["a"],
      "section": "AWS Database",
      "question_plain": "An Amazon RDS PostgreSQL database is configured as Multi-AZ. A solutions architect needs to scale read performance and the solution must be configured for high availability. What is the most cost-effective solution?",
      "related_lectures": []
    }
  ]
}
